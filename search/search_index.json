{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Generative AI Study Hub","text":"<p>Last updated: July 26, 2025</p> <p>This is your central hub for learning and referencing Generative AI concepts.  Choose from the Study Path to go through structured topics or explore the Reference Hub for focused research and tooling insights.</p>"},{"location":"reference-hub/","title":"Reference Hub","text":"<p>Welcome to the Reference Hub! Here you'll find foundational topics, research tools, and hands-on practices for working with Generative AI.</p>"},{"location":"study-path/","title":"Study Path","text":"<p>Welcome to the Study Path! Explore structured lessons on Transformers, LLMOps, and Evaluation Frameworks.</p>"},{"location":"study-path/transformers/01-intro/","title":"Intro","text":"<p>\ud83c\udf93 Course Overview</p> <p>This comprehensive and hands-on course is designed to provide practical mastery of transformers and generative large language models (LLMs). It blends theory, implementation, and real-world applications, helping learners develop production-grade skills in natural language processing (NLP) and generative AI.</p> <p>\u2e3b</p> <p>\ud83d\udd0d Key Highlights     \u2022   Instructor: The Fuzzy Scientist, LLM Engineering Lead and seasoned AI practitioner.     \u2022   Duration: ~7.5 hours     \u2022   Language: English (Auto-generated subtitles available)     \u2022   Audience: AI professionals, engineers, researchers, and students with Python and ML basics.</p> <p>\u2e3b</p> <p>\ud83e\udde0 What You\u2019ll Learn</p> <p>\ud83c\udfd7\ufe0f NLP Foundations     \u2022   Evolution from rule-based systems \u2192 statistical models \u2192 machine learning \u2192 deep learning.     \u2022   Overview of modern NLP workflows and language understanding.</p> <p>\u2699\ufe0f Transformers &amp; Architecture     \u2022   In-depth on attention mechanisms, encoders/decoders, and tokenization/embeddings.     \u2022   Learn pre-training and fine-tuning strategies.</p> <p>\ud83e\udd16 Key Transformer Models     \u2022   BERT: Encoder-only model for comprehension tasks.     \u2022   GPT: Decoder-only model for creative generation.     \u2022   T5: Encoder-decoder that frames all NLP tasks as text-to-text.</p> <p>\ud83d\udd28 Practical Usage     \u2022   Build hands-on projects:     \u2022   Semantic Search Engine     \u2022   Question Answering from Documents     \u2022   Instruction-following LLM (like ChatGPT)     \u2022   Review Generation using Encoder-Decoder Models     \u2022   Explore MLM, embeddings, and tokenization in context.</p> <p>\ud83d\udce6 Large Language Models (LLMs)     \u2022   Core architecture and input/output flows     \u2022   Concepts like RLHF, chat templates, generation strategies     \u2022   Specialized tuning with LoRA, 8-bit/4-bit quantization, FlashAttention</p> <p>\ud83e\uddea Advanced Model Training     \u2022   Optimization using DeepSpeed and FSDP     \u2022   Managing large-scale training: memory, adapter merging, sequence limits</p> <p>\u2e3b</p> <p>\ud83d\udca1 What Makes This Course Unique     \u2022   Combines real-world experience with hands-on practice     \u2022   Includes updated modules to reflect the latest advancements (e.g., LLaMA3, Phi3, Gemma2)     \u2022   Practical orientation: Build real applications and deploy scalable solutions</p> <p>\u2e3b</p> <p>\ud83d\udc64 Ideal For     \u2022   Tech professionals aiming to lead GenAI projects     \u2022   Aspiring AI practitioners &amp; ML engineers     \u2022   Researchers or students wanting to apply LLMs effectively</p> <p>\u2e3b</p> <p>\ud83d\udccc Prerequisites     \u2022   Python programming (basic)     \u2022   Foundational understanding of machine learning     \u2022   Strong curiosity about LLMs and NLP</p>"}]}