{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Generative AI Study Hub","text":"<p>Last updated: July 26, 2025</p> <p>This is your central hub for learning and referencing Generative AI concepts.  Choose from the Study Path to go through structured topics or explore the Reference Hub for focused research and tooling insights.</p>"},{"location":"reference-hub/","title":"Reference Hub","text":"<p>Welcome to the Reference Hub! Here you'll find foundational topics, research tools, and hands-on practices for working with Generative AI.</p>"},{"location":"study-path/","title":"Study Path","text":"<p>Welcome to the Study Path! Explore structured lessons on Transformers, LLMOps, and Evaluation Frameworks.</p>"},{"location":"study-path/transformers/01-intro/","title":"1. Intro","text":"<p>Certainly! Below is your updated Markdown section with a professionally structured \u201d## References &amp; Further Reading\u201d section added at the end\u2014fully compatible with MkDocs Material formatting and aligned with the tone of the rest of your content:</p> <p>\u2e3b</p>"},{"location":"study-path/transformers/01-intro/#what-youll-learn","title":"What You\u2019ll Learn","text":"<p>\ud83d\udccc Quick Navigation</p> <ul> <li>What You\u2019ll Learn</li> <li>Part 1: Fundamentals of NLP and Transformers<ul> <li>Transformer Foundations</li> </ul> </li> <li>Part 2: Working with Large Language Models (LLMs)<ul> <li>Key Model Architectures</li> <li>Real-World Tasks You\u2019ll Implement</li> <li>Advanced Tooling and Techniques</li> </ul> </li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers/01-intro/#part-1-fundamentals-of-nlp-and-transformers","title":"Part 1: Fundamentals of NLP and Transformers","text":"<p>You\u2019ll explore the evolution of natural language processing (NLP) through four historical phases:</p> <ul> <li>Rule-Based Systems </li> <li> <p>Manually defined rules for parsing, tagging, and other language tasks.</p> </li> <li> <p>Statistical Methods </p> </li> <li> <p>Used mathematical probability and co-occurrence to model language.</p> </li> <li> <p>Machine Learning Era </p> </li> <li> <p>Leveraged labeled data for training classifiers like SVMs and Naive Bayes.</p> </li> <li> <p>Deep Learning &amp; Embeddings </p> </li> <li>Enabled dense semantic understanding and contextual word representations.</li> </ul>"},{"location":"study-path/transformers/01-intro/#transformer-foundations","title":"Transformer Foundations","text":"<p>You\u2019ll also build a deep understanding of the architecture that powers modern LLMs:</p> <ul> <li>Attention Mechanism </li> <li> <p>Allows models to focus on important parts of the input.</p> </li> <li> <p>Encoder-Decoder Structure </p> </li> <li> <p>Enables tasks like translation, summarization, and text generation.</p> </li> <li> <p>Tokenization &amp; Embeddings </p> </li> <li> <p>Converts text into vectors, enabling model computation.</p> </li> <li> <p>Pretraining and Fine-Tuning </p> </li> <li>Learn how general-purpose models adapt to specific tasks.</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/01-intro/#part-2-working-with-large-language-models-llms","title":"Part 2: Working with Large Language Models (LLMs)","text":"<p>This section focuses on state-of-the-art transformer-based models and their practical applications.</p>"},{"location":"study-path/transformers/01-intro/#key-model-architectures","title":"Key Model Architectures","text":"<ul> <li>BERT (Encoder-only) </li> <li> <p>Learns bidirectional context; ideal for understanding tasks like classification or Q&amp;A.</p> </li> <li> <p>GPT (Decoder-only) </p> </li> <li> <p>Generates coherent, fluent text; the backbone of tools like ChatGPT.</p> </li> <li> <p>T5 (Encoder-Decoder) </p> </li> <li>Treats all problems as a text-to-text task, offering maximum flexibility.</li> </ul>"},{"location":"study-path/transformers/01-intro/#real-world-tasks-youll-implement","title":"Real-World Tasks You\u2019ll Implement","text":"<ul> <li>Masked Language Modeling (MLM)  </li> <li>Semantic Search with embeddings  </li> <li>Document-Based Question Answering  </li> <li>Instruction-Following Text Generation  </li> <li>Product Review Generation (prompt-based)</li> </ul>"},{"location":"study-path/transformers/01-intro/#advanced-tooling-and-techniques","title":"Advanced Tooling and Techniques","text":"<p>You\u2019ll gain hands-on experience with modern model optimization strategies:</p> <ul> <li>LoRA and PeFT (parameter-efficient fine-tuning)  </li> <li>8-bit / 4-bit quantization for faster, smaller models  </li> <li>FlashAttention, DeepSpeed, and FSDP for accelerated training  </li> <li>Chat templates and RLHF (Reinforcement Learning from Human Feedback)</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/01-intro/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>Hugging Face: T5-base Amazon Product Reviews Model</li> <li>Hugging Face: DiabloGPT Open Instruct Model</li> <li>Google Colab: T5 Product Review Notebook</li> <li>Attention Is All You Need (Transformer Paper)</li> <li>Hugging Face Transformers Documentation</li> <li>Google AI Blog on BERT</li> <li>Sebastian Ruder: NLP Progress</li> <li>The Illustrated Transformer (by Jay Alammar)</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/02-getting-started/","title":"2. Getting Started","text":"<p>Here\u2019s your updated Markdown with a new References &amp; Further Reading section added at the end, formatted cleanly for MkDocs Material:</p>"},{"location":"study-path/transformers/02-getting-started/#nlp-evolution-timeline","title":"NLP Evolution Timeline","text":""},{"location":"study-path/transformers/02-getting-started/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Historical NLP Techniques</li> <li>2. Statistical NLP Era</li> <li>3. Machine Learning Era in NLP</li> <li>4. Embedding Era in NLP</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers/02-getting-started/#1-historical-nlp-techniques","title":"1. Historical NLP Techniques","text":"<p>Understanding the evolution of NLP techniques provides critical context for modern advancements like transformers. This section explores foundational rule-based systems.</p>"},{"location":"study-path/transformers/02-getting-started/#rule-based-nlp-era","title":"Rule-Based NLP Era","text":"<ul> <li>Built on manually crafted linguistic rules</li> <li>Focused on syntactic analysis:</li> <li>Parsing: Grammatical structure and relationships</li> <li>Part-of-Speech Tagging: Identifying grammatical roles</li> <li>Applications:</li> <li>Syntax analysis</li> <li>Text summarization</li> <li>Machine translation</li> </ul>"},{"location":"study-path/transformers/02-getting-started/#key-limitations","title":"Key Limitations","text":"<ul> <li>Ambiguity: Poor context awareness</li> <li>Scalability: Rule creation and maintenance were not feasible at scale</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/02-getting-started/#2-statistical-nlp-era","title":"2. Statistical NLP Era","text":"<p>The transition to data-driven statistical techniques marked a turning point in NLP.</p>"},{"location":"study-path/transformers/02-getting-started/#key-innovations","title":"Key Innovations","text":"<ul> <li>Data-Driven Shift: Replaced rules with learned probabilities</li> <li>Probabilistic Language Models: Modeled word likelihoods and co-occurrence patterns</li> <li>n-Grams: Captured word sequences (e.g., bigrams, trigrams)</li> <li>Hidden Markov Models (HMMs):</li> <li>Used for sequence tasks (POS tagging, NER)</li> <li>Modeled state transitions for linguistic structure</li> </ul>"},{"location":"study-path/transformers/02-getting-started/#applications","title":"Applications","text":"<ul> <li>POS Tagging: Predict tags using probability sequences</li> <li>Named Entity Recognition (NER): Detect names, dates, organizations</li> </ul>"},{"location":"study-path/transformers/02-getting-started/#limitations","title":"Limitations","text":"<ul> <li>Data Sparsity: Rare word combinations weakened predictions</li> <li>Shallow Semantics: Couldn\u2019t truly \u201cunderstand\u201d meaning</li> </ul>"},{"location":"study-path/transformers/02-getting-started/#evolution","title":"Evolution","text":"<p>These limitations led to machine learning and neural models, enabling more scalable, adaptive solutions.</p> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/02-getting-started/#3-machine-learning-era-in-nlp","title":"3. Machine Learning Era in NLP","text":"<p>Machine learning enabled NLP systems to generalize from data without extensive rules or handcrafted features.</p>"},{"location":"study-path/transformers/02-getting-started/#key-advancements","title":"Key Advancements","text":"<ul> <li>Naive Bayes: Probabilistic classifier for text classification (e.g., spam detection)</li> <li>Support Vector Machines (SVMs):</li> <li>Effective for sentiment analysis</li> <li>Worked well on high-dimensional text vectors</li> </ul>"},{"location":"study-path/transformers/02-getting-started/#rise-of-neural-networks","title":"Rise of Neural Networks","text":"<ul> <li>Reduced Feature Engineering: Learned features from raw data</li> <li>Applications: Summarization, translation, sentiment detection</li> </ul>"},{"location":"study-path/transformers/02-getting-started/#specialized-architectures","title":"Specialized Architectures","text":"<ul> <li>RNNs:</li> <li>Process text sequentially</li> <li>Preserve past input using hidden state</li> <li> <p>Limitations: Weak on long-term dependencies</p> </li> <li> <p>LSTMs:</p> </li> <li>Enhanced RNNs with memory cells</li> <li>Better handling of long-range context</li> <li>Enabled language modeling and generation</li> </ul>"},{"location":"study-path/transformers/02-getting-started/#milestones","title":"Milestones","text":"<ul> <li>Shifted to end-to-end learning</li> <li>More flexible and powerful than statistical models</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/02-getting-started/#4-embedding-era-in-nlp","title":"4. Embedding Era in NLP","text":"<p>Dense vector embeddings enabled models to capture word meaning and similarity, surpassing sparse representations like one-hot encoding.</p>"},{"location":"study-path/transformers/02-getting-started/#key-concepts","title":"Key Concepts","text":"<ul> <li>Word Embeddings:</li> <li>Low-dimensional, dense vectors for each word</li> <li> <p>Capture meaning through context-based learning</p> </li> <li> <p>Benefits Over One-Hot Encoding:</p> </li> <li>Smaller dimensionality</li> <li>Encoded meaning and similarity</li> </ul>"},{"location":"study-path/transformers/02-getting-started/#popular-embedding-techniques","title":"Popular Embedding Techniques","text":"Technique Developer Method Highlights Word2Vec Google Skip-gram, CBOW Context prediction via local word windows GloVe Stanford Co-occurrence + global stats Combines frequency and semantics FastText Facebook AI Subword n-grams Handles rare and OOV words better"},{"location":"study-path/transformers/02-getting-started/#applications_1","title":"Applications","text":"<ul> <li>Semantic Similarity: Text comparison</li> <li>Text Classification: Improved input features</li> <li>Translation, QA: Foundation for neural systems</li> <li>Input to Deep Models: Used in RNNs, LSTMs, and later transformers</li> </ul>"},{"location":"study-path/transformers/02-getting-started/#limitations_1","title":"Limitations","text":"<ul> <li>Static Embeddings: One vector per word, no context awareness</li> <li>No Polysemy Handling: Same vector for multiple meanings (e.g., \u201cbank\u201d)</li> </ul> <p>These drawbacks triggered the rise of contextualized embeddings (e.g., ELMo, BERT), marking the start of the Transformer Era.</p> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/02-getting-started/#references-further-reading","title":"References &amp; Further Reading","text":"<p>Here are some recommended resources to explore these topics further:</p> <ul> <li>Word2Vec - Google Research</li> <li>GloVe - Stanford NLP Group</li> <li>FastText - Facebook AI</li> <li>The Illustrated Word2Vec (Chris McCormick)</li> <li>A Primer in BERTology (ACL Survey)</li> <li>NLP Progress</li> <li>The Embedding Project on Hugging Face</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/03-nlp-overview/","title":"NLP Evolution Timeline","text":""},{"location":"study-path/transformers/03-nlp-overview/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Historical NLP Techniques</li> <li>2. Statistical NLP Era</li> <li>3. Machine Learning Era in NLP</li> <li>4. Embedding Era in NLP</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers/03-nlp-overview/#1-historical-nlp-techniques","title":"1. Historical NLP Techniques","text":"<p>Understanding the evolution of NLP techniques provides critical context for modern advancements like transformers. This section explores foundational rule-based systems.</p>"},{"location":"study-path/transformers/03-nlp-overview/#rule-based-nlp-era","title":"Rule-Based NLP Era","text":"<ul> <li>Built on manually crafted linguistic rules</li> <li>Focused on syntactic analysis:</li> <li>Parsing: Grammatical structure and relationships</li> <li>Part-of-Speech Tagging: Identifying grammatical roles</li> <li>Applications:</li> <li>Syntax analysis</li> <li>Text summarization</li> <li>Machine translation</li> </ul>"},{"location":"study-path/transformers/03-nlp-overview/#key-limitations","title":"Key Limitations","text":"<ul> <li>Ambiguity: Poor context awareness</li> <li>Scalability: Rule creation and maintenance were not feasible at scale</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/03-nlp-overview/#2-statistical-nlp-era","title":"2. Statistical NLP Era","text":"<p>The transition to data-driven statistical techniques marked a turning point in NLP.</p>"},{"location":"study-path/transformers/03-nlp-overview/#key-innovations","title":"Key Innovations","text":"<ul> <li>Data-Driven Shift: Replaced rules with learned probabilities</li> <li>Probabilistic Language Models: Modeled word likelihoods and co-occurrence patterns</li> <li>n-Grams: Captured word sequences (e.g., bigrams, trigrams)</li> <li>Hidden Markov Models (HMMs):</li> <li>Used for sequence tasks (POS tagging, NER)</li> <li>Modeled state transitions for linguistic structure</li> </ul>"},{"location":"study-path/transformers/03-nlp-overview/#applications","title":"Applications","text":"<ul> <li>POS Tagging: Predict tags using probability sequences</li> <li>Named Entity Recognition (NER): Detect names, dates, organizations</li> </ul>"},{"location":"study-path/transformers/03-nlp-overview/#limitations","title":"Limitations","text":"<ul> <li>Data Sparsity: Rare word combinations weakened predictions</li> <li>Shallow Semantics: Couldn\u2019t truly \u201cunderstand\u201d meaning</li> </ul>"},{"location":"study-path/transformers/03-nlp-overview/#evolution","title":"Evolution","text":"<p>These limitations led to machine learning and neural models, enabling more scalable, adaptive solutions.</p> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/03-nlp-overview/#3-machine-learning-era-in-nlp","title":"3. Machine Learning Era in NLP","text":"<p>Machine learning enabled NLP systems to generalize from data without extensive rules or handcrafted features.</p>"},{"location":"study-path/transformers/03-nlp-overview/#key-advancements","title":"Key Advancements","text":"<ul> <li>Naive Bayes: Probabilistic classifier for text classification (e.g., spam detection)</li> <li>Support Vector Machines (SVMs):</li> <li>Effective for sentiment analysis</li> <li>Worked well on high-dimensional text vectors</li> </ul>"},{"location":"study-path/transformers/03-nlp-overview/#rise-of-neural-networks","title":"Rise of Neural Networks","text":"<ul> <li>Reduced Feature Engineering: Learned features from raw data</li> <li>Applications: Summarization, translation, sentiment detection</li> </ul>"},{"location":"study-path/transformers/03-nlp-overview/#specialized-architectures","title":"Specialized Architectures","text":"<ul> <li>RNNs:</li> <li>Process text sequentially</li> <li>Preserve past input using hidden state</li> <li> <p>Limitations: Weak on long-term dependencies</p> </li> <li> <p>LSTMs:</p> </li> <li>Enhanced RNNs with memory cells</li> <li>Better handling of long-range context</li> <li>Enabled language modeling and generation</li> </ul>"},{"location":"study-path/transformers/03-nlp-overview/#milestones","title":"Milestones","text":"<ul> <li>Shifted to end-to-end learning</li> <li>More flexible and powerful than statistical models</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/03-nlp-overview/#4-embedding-era-in-nlp","title":"4. Embedding Era in NLP","text":"<p>Dense vector embeddings enabled models to capture word meaning and similarity, surpassing sparse representations like one-hot encoding.</p>"},{"location":"study-path/transformers/03-nlp-overview/#key-concepts","title":"Key Concepts","text":"<ul> <li>Word Embeddings:</li> <li>Low-dimensional, dense vectors for each word</li> <li> <p>Capture meaning through context-based learning</p> </li> <li> <p>Benefits Over One-Hot Encoding:</p> </li> <li>Smaller dimensionality</li> <li>Encoded meaning and similarity</li> </ul>"},{"location":"study-path/transformers/03-nlp-overview/#popular-embedding-techniques","title":"Popular Embedding Techniques","text":"Technique Developer Method Highlights Word2Vec Google Skip-gram, CBOW Context prediction via local word windows GloVe Stanford Co-occurrence + global stats Combines frequency and semantics FastText Facebook AI Subword n-grams Handles rare and OOV words better"},{"location":"study-path/transformers/03-nlp-overview/#applications_1","title":"Applications","text":"<ul> <li>Semantic Similarity: Text comparison</li> <li>Text Classification: Improved input features</li> <li>Translation, QA: Foundation for neural systems</li> <li>Input to Deep Models: Used in RNNs, LSTMs, and later transformers</li> </ul>"},{"location":"study-path/transformers/03-nlp-overview/#limitations_1","title":"Limitations","text":"<ul> <li>Static Embeddings: One vector per word, no context awareness</li> <li>No Polysemy Handling: Same vector for multiple meanings (e.g., \u201cbank\u201d)</li> </ul> <p>These drawbacks triggered the rise of contextualized embeddings (e.g., ELMo, BERT), marking the start of the Transformer Era.</p> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/03-nlp-overview/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>Word2Vec Explained (Google Research)</li> <li>GloVe: Global Vectors for Word Representation (Stanford)</li> <li>FastText (Facebook AI)</li> <li>The Illustrated Transformer (Jay Alammar)</li> <li>Sebastian Ruder: NLP Progress Tracker</li> <li>Hugging Face: T5-base Product Review Model</li> <li>Google Colab: Try Word Embeddings</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/04-transformer-intro/","title":"Transformer Fundamentals","text":""},{"location":"study-path/transformers/04-transformer-intro/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Transformer Architecture Overview</li> <li>2. Transformer Training Paradigm: Pre-training and Fine-tuning</li> <li>3. Tokenization and Embeddings in Transformer Models</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#1-transformer-architecture-overview","title":"1. Transformer Architecture Overview","text":"<p>This lesson introduces the transformer model architecture, emphasizing its structural innovations, key mechanisms, and how it revolutionized NLP by overcoming the limitations of RNNs and LSTMs.</p>"},{"location":"study-path/transformers/04-transformer-intro/#origins-and-significance","title":"Origins and Significance","text":"<ul> <li>Introduced in 2017 via the paper \"Attention Is All You Need\"</li> <li>Replaced sequential RNN/LSTM processing with fully parallel architecture</li> <li>Solved long-range dependency issues and improved training speed</li> <li>Enabled large-scale model training and breakthroughs in language understanding</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#core-components-of-transformer-architecture","title":"Core Components of Transformer Architecture","text":""},{"location":"study-path/transformers/04-transformer-intro/#encoder-decoder-structure","title":"Encoder-Decoder Structure","text":"<ul> <li>Encoder: Converts input text into continuous vector representations capturing context and relationships</li> <li>Decoder: Generates output text from encoder\u2019s processed information</li> <li>Enables tasks like translation, summarization, and question answering</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#attention-mechanisms","title":"Attention Mechanisms","text":"<ul> <li>Self-Attention: Weighs each word relative to others to build context-aware representations</li> <li>Scaled Dot-Product Attention: Computes dot products, scales scores, and applies softmax</li> <li>Multi-Head Attention: Uses multiple heads to capture diverse semantic/syntactic patterns</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#positional-encoding","title":"Positional Encoding","text":"<ul> <li>Compensates for lack of inherent word order in attention-only models</li> <li>Adds position-based signals to token embeddings</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#feed-forward-network-layer-normalization","title":"Feed-Forward Network &amp; Layer Normalization","text":"<ul> <li>Feed-Forward Network: Applies non-linear transformations to extract high-level features</li> <li>Layer Normalization: Stabilizes training by normalizing outputs between layers</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#full-encoder-and-decoder-block","title":"Full Encoder and Decoder Block","text":"<ul> <li>Composed of stacked layers with:</li> <li>Multi-head attention</li> <li>Feed-forward networks</li> <li>Layer normalization</li> <li>Decoder includes additional encoder-decoder attention to align output generation</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#real-world-application-example","title":"Real-World Application Example","text":"<p>Abstractive Question Answering</p> <ul> <li>Input: Paragraph + Question</li> <li>Encoder: Processes both into contextual embeddings</li> <li>Decoder: Generates an answer from the learned representation</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Transformers enabled scalable, parallel NLP processing</li> <li>Encoder-decoder architecture allows diverse tasks</li> <li>Attention mechanisms are key to understanding global context</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/04-transformer-intro/#2-transformer-training-paradigm-pre-training-and-fine-tuning","title":"2. Transformer Training Paradigm: Pre-training and Fine-tuning","text":"<p>This lesson outlines the two-phase training process of transformer models\u2014pre-training and fine-tuning\u2014contrasting it with traditional ML workflows.</p>"},{"location":"study-path/transformers/04-transformer-intro/#training-structure-overview","title":"Training Structure Overview","text":"<ul> <li>Pre-training: General language learning from large unlabeled datasets</li> <li>Fine-tuning: Task-specific adaptation using labeled datasets</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#pre-training-phase","title":"Pre-training Phase","text":"<ul> <li>Learns grammar, context, word relationships, and long-range dependencies</li> <li>Massive-scale unsupervised training</li> <li>\ud83d\udd01 Analogy: Like learning music theory before mastering a genre</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#fine-tuning-phase","title":"Fine-tuning Phase","text":"<ul> <li>Adapts pre-trained models to tasks like NER, translation, QA, etc.</li> <li>Requires smaller supervised datasets</li> <li>Leverages transfer learning</li> <li>\ud83d\udd01 Analogy: Like a trained pianist specializing in jazz</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#combined-workflow","title":"Combined Workflow","text":"<ol> <li>Step 1: Pre-training</li> <li>Random initialization \u2192 trained on general data</li> <li>Step 2: Fine-tuning</li> <li>Task-specific data \u2192 adapted for downstream performance</li> </ol>"},{"location":"study-path/transformers/04-transformer-intro/#real-world-considerations","title":"Real-world Considerations","text":"<ul> <li>Pre-training requires huge compute and data (done by orgs like Google, OpenAI)</li> <li>Most use pre-trained models and fine-tune</li> <li>Full pre-training is rare unless:</li> <li>You work with proprietary, underrepresented, or specialized domains (e.g., legal, clinical)</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#key-takeaways_1","title":"Key Takeaways","text":"<ul> <li>Pre-training + fine-tuning is the standard approach in NLP</li> <li>Enables rapid model deployment with high performance</li> <li>Specialized domains may benefit from custom pre-training</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/04-transformer-intro/#3-tokenization-and-embeddings-in-transformer-models","title":"3. Tokenization and Embeddings in Transformer Models","text":"<p>This lesson covers how transformers process raw text into vector representations using tokenization and embeddings.</p>"},{"location":"study-path/transformers/04-transformer-intro/#tokenization","title":"Tokenization","text":""},{"location":"study-path/transformers/04-transformer-intro/#purpose","title":"Purpose","text":"<ul> <li>Breaks text into smaller units called tokens</li> <li>Translates natural language into numerical input (token IDs)</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#types-of-tokenization","title":"Types of Tokenization","text":"<ul> <li>Word-level: One token per word; suffers from OOV (out-of-vocabulary) issues</li> <li>Character-level: Every character is a token; leads to longer sequences</li> <li>Subword-level (common): Breaks unknown words into known parts (e.g., Byte-Pair Encoding)</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#workflow","title":"Workflow","text":"<ol> <li>Breaks text into tokens</li> <li>Maps tokens to IDs using a predefined vocabulary</li> <li>Feeds IDs into the transformer model</li> </ol>"},{"location":"study-path/transformers/04-transformer-intro/#embeddings","title":"Embeddings","text":""},{"location":"study-path/transformers/04-transformer-intro/#purpose_1","title":"Purpose","text":"<ul> <li>Convert token IDs into high-dimensional dense vectors</li> <li>Capture meaning and contextual usage of tokens</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#key-concepts","title":"Key Concepts","text":"<ul> <li>Embeddings are context-aware (e.g., \"bank\" in finance vs. riverbank)</li> <li>Contextual embeddings change based on surrounding text</li> <li>Learned during pre-training</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#example","title":"Example","text":"<p>```text Sentence 1: She picked a rose. Sentence 2: The sun rose early.</p>"},{"location":"study-path/transformers/04-transformer-intro/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>Attention Is All You Need (Original Transformer Paper)</li> <li>The Illustrated Transformer by Jay Alammar</li> <li>Hugging Face Transformers Documentation</li> <li>Google Colab: Transformer Architecture Notebook</li> <li>Hugging Face: T5-base Model (Amazon Product Reviews)</li> <li>Hugging Face: diabloGPT Instruction Model</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/05-popular-transformer-models/","title":"Transformer Architectures Study Hub","text":""},{"location":"study-path/transformers/05-popular-transformer-models/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. BERT: Encoder-Only Transformer Architecture</li> <li>2. Transformer &amp; GPT Evolution</li> <li>3. T5: Text-To-Text Transfer Transformer</li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#1-bert-encoder-only-transformer-architecture","title":"1. BERT: Encoder-Only Transformer Architecture","text":"<p>BERT (Bidirectional Encoder Representations from Transformers) revolutionized NLP in 2018 by introducing a bidirectional, encoder-only architecture designed for deep contextual understanding of language. This section explores BERT\u2019s structure, training strategy, practical applications, and the latest advancements in its ecosystem.</p>"},{"location":"study-path/transformers/05-popular-transformer-models/#model-overview","title":"Model Overview","text":""},{"location":"study-path/transformers/05-popular-transformer-models/#key-characteristics","title":"Key Characteristics","text":"<ul> <li> <p>Bidirectional   BERT reads text in both directions (left-to-right and right-to-left) simultaneously to capture full context.</p> </li> <li> <p>Encoder-Only Architecture   Built entirely on stacked encoders with self-attention mechanisms.   Optimized for understanding, not generating, text.</p> </li> <li> <p>Representations   Learns dense vector embeddings that reflect token meaning in context.</p> </li> <li> <p>Transformer-Based   Leverages the original transformer architecture\u2014only the encoder side.</p> </li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#pre-training-strategy","title":"Pre-training Strategy","text":""},{"location":"study-path/transformers/05-popular-transformer-models/#datasets","title":"Datasets","text":"<ul> <li>English Wikipedia  </li> <li>10,000+ unpublished English books  </li> <li>Total: Over 3 billion words</li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#pre-training-objectives","title":"Pre-training Objectives","text":"<ul> <li> <p>Masked Language Modeling (MLM)   Randomly masks 15% of tokens; the model must predict them using surrounding context.   Enables deep semantic and syntactic comprehension.</p> </li> <li> <p>Next Sentence Prediction (NSP)   Trains BERT to classify whether one sentence follows another.   Aids understanding of inter-sentence relationships.   Later models (e.g., RoBERTa) removed this due to limited benefit.</p> </li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#fine-tuning-applications","title":"Fine-tuning Applications","text":""},{"location":"study-path/transformers/05-popular-transformer-models/#text-classification","title":"Text Classification","text":"<ul> <li>Sentiment analysis, spam detection, topic categorization  </li> <li>Produces a single class label from the encoded text</li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#named-entity-recognition-ner","title":"Named Entity Recognition (NER)","text":"<ul> <li>Identifies token-level entities (e.g., people, dates, organizations)  </li> <li>BERT's contextual awareness improves accuracy in boundary detection</li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#extractive-question-answering","title":"Extractive Question Answering","text":"<ul> <li>Extracts answers directly from a provided context passage  </li> <li>Predicts start and end token positions  </li> <li>Used in customer service, document retrieval</li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#semantic-similarity","title":"Semantic Similarity","text":"<ul> <li>Produces embeddings for entire sentences or passages  </li> <li>Used in:</li> <li>Duplicate detection  </li> <li>Paraphrase recognition  </li> <li>Semantic search  </li> <li>Vector-based retrieval systems</li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#bert-model-variants","title":"BERT Model Variants","text":"Model Parameters Notes BERT-Base ~110M 12 layers, 12 heads, 768 hidden units BERT-Large ~340M 24 layers, 16 heads, 1024 hidden units DistilBERT ~66M Lightweight version by Hugging Face RoBERTa ~125M+ No NSP, trained longer, dynamic masking (Meta) ALBERT ~12M\u2013223M Weight-sharing, efficient training (Google Research) DeBERTa Varies Disentangled attention and enhanced position embeddings (Microsoft)"},{"location":"study-path/transformers/05-popular-transformer-models/#latest-developments-as-of-2025","title":"Latest Developments (as of 2025)","text":"<ul> <li>BERT is foundational for retrieval-augmented generation (RAG) and embedding-based search systems.</li> <li>Multilingual BERT (mBERT) supports 100+ languages.</li> <li>BERT encoders are commonly paired with large decoders like GPT-4o for hybrid retrieval-generation systems.</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/05-popular-transformer-models/#2-transformer-gpt-evolution","title":"2. Transformer &amp; GPT Evolution","text":""},{"location":"study-path/transformers/05-popular-transformer-models/#gpt-45-orion","title":"GPT-4.5 (\u201cOrion\u201d)","text":"<ul> <li>Released: Feb 27, 2025</li> <li>Enhanced instruction-following, fewer hallucinations</li> <li>API &amp; ChatGPT Pro access</li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#gpt-41-family","title":"GPT-4.1 Family","text":"<ul> <li>Released: April 14, 2025</li> <li>Includes mini/nano variants supporting 1M-token context</li> <li>More efficient than GPT-4o</li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#reasoning-models-o1-o3-mini-o4-mini","title":"Reasoning Models (o1, o3-mini, o4-mini)","text":"<ul> <li>Optimized for logic, math, and science</li> <li>o3-mini and o4-mini include multimodal chain-of-thought support</li> <li>Ideal for autonomous agents and structured tool use</li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#gpt-5-expected-august-2025","title":"GPT-5 (Expected August 2025)","text":"<ul> <li>Will include reasoning from o3</li> <li>Multimodal + open access discussions ongoing</li> <li>Expected to set a new benchmark for general-purpose AI</li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#why-it-matters","title":"Why It Matters","text":"<ul> <li>Shift from scaling parameters to scaling reasoning</li> <li>GPT-4.5/5 marks evolution toward modular, low-latency, high-accuracy models</li> </ul> Model Category Architecture Strengths Use Cases GPT\u20114.5 Instructional GPT Decoder-only Prompt-following, fewer hallucinations General NLP, coding, chatbots GPT\u20114.1 mini Efficient GPT Decoder-only 1M context, fast inference Coding, RAG o3-mini Reasoning LLM Decoder-only Logic + math + tool use Agents, science tasks GPT\u20115 Unified Multi-module Multimodal, reasoning-first Enterprise AI, general AI <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/05-popular-transformer-models/#3-t5-text-to-text-transfer-transformer","title":"3. T5: Text-To-Text Transfer Transformer","text":"<p>T5 reframes every NLP problem as a text-to-text task (e.g., input: \u201cTranslate English to German: How are you?\u201d \u2192 output: \u201cWie geht es dir?\u201d). This unified approach enables a wide range of applications across translation, QA, summarization, and more.</p>"},{"location":"study-path/transformers/05-popular-transformer-models/#model-overview_1","title":"Model Overview","text":"<ul> <li>Encoder-decoder transformer with BERT-style encoding + GPT-style generation</li> <li>Flexible task control via text prefixes (e.g., \u201csummarize:\u201d, \u201ctranslate:\u201d)</li> <li>First model to fully embrace text-to-text multitask learning</li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#pre-training-c4-dataset-fill-in-the-blank-generation","title":"Pre-training: C4 Dataset + Fill-in-the-Blank Generation","text":"<ul> <li>Uses a corrupt-and-reconstruct pre-training objective</li> <li>Learns both contextual understanding and sequence generation</li> <li>Trained on C4 (Colossal Cleaned Crawled Corpus)</li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#key-use-cases","title":"Key Use Cases","text":"<ul> <li>Translation: Understands bidirectional input, generates fluent target text</li> <li>Summarization: Converts long passages into concise summaries</li> <li>Question Answering: Context-aware, generative answers</li> <li>Keyword Generation: Contextual phrase extraction</li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#product-evolution-table","title":"Product Evolution Table","text":"Model Architecture Strengths Use Cases Developer T5-Base Encoder-Decoder Multitask learning, flexible Translation, QA, summarization Google AI mT5 Encoder-Decoder Multilingual model (100+ langs) Cross-lingual NLP Google AI FLAN-T5 Enc-Dec + Tuning Instruction tuning Zero-shot &amp; few-shot NLP Google Research UL2 Encoder-Decoder Supports multiple objective modes General-purpose transformer Google DeepMind Gemini 1.5 Multimodal Unified vision + text + code Multimodal reasoning, generation Google DeepMind"},{"location":"study-path/transformers/05-popular-transformer-models/#takeaways","title":"Takeaways","text":"<ul> <li>T5 demonstrates the power of a unified framework in solving diverse NLP tasks</li> <li>Its design has influenced instruction-tuned and multimodal model families</li> <li>Continues to power a range of Google products and NLP pipelines</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/05-popular-transformer-models/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google Research)</li> <li>RoBERTa: A Robustly Optimized BERT Pretraining Approach (Meta AI)</li> <li>DeBERTa: Decoding-enhanced BERT with Disentangled Attention (Microsoft)</li> <li>DistilBERT by Hugging Face (Model Page)</li> <li>mBERT: Multilingual BERT (Google AI)</li> <li>T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</li> <li>FLAN-T5 Instruction-Tuned Models (Google Research)</li> <li>UL2: Unified Language Learning</li> <li>Gemini 1.5 Model Overview (Google DeepMind)</li> <li>GPT-4.5 and GPT-5 Updates (OpenAI Blog)</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/06-using-transformers/","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Course Overview</li> <li>Key Concepts</li> <li>Tokenizer and Embeddings</li> <li>Masked Language Modeling</li> <li>Semantic Search Engine</li> <li>Model Evolution Table</li> </ul>"},{"location":"study-path/transformers/06-using-transformers/#course-overview","title":"Course Overview","text":"<p>This section focuses on transitioning from theoretical knowledge of transformer models to their practical implementation and engineering components, emphasizing real-world applications such as semantic search and embedding usage.</p> <ul> <li>Prepares learners to apply transformer embeddings for NLP tasks</li> <li>Covers tokenization, embeddings, model internals, and downstream tasks</li> <li>Includes practical hands-on coding with Hugging Face Transformers and PyTorch</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/06-using-transformers/#key-concepts","title":"Key Concepts","text":""},{"location":"study-path/transformers/06-using-transformers/#transformer-engineering-focus","title":"Transformer Engineering Focus","text":"<ul> <li>Embeddings: Represent words/sentences as dense vectors for downstream processing</li> <li>Tokenization: Converts raw text to token IDs; includes handling special tokens</li> <li>Attention Mechanism: Key to contextual representation in transformers</li> <li>Model Inputs: Includes token IDs, attention masks, and token type IDs</li> <li>Sentence Transformers: Fine-tuned models for capturing sentence-level semantics</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/06-using-transformers/#tokenizer-and-embeddings","title":"Tokenizer and Embeddings","text":""},{"location":"study-path/transformers/06-using-transformers/#tokenization-pipeline","title":"Tokenization Pipeline","text":"<ul> <li>Tokenizers split sentences into subword tokens</li> <li>Maintains a vocabulary of ~30k+ tokens</li> <li>Returns token IDs, attention masks, and token type IDs</li> <li>Important to use model-specific tokenizers for consistency</li> </ul>"},{"location":"study-path/transformers/06-using-transformers/#try-it-yourself","title":"Try It Yourself","text":"<p>Explore and run the notebook interactively using Google Colab:</p> <p>Open the Notebook in Colab</p> <p></p>"},{"location":"study-path/transformers/06-using-transformers/#embeddings","title":"Embeddings","text":"<ul> <li>Token IDs are converted to high-dimensional vectors</li> <li>Two key outputs:</li> <li>Last Hidden State: Embeddings for individual tokens (shape: seq_len \u00d7 hidden_dim)</li> <li>Pooled Output: Embedding for the entire sequence, used in classification</li> </ul>"},{"location":"study-path/transformers/06-using-transformers/#try-it-yourself_1","title":"Try It Yourself","text":"<p>You can run and explore the notebook directly in Google Colab:</p> <p>Open the Notebook in Colab</p> <p></p> <p>Back to Top</p>"},{"location":"study-path/transformers/06-using-transformers/#semantic-distance","title":"Semantic Distance","text":"<ul> <li>Embeddings compared using cosine similarity</li> <li>Allows words with different meanings (e.g., \"fly\") to be distinguished contextually</li> </ul>"},{"location":"study-path/transformers/06-using-transformers/#masked-language-modeling","title":"Masked Language Modeling","text":"<ul> <li>Pretraining task for models like BERT</li> <li>Random tokens replaced with <code>[MASK]</code> and predicted by the model</li> <li>Output logits converted to probabilities via softmax</li> <li>Used to help the model build a strong language understanding foundation</li> </ul>"},{"location":"study-path/transformers/06-using-transformers/#example","title":"Example","text":"<ul> <li>Input: <code>\"I want to [MASK] pizza for tonight\"</code></li> <li>Output: <code>\"have\"</code>, <code>\"get\"</code>, <code>\"eat\"</code>, <code>\"make\"</code>, <code>\"order\"</code> as top predictions</li> </ul>"},{"location":"study-path/transformers/06-using-transformers/#try-it-yourself_2","title":"Try It Yourself","text":"<p>You can experiment with the code by opening the notebook in Google Colab:</p> <p>Open the Notebook in Colab</p> <p></p> <p>Back to Top</p>"},{"location":"study-path/transformers/06-using-transformers/#semantic-search-engine","title":"Semantic Search Engine","text":""},{"location":"study-path/transformers/06-using-transformers/#goal","title":"Goal","text":"<p>Build a semantic search engine that finds the most relevant document to a query based on meaning, not keyword match.</p>"},{"location":"study-path/transformers/06-using-transformers/#tools-dataset","title":"Tools &amp; Dataset","text":"<ul> <li>Dataset: Multi-News (2000 article summaries)</li> <li>Model: SentenceTransformer for lightweight sentence embeddings (384-dim)</li> <li>Libraries: Hugging Face Transformers, PyTorch, Pandas</li> </ul>"},{"location":"study-path/transformers/06-using-transformers/#process","title":"Process","text":"<ul> <li>Embed all documents once</li> <li>Embed user\u2019s query</li> <li>Compute cosine similarity between query and all document embeddings</li> <li>Retrieve top-k relevant results using <code>torch.topk</code></li> </ul>"},{"location":"study-path/transformers/06-using-transformers/#example-queries","title":"Example Queries","text":"<ul> <li>\"Artificial Intelligence\": returned AI-related articles</li> <li>\"Natural Disasters\": returned disaster-related summaries</li> <li>\"Law Enforcement\", \"Politics\": worked as expected</li> </ul>"},{"location":"study-path/transformers/06-using-transformers/#try-it-yourself_3","title":"Try It Yourself","text":"<p>Give it a try by opening the interactive Google Colab notebook below:</p> <p>Open the Notebook in Colab</p> <p></p> <p>Back to Top</p>"},{"location":"study-path/transformers/06-using-transformers/#model-evolution-table","title":"Model Evolution Table","text":"Model Name Category Architecture Strengths Ideal Use Cases Latest Version Info BERT Encoder-only Transformer Bidirectional context, strong understanding Text classification, Q&amp;A, embedding generation BERT-Base / BERT-Large GPT Decoder-only Transformer Text generation, instruction following Chatbots, creative writing, code generation GPT-4o (June 2024) T5 Encoder-Decoder Transformer Unified text-to-text architecture Translation, summarization, Q&amp;A T5.1.1, Flan-T5 Gemini Multi-modal Transformer + Vision + Memory Text + image processing, powerful LLM+VLM hybrid Multi-modal tasks, agentic reasoning Gemini 1.5 (June 2025) SentenceTransformer Encoder-only Siamese / Bi-encoder Transformer Sentence similarity, semantic search Embedding generation, retrieval, clustering <code>all-MiniLM-L6-v2</code> <p>Back to Top</p>"},{"location":"study-path/transformers/06-using-transformers/#references-further-exploration","title":"References &amp; Further Exploration","text":"<ul> <li>\ud83e\udd17 Hugging Face Models and Tools</li> <li>BERT (bert-base-uncased)</li> <li>GPT-2 (gpt2)</li> <li>T5 (t5-base)</li> <li>SentenceTransformer (all-MiniLM-L6-v2)</li> <li> <p>T5-based Amazon Product Review Generator by TheFuzzyScientist</p> </li> <li> <p>\ud83d\udcd3 Colab Notebooks (Used in This Module)</p> </li> <li>Tokenizer &amp; Embeddings Colab</li> <li>Masked Language Modeling (MLM) Demo</li> <li>Semantic Search with Transformers</li> <li> <p>Tokenizer Pipeline Walkthrough</p> </li> <li> <p>\ud83d\udcda Further Reading</p> </li> <li>Attention Is All You Need (Vaswani et al.)</li> <li>The Illustrated Transformer (Jay Alammar)</li> <li>Hugging Face Transformers Documentation</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/07-real-world-scenario-llm/","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Course Overview</li> <li>Part 1: NLP &amp; Transformer Fundamentals</li> <li>Part 2: Practical LLM Applications</li> <li>Model Comparison Summary</li> <li>Key Takeaways</li> <li>References &amp; Further Exploration</li> </ul>"},{"location":"study-path/transformers/07-real-world-scenario-llm/#course-overview","title":"Course Overview","text":"<p>This course is a hands-on introduction to transformer-based language models, combining theoretical foundations with practical implementations. The curriculum covers BERT, GPT, and T5 models, including their use in real-world NLP tasks.</p>"},{"location":"study-path/transformers/07-real-world-scenario-llm/#part-1-nlp-transformer-fundamentals","title":"Part 1: NLP &amp; Transformer Fundamentals","text":""},{"location":"study-path/transformers/07-real-world-scenario-llm/#historical-phases-of-nlp","title":"Historical Phases of NLP","text":"<ul> <li>Rule-Based Systems: Manually defined linguistic rules</li> <li>Statistical Methods: Word co-occurrence and probabilistic models</li> <li>Machine Learning: Feature-based methods (e.g., SVM, Naive Bayes)</li> <li>Deep Learning: Dense vector embeddings and neural models</li> </ul>"},{"location":"study-path/transformers/07-real-world-scenario-llm/#core-transformer-concepts","title":"Core Transformer Concepts","text":"<ul> <li>Attention Mechanism: Enables global contextual representation</li> <li>Tokenization: Breaks text into subwords with positional info</li> <li>Encoder-Decoder: Structure used in models like T5, BART</li> <li>Fine-tuning: Adjusts pretrained models for specific tasks</li> </ul> <p>\ud83d\udd17 Reference: Illustrated Transformer \u2013 Jay Alammar</p>"},{"location":"study-path/transformers/07-real-world-scenario-llm/#part-2-practical-llm-applications","title":"Part 2: Practical LLM Applications","text":""},{"location":"study-path/transformers/07-real-world-scenario-llm/#bert-extractive-question-answering","title":"\ud83d\udfe2 BERT \u2013 Extractive Question Answering","text":"<ul> <li>Extracts an answer span from context using start and end logits</li> <li>Ideal for closed-domain QA</li> <li>Handles context chunks using stride</li> </ul> <p>\ud83d\udc49 Open in Colab </p> <p>\ud83d\udd17 Model: bert-base-uncased \ud83d\udcc4 Paper: BERT: Pre-training of Deep Bidirectional Transformers</p>"},{"location":"study-path/transformers/07-real-world-scenario-llm/#gpt-instruction-following-generation","title":"\ud83d\udd35 GPT \u2013 Instruction-Following Generation","text":"<ul> <li>Trained using causal language modeling</li> <li>Uses instruction + response prompts</li> <li>Fine-tuned with Open-Instruct dataset</li> </ul> <p>\ud83d\udc49 Open in Colab </p> <p>\ud83d\udd17 Model: DiabloGPT on Hugging Face \ud83d\udcc4 Paper: GPT-2 \ud83d\udcc4 Dataset: Open-Instruct</p>"},{"location":"study-path/transformers/07-real-world-scenario-llm/#t5-text-to-text-product-review-generation","title":"\ud83d\udd34 T5 \u2013 Text-to-Text Product Review Generation","text":"<ul> <li>Treats all tasks as text-to-text (e.g., <code>summarize:</code> or <code>translate:</code>)</li> <li>Pretrained on C4 corpus with span corruption</li> <li>Ideal for summarization, QA, translation, and generation</li> </ul> <p>\ud83d\udc49 Open in Colab </p> <p>\ud83d\udd17 Model: T5-base, Amazon Review Model \ud83d\udcc4 Paper: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</p>"},{"location":"study-path/transformers/07-real-world-scenario-llm/#model-comparison-summary","title":"Model Comparison Summary","text":"Model Architecture Directionality Pretraining Task Ideal Use Cases Limitations BERT Encoder-only Bidirectional Masked Language Modeling QA, classification, embeddings 512-token limit GPT-2 Decoder-only Unidirectional Causal Language Modeling Instruction generation, chatbots No bidirectional context T5 Encoder-Decoder Bi/Uni (input/output) Span corruption (text-to-text) Summarization, QA, translation Needs task-specific prompt Gemini Multi-modal Flexible MoE + RLHF + VLM Multimodal generation, reasoning Closed-source"},{"location":"study-path/transformers/07-real-world-scenario-llm/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Leverage pretrained models to reduce time and cost</li> <li>Use token chunking and stride for input limits</li> <li>Even small models like GPT-2 perform well when fine-tuned</li> <li>T5\u2019s text-to-text design enables flexibility across tasks</li> </ul>"},{"location":"study-path/transformers/07-real-world-scenario-llm/#references-further-exploration","title":"References &amp; Further Exploration","text":""},{"location":"study-path/transformers/07-real-world-scenario-llm/#foundational-papers","title":"\ud83e\udde0 Foundational Papers","text":"<ul> <li>Attention Is All You Need</li> <li>BERT: Pre-training of Deep Bidirectional Transformers</li> <li>GPT-2</li> <li>T5</li> </ul>"},{"location":"study-path/transformers/07-real-world-scenario-llm/#hugging-face-models","title":"\ud83e\udd17 Hugging Face Models","text":"<ul> <li>bert-base-uncased</li> <li>gpt2</li> <li>t5-base</li> <li>DiabloGPT</li> <li>Amazon T5 Review Model</li> </ul>"},{"location":"study-path/transformers/07-real-world-scenario-llm/#colab-notebooks","title":"\ud83e\uddea Colab Notebooks","text":"<ul> <li>BERT QA</li> <li>GPT Instruction Tuning</li> <li>T5 Product Review Generator</li> </ul>"},{"location":"study-path/transformers/08-llm-intro/","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Course Overview</li> <li>What is a Large Language Model?</li> <li>Decoder-Only Architecture</li> <li>Chat Templates &amp; Structured Inputs</li> <li>Model Selection on Hugging Face</li> <li>Code Demonstration: TinyLlama</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers/08-llm-intro/#course-overview","title":"Course Overview","text":"<p>This section explores Large Language Models (LLMs) built on the transformer architecture, their training procedures, deployment challenges, and how they are applied in real-world interactive systems. The goal is to bridge conceptual understanding with hands-on implementation.</p> <ul> <li>Core Focus:</li> <li>Decoder-only transformer models</li> <li>Tokenization &amp; input formatting</li> <li>Reinforcement Learning from Human Feedback (RLHF)</li> <li>Chat templates</li> <li>Model selection and generation parameters</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/08-llm-intro/#what-is-a-large-language-model","title":"What is a Large Language Model?","text":"<p>LLMs refer to powerful NLP models capable of generating complex, human-like text. They\u2019re built using decoder-only transformer architectures and trained at scale using massive datasets.</p> <ul> <li>Scale:</li> <li>Models like LLaMA-3 and GPT-4 have up to 70+ billion parameters.</li> <li>Small LLMs (e.g., 2\u20137B) are optimized for consumer hardware.</li> <li>Architecture:</li> <li>Modern LLMs are generally decoder-only models.</li> <li>Capabilities:</li> <li>High factual recall</li> <li>Scalable deployment</li> <li>Robust contextual understanding</li> <li>Challenges:</li> <li>Hallucination</li> <li>Deployment complexity</li> <li>High compute requirements</li> </ul> <p>\ud83e\udde0 Despite limitations, they\u2019ve surpassed average human factual knowledge.</p> <p>Back to Top</p>"},{"location":"study-path/transformers/08-llm-intro/#decoder-only-architecture","title":"Decoder-Only Architecture","text":""},{"location":"study-path/transformers/08-llm-intro/#key-properties","title":"Key Properties","text":"<ul> <li>LLMs process inputs as a single concatenated sequence.</li> <li>Interaction is simulated using autoregession, where the model predicts the next token.</li> <li>Requires clever input formatting to mimic input-output behavior.</li> </ul>"},{"location":"study-path/transformers/08-llm-intro/#fine-tuning-techniques","title":"Fine-tuning Techniques","text":"<ul> <li>Supervised Fine-Tuning: Uses input-response pairs to guide expected outputs.</li> <li>Reinforcement Learning from Human Feedback (RLHF):</li> <li>Multiple responses are generated.</li> <li>Human annotators rank responses.</li> <li>Used to improve contextual accuracy and helpfulness.</li> </ul> <p>\ud83d\udcca Illustration:</p> <p> Source: Jay Alammar\u2019s GPT2 visual guide</p>"},{"location":"study-path/transformers/08-llm-intro/#applications","title":"Applications:","text":"<ul> <li>Chatbots  </li> <li>Code generation  </li> <li>Instruction following  </li> <li>Document summarization</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/08-llm-intro/#chat-templates-structured-inputs","title":"Chat Templates &amp; Structured Inputs","text":"<p>LLMs simulate dialogue using chat templates that structure user-assistant messages.</p>"},{"location":"study-path/transformers/08-llm-intro/#input-structure","title":"Input Structure","text":"<ul> <li>A \"conversation\" is a series of messages with:</li> <li><code>role</code>: Identifies speaker (user/assistant)</li> <li><code>content</code>: Message text</li> </ul>"},{"location":"study-path/transformers/08-llm-intro/#model-specific-template-examples","title":"Model-specific Template Examples","text":"Model Structure Type Special Tokens Role Awareness Instruction Capable Blenderbot Basic concat \u274c \u274c \u274c Mistral Instruction tokens \u2705 \u26a0\ufe0f (Partial) \u2705 Gemma Turn-based format \u2705\u2705 \u2705 \u2705\u2705 LLaMA 3 Header tokens \u2705\u2705\u2705 \u2705 \u2705\u2705\u2705 <p>\ud83e\udde9 These templates are essential during fine-tuning to teach models interaction patterns.</p> <p>Back to Top</p>"},{"location":"study-path/transformers/08-llm-intro/#model-selection-on-hugging-face","title":"Model Selection on Hugging Face","text":"<p>\ud83d\udee0\ufe0f Choosing the right LLM impacts performance, cost, and resource needs.</p>"},{"location":"study-path/transformers/08-llm-intro/#what-to-look-for","title":"What to Look For:","text":"<ul> <li>Model Family: LLaMA, Mistral, Phi, Gemma, etc.</li> <li>Size (Parameters):</li> <li>Small: 2B\u20137B</li> <li>Medium: 13B\u201334B</li> <li>Large: 70B+</li> <li>Instruction-Following:</li> <li>Look for <code>instruct</code> or <code>chat</code> variants</li> <li>Context Length:</li> <li>Defined via <code>max_position_embeddings</code> in <code>config.json</code></li> <li>Affects how much prompt+response can be handled</li> </ul> <p>\ud83d\udca1 Hugging Face Model Hub: \ud83d\udd17 Browse Models</p> <p>Back to Top</p>"},{"location":"study-path/transformers/08-llm-intro/#code-demonstration-tinyllama","title":"Code Demonstration: TinyLlama","text":"<p>\ud83d\udc49 Open in Colab </p> <p>We explore TinyLlama to demonstrate basic generation and parameter tuning.</p>"},{"location":"study-path/transformers/08-llm-intro/#workflow","title":"Workflow","text":"<ul> <li>Load model + tokenizer  </li> <li>Prepare chat messages using templates  </li> <li>Encode as tokens  </li> <li>Generate response  </li> <li>Decode and analyze output  </li> </ul>"},{"location":"study-path/transformers/08-llm-intro/#key-generation-parameters","title":"Key Generation Parameters","text":"Parameter Description <code>max_new_tokens</code> Limits length of generated response <code>temperature</code> Controls creativity/randomness (higher = more) <code>top_p</code> Nucleus sampling: restricts to top % of prob. <code>do_sample</code> Enables randomness in output <p>\ud83d\udfe2 Temperature Examples:</p> <ul> <li>1.0 \u2192 Creative, varied responses  </li> <li>0.1 \u2192 Deterministic, factual outputs  </li> </ul> <p>\ud83d\udcce Prompt token count impacts total input length (important for context fitting).</p> <p>Back to Top</p>"},{"location":"study-path/transformers/08-llm-intro/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>\ud83d\udcdc Attention Is All You Need (Vaswani et al.)</li> <li>\ud83e\udd17 Hugging Face Transformers Documentation</li> <li>\ud83d\uddbc\ufe0f Jay Alammar\u2019s Illustrated Transformer</li> <li>\ud83e\udde0 LLaMA 3 on Hugging Face</li> <li>\ud83d\udcd8 RLHF Explained \u2013 Hugging Face Blog</li> <li>\ud83d\udcc4 OpenAI: ChatGPT Fine-tuning Guide</li> <li>\ud83d\udcda Gemma Tokenizer Guide \u2013 Google</li> <li>\ud83d\udd2c Microsoft Phi Models on Hugging Face</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/","title":"9. Preparing LLMs","text":""},{"location":"study-path/transformers/09-llm-prep/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Comprehensive Dive into Sequence Length</li> <li>Token Counts: Practical Intuition &amp; Impact</li> <li>Precision Matters: Numerical Precision in Training</li> <li>Navigating GPU Selection: A Guide to Hardware Platform</li> <li>Practice Fundamentals: Most Basic Form of Training LLMs</li> <li>Practice Fundamentals Part 2: Most Basic Form of Training LLMs</li> <li>Practice Fundamentals Part 3: Most Basic Form of Training LLMs</li> </ul>"},{"location":"study-path/transformers/09-llm-prep/#comprehensive-dive-into-sequence-length","title":"\ud83d\udcd8 Comprehensive Dive into Sequence Length","text":""},{"location":"study-path/transformers/09-llm-prep/#quick-navigation_1","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Understanding Sequence Length </li> <li>Why Sequence Length Matters </li> <li>Hardware Implications </li> <li>Impact on Task Suitability </li> <li>Use Cases: Short vs Long Sequences </li> <li>Guidelines for Choosing Sequence Length </li> <li>References &amp; Further Reading </li> </ul>"},{"location":"study-path/transformers/09-llm-prep/#understanding-sequence-length","title":"Understanding Sequence Length","text":"<p>In this foundational lesson, we examine the concept of sequence length in large language models (LLMs), particularly as it relates to fine-tuning and model design. Sequence length determines how much context a model can consider during training and inference. Once a model is trained with a specific maximum sequence length, this cannot be extended without retraining.</p>"},{"location":"study-path/transformers/09-llm-prep/#key-concepts","title":"Key Concepts","text":"<ul> <li>Sequence Length defines the number of tokens a model can process at once.  </li> <li>This parameter is fixed after pretraining.  </li> <li>You can feed shorter inputs into a longer-trained model, but not vice versa.  </li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#why-sequence-length-matters","title":"Why Sequence Length Matters","text":""},{"location":"study-path/transformers/09-llm-prep/#fixed-architecture-constraint","title":"Fixed Architecture Constraint","text":"<ul> <li>Pretraining fixes the maximum window size (e.g., 4K, 8K, 16K tokens).  </li> <li>Longer contexts require higher computational resources and memory.  </li> </ul>"},{"location":"study-path/transformers/09-llm-prep/#unidirectional-compatibility","title":"Unidirectional Compatibility","text":"<ul> <li>Models trained with longer windows can handle shorter inputs effortlessly.  </li> <li>Short-window models cannot be upgraded to handle longer contexts post hoc.  </li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#hardware-implications","title":"Hardware Implications","text":"<p>Sequence length directly impacts training and inference costs:</p> <ul> <li>Longer sequence lengths = higher VRAM requirements </li> <li>Training larger windows is exponentially slower </li> <li>Even inference (e.g., chatbots) demands more memory with longer inputs  </li> </ul> <p>Modern workarounds:</p> <ul> <li>Sparse Attention (e.g., Longformer, BigBird)  </li> <li>Memory-augmented transformers (e.g., Transformer-XL)  </li> </ul> <p>These techniques allow partial mitigation of the cost explosion from large contexts.</p> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#impact-on-task-suitability","title":"Impact on Task Suitability","text":"<p>The sequence length determines the range and complexity of tasks that LLMs can solve. Below is a breakdown:</p>"},{"location":"study-path/transformers/09-llm-prep/#short-sequences-128-512-tokens","title":"Short Sequences (128 - 512 tokens)","text":"<ul> <li>\u2705 Sentiment Analysis  </li> <li>\u2705 Language Detection  </li> <li>\u2705 Named Entity Recognition  </li> </ul> <p>Advantages:</p> <ul> <li>Faster training  </li> <li>Lower compute overhead  </li> <li>Context is usually local and easily chunkable  </li> </ul>"},{"location":"study-path/transformers/09-llm-prep/#long-sequences-2048-tokens","title":"Long Sequences (2048+ tokens)","text":"<ul> <li>\u2705 Long-form QA  </li> <li>\u2705 Document Summarization  </li> <li>\u2705 Scriptwriting / Story Generation  </li> <li>\u2705 Multi-turn Dialogue Systems  </li> </ul> <p>Advantages:</p> <ul> <li>Maintains global context  </li> <li>Enables high-fidelity content generation  </li> <li>Suitable for documents, books, and extended chat history  </li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#use-cases-short-vs-long-sequences","title":"Use Cases: Short vs Long Sequences","text":""},{"location":"study-path/transformers/09-llm-prep/#short-sequence-use-cases","title":"Short Sequence Use Cases","text":"<ul> <li>Sentiment Analysis: Determine tone from key phrases  </li> <li>NER: Recognize entities within short contexts  </li> <li>Language Identification: Detect language using just a few words  </li> </ul>"},{"location":"study-path/transformers/09-llm-prep/#long-sequence-use-cases","title":"Long Sequence Use Cases","text":"<ul> <li>Conversational AI: Maintain long-term context across multiple exchanges  </li> <li>Content Generation: Write consistent long-form narratives or reports  </li> <li>Document Understanding: Answer questions or summarize content from full documents  </li> </ul> <p>\ud83d\udd39 These applications demonstrate the practical trade-offs of context length in fine-tuning.</p> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#guidelines-for-choosing-sequence-length","title":"Guidelines for Choosing Sequence Length","text":"Task Type Suggested Sequence Benefits Limitations Classification (Sentiment, NER) 128 - 512 tokens Efficient, fast inference Limited to local context Chatbots / Assistants 2048 - 8192 tokens Maintains conversational coherence Higher cost and latency Summarization 4096 - 16000 tokens Holistic document understanding Truncation risk if too short Code Generation 2048 - 8192 tokens Handles longer code blocks Needs longer memory if multi-file <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>\ud83d\udd17 Attention Is All You Need (Vaswani et al.) </li> <li>\ud83d\udd17 Hugging Face Transformers Documentation </li> <li>\ud83d\udd17 Jay Alammar: Illustrated Transformer </li> <li>\ud83d\udd17 Google Research: Efficient Transformers </li> <li>\ud83d\udd17 OpenAI: Scaling Laws for Neural Language Models </li> <li>\ud83d\udd17 Facebook AI: Long-Range Arena Benchmark </li> <li>\ud83d\udd17 NVIDIA Megatron-LM Documentation </li> <li>\ud83d\udd17 Microsoft DeepSpeed for Long Sequence Training </li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#token-counts-practical-intuition-impact","title":"\ud83d\udcd7 Token Counts: Practical Intuition &amp; Impact","text":""},{"location":"study-path/transformers/09-llm-prep/#quick-navigation_2","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview</li> <li>Tokenization Mechanics</li> <li>Tokenizer Comparisons</li> <li>Vocabulary Size &amp; Token Efficiency</li> <li>Model Comparison Table</li> <li>Colab Demo</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers/09-llm-prep/#overview","title":"Overview","text":"<p>This lesson introduces the practical impact of token counts in generative AI, with hands-on comparisons between various tokenizer behaviors and model capacities. By examining real-world input (e.g., Wikipedia pages), learners gain intuition about:</p> <ul> <li>Sequence length constraints in models like BERT, LLaMA, and Mistral.</li> <li>Vocabulary size trade-offs.</li> <li>Tokenization efficiency and its effect on model performance and training design.</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#tokenization-mechanics","title":"Tokenization Mechanics","text":"<ul> <li>Text tokenization converts raw text into input IDs and attention masks.</li> <li>Input IDs directly affect the maximum context size a model can handle.</li> <li>For example, the phrase <code>\"fuzzy scientist\"</code> gets broken into only 5 tokens by the LLaMA3 tokenizer.</li> </ul>"},{"location":"study-path/transformers/09-llm-prep/#case-study-wikipedia-paragraph-on-whales","title":"Case Study: Wikipedia Paragraph on Whales","text":"<ul> <li>~170 words = ~300 tokens (using LLaMA3).</li> <li>Word-to-token ratio is approximately 1.76x.</li> <li>Demonstrates how even short paragraphs can consume large portions of traditional transformer limits.</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#tokenizer-comparisons","title":"Tokenizer Comparisons","text":""},{"location":"study-path/transformers/09-llm-prep/#same-paragraph-different-tokenizers","title":"Same Paragraph, Different Tokenizers:","text":"<ul> <li>BERT: ~20 more tokens than LLaMA3.</li> <li>Mistral: ~30 more than BERT, ~50 more than LLaMA3.</li> </ul>"},{"location":"study-path/transformers/09-llm-prep/#full-section-1000-tokens","title":"Full Section (~1,000 tokens):","text":"<ul> <li>Fits within LLaMA3 and Mistral (8K\u201332K context sizes).</li> <li>Exceeds BERT\u2019s 512-token limit.</li> </ul>"},{"location":"study-path/transformers/09-llm-prep/#entire-wikipedia-page","title":"Entire Wikipedia Page:","text":"<ul> <li>LLaMA3: ~21,000 tokens</li> <li>Mistral: ~26,000 tokens</li> </ul> <p>\ud83d\udfe2 Mistral fits due to 32K context, but is near capacity.</p> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#vocabulary-size-token-efficiency","title":"Vocabulary Size &amp; Token Efficiency","text":"Model Vocabulary Size Tokens Needed for Wiki Page Notes LLaMA3 128,000 21,000 More efficient; fewer tokens per input Mistral 32,000 26,000 Less efficient but smaller vocab size <ul> <li>Trade-off:</li> <li>Larger vocab \u2192 fewer tokens \u2192 more efficient inference</li> <li> <p>Smaller vocab \u2192 easier pretraining \u2192 more tokens used</p> </li> <li> <p>Tokenizer Strategy:</p> </li> <li>LLaMA3: Breaks input into fewer, more specific tokens.</li> <li>Mistral: Uses more tokens to represent same input.</li> </ul> <p>\ud83d\udccc Takeaway: Vocabulary size directly impacts token efficiency, model generalization, and context window utilization.</p> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#model-comparison-table","title":"Model Comparison Table","text":"Model Directionality Max Context Length Vocab Size Token Efficiency Use Case Fit BERT Encoder-only 512 tokens ~30K \ud83d\udd34 Low QA, embeddings LLaMA3 Decoder-only 8K \u2013 128K tokens 128K \ud83d\udfe2 High Chat, summarization Mistral Decoder-only 32K tokens 32K \ud83d\udfe1 Medium Long-form generation <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#colab-demo","title":"Colab Demo","text":"<p>\ud83d\udc49 Open in Colab </p> <p>This exercise allows you to: - Load LLaMA3, Mistral, and BERT tokenizers - Input arbitrary text (e.g., Wikipedia) and compare token counts - Explore vocabulary-driven tokenization behaviors</p>"},{"location":"study-path/transformers/09-llm-prep/#back-to-top","title":"Back to Top","text":""},{"location":"study-path/transformers/09-llm-prep/#references-further-reading_1","title":"References &amp; Further Reading","text":"<ul> <li>\u201cAttention Is All You Need\u201d \u2013 Vaswani et al. (ArXiv)</li> <li>Hugging Face Transformers Docs</li> <li>Jay Alammar \u2013 Illustrated Transformer</li> <li>LLaMA3 Model Card (Hugging Face)</li> <li>Mistral Model Card (Hugging Face)</li> <li>Google Research on Subword Tokenization</li> <li>NVIDIA LLM Efficiency Resources</li> <li>Facebook AI Blog \u2013 Mistral Architecture Insights</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#precision-matters-numerical-precision-in-training","title":"\ud83d\udcd9 Precision Matters: Numerical Precision in Training","text":""},{"location":"study-path/transformers/09-llm-prep/#quick-navigation_3","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview</li> <li>Numerical Precision in Machine Learning</li> <li>Precision Formats Explained</li> <li>Model Size, Memory, and Precision Trade-offs</li> <li>Hardware Limitations and Use Cases</li> <li>Lower Precision Formats (INT8, 4-bit)</li> <li>Precision vs Speed: Hardware Implications</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers/09-llm-prep/#overview_1","title":"Overview","text":"<p>This lesson explores the role of numerical precision in training and deploying large language models (LLMs). Understanding how floating-point representations affect performance, memory efficiency, and hardware compatibility is crucial when working with multi-billion parameter models.</p> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#numerical-precision-in-machine-learning","title":"Numerical Precision in Machine Learning","text":"<ul> <li>In ML, parameters (weights) are represented as floating-point numbers.</li> <li>Common format: float32 (32-bit), offering high accuracy but high memory cost.</li> <li>Trade-off: higher bit precision = better accuracy but slower training &amp; higher memory.</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#precision-formats-explained","title":"Precision Formats Explained","text":""},{"location":"study-path/transformers/09-llm-prep/#float32-fp32","title":"\ud83d\udd35 Float32 (FP32)","text":"<ul> <li>32 bits per number \u2192 4 bytes</li> <li>High accuracy</li> <li>High memory usage</li> </ul>"},{"location":"study-path/transformers/09-llm-prep/#float16-fp16-mixed-precision","title":"\ud83d\udfe2 Float16 (FP16) / Mixed Precision","text":"<ul> <li>16 bits per number \u2192 2 bytes</li> <li>Slight loss of precision, but enables:</li> <li>Half the memory</li> <li>Double computation speed (if hardware supports)</li> </ul>"},{"location":"study-path/transformers/09-llm-prep/#bf16-brain-floating-point-16","title":"\ud83d\udfe1 BF16 (Brain Floating Point 16)","text":"<ul> <li>Also 16-bit, optimized for machine learning</li> <li>Better gradient/weight representation</li> <li>Widely adopted in modern models</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#model-size-memory-and-precision-trade-offs","title":"Model Size, Memory, and Precision Trade-offs","text":"Precision Bits Bytes 8B Param Model Memory 70B Param Model Memory FP32 32 4 32 GB 280 GB FP16 16 2 16 GB 140 GB INT8 8 1 8 GB 70 GB 4-bit 4 0.5 4 GB 35 GB <p>\ud83e\udde0 Rule of Thumb: For FP16, memory = 2 \u00d7 parameter count in GB.</p> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#hardware-limitations-and-use-cases","title":"Hardware Limitations and Use Cases","text":""},{"location":"study-path/transformers/09-llm-prep/#consumer-gpus","title":"Consumer GPUs","text":"<ul> <li>RTX 4090: 24GB VRAM</li> <li>Can run 8B models in inference mode using FP16</li> <li>Cannot support full training due to memory overhead</li> </ul>"},{"location":"study-path/transformers/09-llm-prep/#enterprise-gpus","title":"Enterprise GPUs","text":"<ul> <li>NVIDIA A100/H100: 40\u201380GB VRAM</li> <li>Can train 7\u201313B parameter models with FP16</li> <li>Need multi-GPU setups for models \u226570B</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#lower-precision-formats-int8-4-bit","title":"Lower Precision Formats (INT8, 4-bit)","text":"<ul> <li>INT8: 1 byte per param \u2192 8B model = 8 GB</li> <li>4-bit: 0.5 bytes per param \u2192 8B model = 4 GB</li> </ul> <p>\u2705 Pros: - Drastic memory savings - Enables huge models to fit on limited VRAM</p> <p>\u26a0\ufe0f Cons: - Lower precision may reduce model accuracy - Often used in inference, not training</p> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#precision-vs-speed-hardware-implications","title":"Precision vs Speed: Hardware Implications","text":"<ul> <li>GPUs are optimized for FP16/FP32</li> <li>Very low-precision formats (e.g., INT4) may:</li> <li>Require internal conversions</li> <li>Lead to slower inference</li> <li>Reduce ability to utilize full GPU throughput</li> </ul> <p>\ud83d\udccc In practice: - Use FP16/BF16 for training - INT8/INT4 for memory-constrained inference</p> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#references-further-reading_2","title":"References &amp; Further Reading","text":"<ul> <li>Mixed Precision Training - NVIDIA</li> <li>Google TPU BF16 Overview</li> <li>\u201c8-Bit Optimizers via Block-wise Quantization\u201d \u2013 Dettmers et al.</li> <li>Hugging Face Guide to Quantization</li> <li>Jay Alammar\u2019s Illustrated Transformer</li> <li>PyTorch AMP Documentation</li> <li>INT4 Quantization - Facebook AI</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#navigating-gpu-selection-a-guide-to-hardware-platform","title":"\ud83d\udcd5 Navigating GPU Selection: A Guide to Hardware Platform","text":""},{"location":"study-path/transformers/09-llm-prep/#quick-navigation_4","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview</li> <li>Platform Comparison</li> <li>Consumer Platforms</li> <li>Google Colab</li> <li>RunPod</li> <li>Vast.ai</li> <li>Enterprise &amp; Cloud Platforms</li> <li>Lambda Labs</li> <li>Google Cloud Platform (GCP)</li> <li>Amazon Web Services (AWS)</li> <li>Recommended Path for Learners</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers/09-llm-prep/#overview_2","title":"Overview","text":"<p>Selecting the right GPU platform is essential for training and deploying large language models. This chapter provides a comparative guide to free, consumer-grade, and enterprise-level GPU options. Whether you're a student experimenting with smaller models or a researcher training multi-billion parameter LLMs, the right hardware can make all the difference.</p> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#platform-comparison","title":"Platform Comparison","text":"Platform Type Cost Best For Notes Google Colab Consumer/Free $0\u2013$11/mo Students, Quick Experiments Limited runtime &amp; GPU availability RunPod Consumer Pay-as-you-go Developers, Researchers Access to RTX 4090 and templates Vast.ai Peer-to-peer Lowest Technical Users Requires custom setup Lambda Labs Enterprise Premium High-performance DL workloads Best for large training GCP Cloud Variable Scalable ML solutions Complicated pricing AWS Cloud Expensive Production environments Complex and costly"},{"location":"study-path/transformers/09-llm-prep/#back-to-top_1","title":"Back to Top","text":""},{"location":"study-path/transformers/09-llm-prep/#consumer-platforms","title":"Consumer Platforms","text":""},{"location":"study-path/transformers/09-llm-prep/#google-colab","title":"Google Colab","text":"<ul> <li>Provides a Jupyter-based interface with Google Drive integration.</li> <li>Free Tier:</li> <li>Limited GPU types (T4, K80)</li> <li>Session timeouts (~90 mins)</li> <li>Colab Pro ($11/month):</li> <li>Access to more powerful GPUs</li> <li>Longer sessions</li> <li>Best for: prototyping, student learning, light inference workloads</li> </ul> <p>\ud83d\udc49 Try Colab</p>"},{"location":"study-path/transformers/09-llm-prep/#runpod","title":"RunPod","text":"<ul> <li>Access to high-end GPUs like RTX 4090</li> <li>Straightforward hourly pricing</li> <li>Docker templates preconfigured for DL</li> <li>No long-term commitments</li> </ul> <p>\ud83d\udc49 Explore RunPod</p>"},{"location":"study-path/transformers/09-llm-prep/#vastai","title":"Vast.ai","text":"<ul> <li>Marketplace for renting idle GPUs from other users</li> <li>Potentially lowest prices</li> <li>Requires custom setup and technical knowledge</li> <li>Performance may vary based on provider</li> </ul> <p>\ud83d\udc49 Try Vast.ai</p> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#enterprise-cloud-platforms","title":"Enterprise &amp; Cloud Platforms","text":""},{"location":"study-path/transformers/09-llm-prep/#lambda-labs","title":"Lambda Labs","text":"<ul> <li>Designed for deep learning workloads</li> <li>Offers stable infrastructure and optimized environments</li> <li>Higher cost; suitable for long-term research training</li> </ul> <p>\ud83d\udc49 Visit Lambda</p>"},{"location":"study-path/transformers/09-llm-prep/#google-cloud-platform-gcp","title":"Google Cloud Platform (GCP)","text":"<ul> <li>Highly scalable</li> <li>Wide selection of GPU types (A100, T4, V100)</li> <li>Suitable for:</li> <li>Large-scale training pipelines</li> <li>Distributed training</li> <li>Steep learning curve and complex pricing</li> </ul> <p>\ud83d\udc49 Explore GCP</p>"},{"location":"study-path/transformers/09-llm-prep/#amazon-web-services-aws","title":"Amazon Web Services (AWS)","text":"<ul> <li>Most comprehensive cloud ecosystem</li> <li>Broad GPU instance support (P4, G5, etc.)</li> <li>Very flexible, but can become prohibitively expensive</li> <li>Recommended for:</li> <li>Production deployments</li> <li>Enterprise-grade inference</li> </ul> <p>\ud83d\udc49 Visit AWS</p> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#recommended-path-for-learners","title":"Recommended Path for Learners","text":"<ul> <li>Start with Google Colab (Free/Pro) for initial lessons and exploration.</li> <li>As you progress to heavier training:</li> <li>Move to RunPod for access to high-end consumer GPUs.</li> <li>Consider Lambda Labs for long-term deep learning needs.</li> <li>If cost is a constraint and you're technically inclined, Vast.ai may offer unbeatable pricing.</li> <li>Use GCP or AWS only if:</li> <li>You\u2019re deploying at scale</li> <li>You\u2019re familiar with managing cloud infrastructure</li> </ul>"},{"location":"study-path/transformers/09-llm-prep/#back-to-top_2","title":"Back to Top","text":""},{"location":"study-path/transformers/09-llm-prep/#references-further-reading_3","title":"References &amp; Further Reading","text":"<ul> <li>Google Colab</li> <li>RunPod</li> <li>Vast.ai</li> <li>Lambda Labs</li> <li>Google Cloud GPU Pricing</li> <li>AWS EC2 GPU Instances</li> <li>NVIDIA Deep Learning GPU Guide</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#practice-fundamentals-most-basic-form-of-training-llms","title":"\ud83e\uddea Practice Fundamentals: Most Basic Form of Training LLMs","text":""},{"location":"study-path/transformers/09-llm-prep/#quick-navigation_5","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview</li> <li>Platform Comparison</li> <li>Consumer Platforms</li> <li>Google Colab</li> <li>RunPod</li> <li>Vast.ai</li> <li>Enterprise &amp; Cloud Platforms</li> <li>Lambda Labs</li> <li>Google Cloud Platform (GCP)</li> <li>Amazon Web Services (AWS)</li> <li>Recommended Path for Learners</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers/09-llm-prep/#overview_3","title":"Overview","text":"<p>Selecting the right GPU platform is essential for training and deploying large language models. This chapter provides a comparative guide to free, consumer-grade, and enterprise-level GPU options. Whether you're a student experimenting with smaller models or a researcher training multi-billion parameter LLMs, the right hardware can make all the difference.</p> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#platform-comparison_1","title":"Platform Comparison","text":"Platform Type Cost Best For Notes Google Colab Consumer/Free $0\u2013$11/mo Students, Quick Experiments Limited runtime &amp; GPU availability RunPod Consumer Pay-as-you-go Developers, Researchers Access to RTX 4090 and templates Vast.ai Peer-to-peer Lowest Technical Users Requires custom setup Lambda Labs Enterprise Premium High-performance DL workloads Best for large training GCP Cloud Variable Scalable ML solutions Complicated pricing AWS Cloud Expensive Production environments Complex and costly <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#consumer-platforms_1","title":"Consumer Platforms","text":""},{"location":"study-path/transformers/09-llm-prep/#google-colab_1","title":"Google Colab","text":"<ul> <li>Provides a Jupyter-based interface with Google Drive integration.</li> <li>Free Tier:</li> <li>Limited GPU types (T4, K80)</li> <li>Session timeouts (~90 mins)</li> <li>Colab Pro ($11/month):</li> <li>Access to more powerful GPUs</li> <li>Longer sessions</li> <li>Best for: prototyping, student learning, light inference workloads</li> </ul> <p>\ud83d\udc49 Try Colab</p>"},{"location":"study-path/transformers/09-llm-prep/#runpod_1","title":"RunPod","text":"<ul> <li>Access to high-end GPUs like RTX 4090</li> <li>Straightforward hourly pricing</li> <li>Docker templates preconfigured for DL</li> <li>No long-term commitments</li> </ul> <p>\ud83d\udc49 Explore RunPod</p>"},{"location":"study-path/transformers/09-llm-prep/#vastai_1","title":"Vast.ai","text":"<ul> <li>Marketplace for renting idle GPUs from other users</li> <li>Potentially lowest prices</li> <li>Requires custom setup and technical knowledge</li> <li>Performance may vary based on provider</li> </ul> <p>\ud83d\udc49 Try Vast.ai</p> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#enterprise-cloud-platforms_1","title":"Enterprise &amp; Cloud Platforms","text":""},{"location":"study-path/transformers/09-llm-prep/#lambda-labs_1","title":"Lambda Labs","text":"<ul> <li>Designed for deep learning workloads</li> <li>Offers stable infrastructure and optimized environments</li> <li>Higher cost; suitable for long-term research training</li> </ul> <p>\ud83d\udc49 Visit Lambda</p>"},{"location":"study-path/transformers/09-llm-prep/#google-cloud-platform-gcp_1","title":"Google Cloud Platform (GCP)","text":"<ul> <li>Highly scalable</li> <li>Wide selection of GPU types (A100, T4, V100)</li> <li>Suitable for:</li> <li>Large-scale training pipelines</li> <li>Distributed training</li> <li>Steep learning curve and complex pricing</li> </ul> <p>\ud83d\udc49 Explore GCP</p>"},{"location":"study-path/transformers/09-llm-prep/#amazon-web-services-aws_1","title":"Amazon Web Services (AWS)","text":"<ul> <li>Most comprehensive cloud ecosystem</li> <li>Broad GPU instance support (P4, G5, etc.)</li> <li>Very flexible, but can become prohibitively expensive</li> <li>Recommended for:</li> <li>Production deployments</li> <li>Enterprise-grade inference</li> </ul> <p>\ud83d\udc49 Visit AWS</p> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#recommended-path-for-learners_1","title":"Recommended Path for Learners","text":"<ul> <li>Start with Google Colab (Free/Pro) for initial lessons and exploration.</li> <li>As you progress to heavier training:</li> <li>Move to RunPod for access to high-end consumer GPUs.</li> <li>Consider Lambda Labs for long-term deep learning needs.</li> <li>If cost is a constraint and you're technically inclined, Vast.ai may offer unbeatable pricing.</li> <li>Use GCP or AWS only if:</li> <li>You\u2019re deploying at scale</li> <li>You\u2019re familiar with managing cloud infrastructure</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#references-further-reading_4","title":"References &amp; Further Reading","text":"<ul> <li>Google Colab</li> <li>RunPod</li> <li>Vast.ai</li> <li>Lambda Labs</li> <li>Google Cloud GPU Pricing</li> <li>AWS EC2 GPU Instances</li> <li>NVIDIA Deep Learning GPU Guide</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#practice-fundamentals-part-2-most-basic-form-of-training-llms","title":"\ud83e\uddea Practice Fundamentals Part 2: Most Basic Form of Training LLMs","text":""},{"location":"study-path/transformers/09-llm-prep/#quick-navigation_6","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview</li> <li>Training Control with YAML Config</li> <li>Model Setup Parameters</li> <li>Dataset &amp; Formatting Logic</li> <li>Training Configuration File (YAML)</li> <li>Colab Integration</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers/09-llm-prep/#overview_4","title":"Overview","text":"<p>In this chapter, we explore how to define and control your LLM training pipeline using Axolotl's YAML-based configuration system. The focus is on training a small decoder-only model (TinyLlama) to generate short stories based on prompts using a dataset from Hugging Face.</p> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#training-control-with-yaml-config","title":"Training Control with YAML Config","text":"<p>Axolotl leverages <code>.yml</code> configuration files to simplify LLM training orchestration. Rather than scripting logic, users can define:</p> <ul> <li>Model checkpoint and architecture</li> <li>Tokenizer type</li> <li>Dataset path and format</li> <li>Training hyperparameters</li> <li>Output and logging setup</li> </ul> <p>This allows non-programmers or fast-moving practitioners to quickly train, tune, and test models.</p>"},{"location":"study-path/transformers/09-llm-prep/#back-to-top_3","title":"Back to Top","text":""},{"location":"study-path/transformers/09-llm-prep/#model-setup-parameters","title":"Model Setup Parameters","text":"<p>Key fields in the YAML:</p> <ul> <li><code>base_model</code>: Pretrained checkpoint (e.g., TinyLlama 1.1B Chat)</li> <li><code>model_type</code>: Architecture class (LlamaForCausalLM)</li> <li><code>tokenizer_type</code>: Hugging Face tokenizer class (LlamaTokenizer)</li> <li><code>sequence_length</code>: Input length cap (e.g., 1024)</li> <li><code>precision</code>: Uses <code>bf16</code> (brain float 16), auto-detected if supported</li> </ul> <p>These map to common fields expected by Hugging Face models and tokenizers. Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#dataset-formatting-logic","title":"Dataset &amp; Formatting Logic","text":"<p>Dataset used: <code>jaydenccc/AI_Storyteller_Dataset</code></p> <ul> <li>Contains:</li> <li><code>synopsis</code> \u2192 serves as instruction prompt</li> <li><code>short_story</code> \u2192 target output the model learns</li> </ul>"},{"location":"study-path/transformers/09-llm-prep/#formatting","title":"Formatting","text":"<pre><code>&lt;|user|&gt;\n {instruction} &lt;/s&gt;\n&lt;|assistant|&gt;\n {short_story}\n</code></pre> <ul> <li>Follows the LLaMA chat template</li> <li>Ensures correct message alignment in decoder-only models</li> <li>Axolotl handles the formatting and tokenization logic internally</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#training-configuration-file-yaml","title":"Training Configuration File (YAML)","text":"<p>Below is the <code>basic_train.yml</code> referenced in this lesson. You can include this block directly in your MkDocs site or host it as a downloadable file.</p> <pre><code># model params\nbase_model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\nmodel_type: LlamaForCausalLM\ntokenizer_type: LlamaTokenizer\n\n# dataset params\ndatasets:\n  - path: jaydenccc/AI_Storyteller_Dataset\n    type: \n      system_prompt: \"\"\n      field_system: system\n      field_instruction: synopsis\n      field_output: short_story\n      format: \"&lt;|user|&gt;\\n {instruction} &lt;/s&gt;\\n&lt;|assistant|&gt;\"\n      no_input_format: \"&lt;|user|&gt; {instruction} &lt;/s&gt;\\n&lt;|assistant|&gt;\"\n\noutput_dir: ./models/TinyLlama_Storyteller\n\n# model params\nsequence_length: 1024\nbf16: auto\ntf32: false\n\n# training params\nbatch_size: 4\nmicro_batch_size: 4\nnum_epochs: 4\noptimizer: adamw_bnb_8bit\nlearning_rate: 0.0002\nlogging_steps: 1\n</code></pre> <p>\u2705 You can also link this YAML as a raw GitHub file or store it in your docs/assets folder for users to download.</p> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#colab-integration","title":"Colab Integration","text":"<p>\ud83d\udc49 Open in Colab </p> <ul> <li>Contains environment setup and config-based training loop</li> <li>Compatible with Colab Pro for GPU-based fine-tuning</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#references-further-reading_5","title":"References &amp; Further Reading","text":"<ul> <li>Axolotl GitHub</li> <li>Colab Exercise Notebook</li> <li>TinyLlama Model Card (Hugging Face)</li> <li>JaydenCCC AI Storytelling Dataset</li> <li>YAML Config Docs from Axolotl</li> <li>Hugging Face Transformers Docs Back to Top</li> </ul>"},{"location":"study-path/transformers/09-llm-prep/#practice-fundamentals-part-3-most-basic-form-of-training-llms","title":"\ud83e\uddea Practice Fundamentals Part 3: Most Basic Form of Training LLMs","text":""},{"location":"study-path/transformers/09-llm-prep/#quick-navigation_7","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview</li> <li>Starting the Training Loop</li> <li>Monitoring Loss and Epochs</li> <li>Testing the Trained Model</li> <li>Colab Integration</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers/09-llm-prep/#overview_5","title":"Overview","text":"<p>This chapter concludes the first end-to-end training workflow using Axolotl and TinyLlama. We run the training script, observe the model's learning progress, and evaluate its performance with sample prompts to test generalization.</p>"},{"location":"study-path/transformers/09-llm-prep/#back-to-top_4","title":"Back to Top","text":""},{"location":"study-path/transformers/09-llm-prep/#starting-the-training-loop","title":"Starting the Training Loop","text":"<p>Once the YAML configuration file (<code>basic_train.yml</code>) is ready, you can launch training by running:</p> <pre><code>python -m axolotl.cli.train basic_train.yml\n</code></pre> <ul> <li>Loads the specified model (TinyLlama-1.1B-Chat-v1.0)</li> <li>Tokenizes dataset <code>jaydenccc/AI_Storyteller_Dataset</code></li> <li>Starts training loop with live logging</li> </ul> <p>\ud83d\udfe2 Axolotl automatically applies formatting, precision, and optimizer choices from the YAML.</p> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#monitoring-loss-and-epochs","title":"Monitoring Loss and Epochs","text":"<ul> <li>Training loss is printed at each step due to <code>logging_steps: 1</code></li> <li>For a small dataset:</li> <li>Training is fast (few minutes with 4 epochs)</li> <li>Loss decreases with each batch, indicating effective learning</li> </ul> <p>\ud83e\udde0 Despite the minimal size of the dataset, the model learns the task effectively due to repetition and targeted prompts. Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#testing-the-trained-model","title":"Testing the Trained Model","text":"<p>Here's a sample Python test script:</p> <pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"./models/TinyLlama_Storyteller\", \n    torch_dtype=torch.float16, \n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"./models/TinyLlama_Storyteller\")\n\n# Prompt: Bright student working with a fuzzy scientist\nprompt = \"&lt;|user|&gt;\nA bright student was working with the fuzzy scientist on a project.&lt;/s&gt;\n&lt;|assistant|&gt;\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**inputs, max_new_tokens=512)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n</code></pre>"},{"location":"study-path/transformers/09-llm-prep/#results","title":"Results:","text":"<ul> <li>The model returns coherent short stories in response to prompts like:</li> <li>\"A bright student was working with a fuzzy scientist on a project.\"</li> <li>\"A global mission for humanity through overcrowded cities.\"</li> </ul> <p>\ud83c\udfaf Even with limited training, the model generalizes narrative structure well.</p>"},{"location":"study-path/transformers/09-llm-prep/#back-to-top_5","title":"Back to Top","text":""},{"location":"study-path/transformers/09-llm-prep/#colab-integration_1","title":"Colab Integration","text":"<p>\ud83d\udc49 Open in Colab </p> <ul> <li>Covers end-to-end steps from setup to inference</li> <li>Ideal for GPU-restricted environments</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/09-llm-prep/#references-further-reading_6","title":"References &amp; Further Reading","text":"<ul> <li>Axolotl GitHub</li> <li>TinyLlama Hugging Face Model Card</li> <li>Hugging Face Transformers</li> <li>JaydenCCC Storytelling Dataset</li> <li>Colab Exercise Notebook</li> <li>Accelerated LLM Training - NVIDIA</li> </ul> <p>Back to Top</p>"}]}