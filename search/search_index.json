{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Generative AI Study Hub","text":"<p>Last updated: July 26, 2025</p> <p>This is your central hub for learning and referencing Generative AI concepts.  Choose from the Study Path to go through structured topics or explore the Reference Hub for focused research and tooling insights.</p>"},{"location":"reference-hub/","title":"Reference Hub","text":"<p>Welcome to the Reference Hub! Here you'll find foundational topics, research tools, and hands-on practices for working with Generative AI.</p>"},{"location":"study-path/","title":"Study Path","text":"<p>Welcome to the Study Path! Explore structured lessons on Transformers, LLMOps, and Evaluation Frameworks.</p>"},{"location":"study-path/transformers-generative-ai/01-intro/","title":"What You\u2019ll Learn","text":"<p>\ud83d\udccc Quick Navigation</p> <ul> <li>What You\u2019ll Learn</li> <li>Part 1: Fundamentals of NLP and Transformers<ul> <li>Transformer Foundations</li> </ul> </li> <li>Part 2: Working with Large Language Models (LLMs)<ul> <li>Key Model Architectures</li> <li>Real-World Tasks You\u2019ll Implement</li> <li>Advanced Tooling and Techniques</li> </ul> </li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers-generative-ai/01-intro/#part-1-fundamentals-of-nlp-and-transformers","title":"Part 1: Fundamentals of NLP and Transformers","text":"<p>You\u2019ll explore the evolution of natural language processing (NLP) through four historical phases:</p> <ul> <li>Rule-Based Systems </li> <li> <p>Manually defined rules for parsing, tagging, and other language tasks.</p> </li> <li> <p>Statistical Methods </p> </li> <li> <p>Used mathematical probability and co-occurrence to model language.</p> </li> <li> <p>Machine Learning Era </p> </li> <li> <p>Leveraged labeled data for training classifiers like SVMs and Naive Bayes.</p> </li> <li> <p>Deep Learning &amp; Embeddings </p> </li> <li>Enabled dense semantic understanding and contextual word representations.</li> </ul>"},{"location":"study-path/transformers-generative-ai/01-intro/#transformer-foundations","title":"Transformer Foundations","text":"<p>??? info \"Why Attention Matters in Transformers\"</p> <pre><code>The attention mechanism enables a model to focus on relevant portions of the input,  \nrather than treating every token equally. This allows:\n\n- Better context awareness\n- Improved long-range dependency modeling\n- Reduced reliance on fixed-size memory\n\n\ud83d\udd0d **Example:** In translation, attention helps align source and target tokens precisely.\n</code></pre> <p>You\u2019ll also build a deep understanding of the architecture that powers modern LLMs:</p> <ul> <li>Attention Mechanism </li> <li> <p>Allows models to focus on important parts of the input.</p> </li> <li> <p>Encoder-Decoder Structure </p> </li> <li> <p>Enables tasks like translation, summarization, and text generation.</p> </li> <li> <p>Tokenization &amp; Embeddings </p> </li> <li> <p>Converts text into vectors, enabling model computation.</p> </li> <li> <p>Pretraining and Fine-Tuning </p> </li> <li>Learn how general-purpose models adapt to specific tasks.</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers-generative-ai/01-intro/#part-2-working-with-large-language-models-llms","title":"Part 2: Working with Large Language Models (LLMs)","text":"<p>This section focuses on state-of-the-art transformer-based models and their practical applications.</p>"},{"location":"study-path/transformers-generative-ai/01-intro/#key-model-architectures","title":"Key Model Architectures","text":"<ul> <li>BERT (Encoder-only) </li> <li> <p>Learns bidirectional context; ideal for understanding tasks like classification or Q&amp;A.</p> </li> <li> <p>GPT (Decoder-only) </p> </li> <li> <p>Generates coherent, fluent text; the backbone of tools like ChatGPT.</p> </li> <li> <p>T5 (Encoder-Decoder) </p> </li> <li>Treats all problems as a text-to-text task, offering maximum flexibility.</li> </ul>"},{"location":"study-path/transformers-generative-ai/01-intro/#real-world-tasks-youll-implement","title":"Real-World Tasks You\u2019ll Implement","text":"<ul> <li>Masked Language Modeling (MLM)  </li> <li>Semantic Search with embeddings  </li> <li>Document-Based Question Answering  </li> <li>Instruction-Following Text Generation  </li> <li>Product Review Generation (prompt-based)</li> </ul>"},{"location":"study-path/transformers-generative-ai/01-intro/#advanced-tooling-and-techniques","title":"Advanced Tooling and Techniques","text":"<p>You\u2019ll gain hands-on experience with modern model optimization strategies:</p> <ul> <li>LoRA and PeFT (parameter-efficient fine-tuning)  </li> <li>8-bit / 4-bit quantization for faster, smaller models  </li> <li>FlashAttention, DeepSpeed, and FSDP for accelerated training  </li> <li>Chat templates and RLHF (Reinforcement Learning from Human Feedback)</li> </ul> <p>??? info \"Why Fine-Tuning is Crucial\"</p> <pre><code>Fine-tuning pre-trained models is essential when applying LLMs to specialized or production environments. It enhances the model's ability to understand domain-specific language, improves generalization, and reduces errors.\n\n**Benefits of Fine-Tuning:**\n\n- \u2705 Adapts general models to niche domains (e.g., law, healthcare, finance)\n- \ud83d\udcc8 Boosts model performance on specific downstream tasks\n- \ud83e\udde0 Learns contextual and jargon-heavy nuances\n- \ud83d\udcbe Saves compute compared to training from scratch\n\n\ud83d\udd0d **Example:**  \nFine-tuning BERT on clinical notes dramatically improves performance in electronic health record (EHR) classification tasks.\n</code></pre> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers-generative-ai/01-intro/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>Hugging Face: T5-base Amazon Product Reviews Model</li> <li>Hugging Face: DiabloGPT Open Instruct Model</li> <li>Google Colab: T5 Product Review Notebook</li> <li>Attention Is All You Need (Transformer Paper)</li> <li>Hugging Face Transformers Documentation</li> <li>Google AI Blog on BERT</li> <li>Sebastian Ruder: NLP Progress</li> <li>The Illustrated Transformer (by Jay Alammar)</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers-generative-ai/02-getting-started/","title":"NLP Evolution Timeline","text":""},{"location":"study-path/transformers-generative-ai/02-getting-started/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Historical NLP Techniques</li> <li>2. Statistical NLP Era</li> <li>3. Machine Learning Era in NLP</li> <li>4. Embedding Era in NLP</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers-generative-ai/02-getting-started/#1-historical-nlp-techniques","title":"1. Historical NLP Techniques","text":"<p>Understanding the evolution of NLP techniques provides critical context for modern advancements like transformers. This section explores foundational rule-based systems.</p>"},{"location":"study-path/transformers-generative-ai/02-getting-started/#rule-based-nlp-era","title":"Rule-Based NLP Era","text":"<ul> <li>Built on manually crafted linguistic rules</li> <li>Focused on syntactic analysis:</li> <li>Parsing: Grammatical structure and relationships</li> <li>Part-of-Speech Tagging: Identifying grammatical roles</li> <li>Applications:</li> <li>Syntax analysis</li> <li>Text summarization</li> <li>Machine translation</li> </ul>"},{"location":"study-path/transformers-generative-ai/02-getting-started/#key-limitations","title":"Key Limitations","text":"<ul> <li>Ambiguity: Poor context awareness</li> <li>Scalability: Rule creation and maintenance were not feasible at scale</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers-generative-ai/02-getting-started/#2-statistical-nlp-era","title":"2. Statistical NLP Era","text":"<p>The transition to data-driven statistical techniques marked a turning point in NLP.</p>"},{"location":"study-path/transformers-generative-ai/02-getting-started/#key-innovations","title":"Key Innovations","text":"<ul> <li>Data-Driven Shift: Replaced rules with learned probabilities</li> <li>Probabilistic Language Models: Modeled word likelihoods and co-occurrence patterns</li> <li>n-Grams: Captured word sequences (e.g., bigrams, trigrams)</li> <li>Hidden Markov Models (HMMs):</li> <li>Used for sequence tasks (POS tagging, NER)</li> <li>Modeled state transitions for linguistic structure</li> </ul>"},{"location":"study-path/transformers-generative-ai/02-getting-started/#applications","title":"Applications","text":"<ul> <li>POS Tagging: Predict tags using probability sequences</li> <li>Named Entity Recognition (NER): Detect names, dates, organizations</li> </ul>"},{"location":"study-path/transformers-generative-ai/02-getting-started/#limitations","title":"Limitations","text":"<ul> <li>Data Sparsity: Rare word combinations weakened predictions</li> <li>Shallow Semantics: Couldn\u2019t truly \u201cunderstand\u201d meaning</li> </ul>"},{"location":"study-path/transformers-generative-ai/02-getting-started/#evolution","title":"Evolution","text":"<p>These limitations led to machine learning and neural models, enabling more scalable, adaptive solutions.</p> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers-generative-ai/02-getting-started/#3-machine-learning-era-in-nlp","title":"3. Machine Learning Era in NLP","text":"<p>Machine learning enabled NLP systems to generalize from data without extensive rules or handcrafted features.</p>"},{"location":"study-path/transformers-generative-ai/02-getting-started/#key-advancements","title":"Key Advancements","text":"<ul> <li>Naive Bayes: Probabilistic classifier for text classification (e.g., spam detection)</li> <li>Support Vector Machines (SVMs):</li> <li>Effective for sentiment analysis</li> <li>Worked well on high-dimensional text vectors</li> </ul>"},{"location":"study-path/transformers-generative-ai/02-getting-started/#rise-of-neural-networks","title":"Rise of Neural Networks","text":"<ul> <li>Reduced Feature Engineering: Learned features from raw data</li> <li>Applications: Summarization, translation, sentiment detection</li> </ul>"},{"location":"study-path/transformers-generative-ai/02-getting-started/#specialized-architectures","title":"Specialized Architectures","text":"<ul> <li>RNNs:</li> <li>Process text sequentially</li> <li>Preserve past input using hidden state</li> <li> <p>Limitations: Weak on long-term dependencies</p> </li> <li> <p>LSTMs:</p> </li> <li>Enhanced RNNs with memory cells</li> <li>Better handling of long-range context</li> <li>Enabled language modeling and generation</li> </ul>"},{"location":"study-path/transformers-generative-ai/02-getting-started/#milestones","title":"Milestones","text":"<ul> <li>Shifted to end-to-end learning</li> <li>More flexible and powerful than statistical models</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers-generative-ai/02-getting-started/#4-embedding-era-in-nlp","title":"4. Embedding Era in NLP","text":"<p>Dense vector embeddings enabled models to capture word meaning and similarity, surpassing sparse representations like one-hot encoding.</p>"},{"location":"study-path/transformers-generative-ai/02-getting-started/#key-concepts","title":"Key Concepts","text":"<ul> <li>Word Embeddings:</li> <li>Low-dimensional, dense vectors for each word</li> <li> <p>Capture meaning through context-based learning</p> </li> <li> <p>Benefits Over One-Hot Encoding:</p> </li> <li>Smaller dimensionality</li> <li>Encoded meaning and similarity</li> </ul>"},{"location":"study-path/transformers-generative-ai/02-getting-started/#popular-embedding-techniques","title":"Popular Embedding Techniques","text":"Technique Developer Method Highlights Word2Vec Google Skip-gram, CBOW Context prediction via local word windows GloVe Stanford Co-occurrence + global stats Combines frequency and semantics FastText Facebook AI Subword n-grams Handles rare and OOV words better"},{"location":"study-path/transformers-generative-ai/02-getting-started/#applications_1","title":"Applications","text":"<ul> <li>Semantic Similarity: Text comparison</li> <li>Text Classification: Improved input features</li> <li>Translation, QA: Foundation for neural systems</li> <li>Input to Deep Models: Used in RNNs, LSTMs, and later transformers</li> </ul>"},{"location":"study-path/transformers-generative-ai/02-getting-started/#limitations_1","title":"Limitations","text":"<ul> <li>Static Embeddings: One vector per word, no context awareness</li> <li>No Polysemy Handling: Same vector for multiple meanings (e.g., \u201cbank\u201d)</li> </ul> <p>These drawbacks triggered the rise of contextualized embeddings (e.g., ELMo, BERT), marking the start of the Transformer Era.</p> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers-generative-ai/02-getting-started/#references-further-reading","title":"References &amp; Further Reading","text":"<p>Here are some recommended resources to explore these topics further:</p> <ul> <li>Word2Vec - Google Research</li> <li>GloVe - Stanford NLP Group</li> <li>FastText - Facebook AI</li> <li>The Illustrated Word2Vec (Chris McCormick)</li> <li>A Primer in BERTology (ACL Survey)</li> <li>NLP Progress</li> <li>The Embedding Project on Hugging Face</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers-generative-ai/03-nlp-overview/","title":"NLP Evolution Timeline","text":""},{"location":"study-path/transformers-generative-ai/03-nlp-overview/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Historical NLP Techniques</li> <li>2. Statistical NLP Era</li> <li>3. Machine Learning Era in NLP</li> <li>4. Embedding Era in NLP</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers-generative-ai/03-nlp-overview/#1-historical-nlp-techniques","title":"1. Historical NLP Techniques","text":"<p>Understanding the evolution of NLP techniques provides critical context for modern advancements like transformers. This section explores foundational rule-based systems.</p>"},{"location":"study-path/transformers-generative-ai/03-nlp-overview/#rule-based-nlp-era","title":"Rule-Based NLP Era","text":"<ul> <li>Built on manually crafted linguistic rules</li> <li>Focused on syntactic analysis:</li> <li>Parsing: Grammatical structure and relationships</li> <li>Part-of-Speech Tagging: Identifying grammatical roles</li> <li>Applications:</li> <li>Syntax analysis</li> <li>Text summarization</li> <li>Machine translation</li> </ul>"},{"location":"study-path/transformers-generative-ai/03-nlp-overview/#key-limitations","title":"Key Limitations","text":"<ul> <li>Ambiguity: Poor context awareness</li> <li>Scalability: Rule creation and maintenance were not feasible at scale</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers-generative-ai/03-nlp-overview/#2-statistical-nlp-era","title":"2. Statistical NLP Era","text":"<p>The transition to data-driven statistical techniques marked a turning point in NLP.</p>"},{"location":"study-path/transformers-generative-ai/03-nlp-overview/#key-innovations","title":"Key Innovations","text":"<ul> <li>Data-Driven Shift: Replaced rules with learned probabilities</li> <li>Probabilistic Language Models: Modeled word likelihoods and co-occurrence patterns</li> <li>n-Grams: Captured word sequences (e.g., bigrams, trigrams)</li> <li>Hidden Markov Models (HMMs):</li> <li>Used for sequence tasks (POS tagging, NER)</li> <li>Modeled state transitions for linguistic structure</li> </ul>"},{"location":"study-path/transformers-generative-ai/03-nlp-overview/#applications","title":"Applications","text":"<ul> <li>POS Tagging: Predict tags using probability sequences</li> <li>Named Entity Recognition (NER): Detect names, dates, organizations</li> </ul>"},{"location":"study-path/transformers-generative-ai/03-nlp-overview/#limitations","title":"Limitations","text":"<ul> <li>Data Sparsity: Rare word combinations weakened predictions</li> <li>Shallow Semantics: Couldn\u2019t truly \u201cunderstand\u201d meaning</li> </ul>"},{"location":"study-path/transformers-generative-ai/03-nlp-overview/#evolution","title":"Evolution","text":"<p>These limitations led to machine learning and neural models, enabling more scalable, adaptive solutions.</p> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers-generative-ai/03-nlp-overview/#3-machine-learning-era-in-nlp","title":"3. Machine Learning Era in NLP","text":"<p>Machine learning enabled NLP systems to generalize from data without extensive rules or handcrafted features.</p>"},{"location":"study-path/transformers-generative-ai/03-nlp-overview/#key-advancements","title":"Key Advancements","text":"<ul> <li>Naive Bayes: Probabilistic classifier for text classification (e.g., spam detection)</li> <li>Support Vector Machines (SVMs):</li> <li>Effective for sentiment analysis</li> <li>Worked well on high-dimensional text vectors</li> </ul>"},{"location":"study-path/transformers-generative-ai/03-nlp-overview/#rise-of-neural-networks","title":"Rise of Neural Networks","text":"<ul> <li>Reduced Feature Engineering: Learned features from raw data</li> <li>Applications: Summarization, translation, sentiment detection</li> </ul>"},{"location":"study-path/transformers-generative-ai/03-nlp-overview/#specialized-architectures","title":"Specialized Architectures","text":"<ul> <li>RNNs:</li> <li>Process text sequentially</li> <li>Preserve past input using hidden state</li> <li> <p>Limitations: Weak on long-term dependencies</p> </li> <li> <p>LSTMs:</p> </li> <li>Enhanced RNNs with memory cells</li> <li>Better handling of long-range context</li> <li>Enabled language modeling and generation</li> </ul>"},{"location":"study-path/transformers-generative-ai/03-nlp-overview/#milestones","title":"Milestones","text":"<ul> <li>Shifted to end-to-end learning</li> <li>More flexible and powerful than statistical models</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers-generative-ai/03-nlp-overview/#4-embedding-era-in-nlp","title":"4. Embedding Era in NLP","text":"<p>Dense vector embeddings enabled models to capture word meaning and similarity, surpassing sparse representations like one-hot encoding.</p>"},{"location":"study-path/transformers-generative-ai/03-nlp-overview/#key-concepts","title":"Key Concepts","text":"<ul> <li>Word Embeddings:</li> <li>Low-dimensional, dense vectors for each word</li> <li> <p>Capture meaning through context-based learning</p> </li> <li> <p>Benefits Over One-Hot Encoding:</p> </li> <li>Smaller dimensionality</li> <li>Encoded meaning and similarity</li> </ul>"},{"location":"study-path/transformers-generative-ai/03-nlp-overview/#popular-embedding-techniques","title":"Popular Embedding Techniques","text":"Technique Developer Method Highlights Word2Vec Google Skip-gram, CBOW Context prediction via local word windows GloVe Stanford Co-occurrence + global stats Combines frequency and semantics FastText Facebook AI Subword n-grams Handles rare and OOV words better"},{"location":"study-path/transformers-generative-ai/03-nlp-overview/#applications_1","title":"Applications","text":"<ul> <li>Semantic Similarity: Text comparison</li> <li>Text Classification: Improved input features</li> <li>Translation, QA: Foundation for neural systems</li> <li>Input to Deep Models: Used in RNNs, LSTMs, and later transformers</li> </ul>"},{"location":"study-path/transformers-generative-ai/03-nlp-overview/#limitations_1","title":"Limitations","text":"<ul> <li>Static Embeddings: One vector per word, no context awareness</li> <li>No Polysemy Handling: Same vector for multiple meanings (e.g., \u201cbank\u201d)</li> </ul> <p>These drawbacks triggered the rise of contextualized embeddings (e.g., ELMo, BERT), marking the start of the Transformer Era.</p> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers-generative-ai/03-nlp-overview/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>Word2Vec Explained (Google Research)</li> <li>GloVe: Global Vectors for Word Representation (Stanford)</li> <li>FastText (Facebook AI)</li> <li>The Illustrated Transformer (Jay Alammar)</li> <li>Sebastian Ruder: NLP Progress Tracker</li> <li>Hugging Face: T5-base Product Review Model</li> <li>Google Colab: Try Word Embeddings</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers-generative-ai/04-transformer-intro/","title":"Transformer Fundamentals","text":""},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Transformer Architecture Overview</li> <li>2. Transformer Training Paradigm: Pre-training and Fine-tuning</li> <li>3. Tokenization and Embeddings in Transformer Models</li> </ul>"},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#1-transformer-architecture-overview","title":"1. Transformer Architecture Overview","text":"<p>This lesson introduces the transformer model architecture, emphasizing its structural innovations, key mechanisms, and how it revolutionized NLP by overcoming the limitations of RNNs and LSTMs.</p>"},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#origins-and-significance","title":"Origins and Significance","text":"<ul> <li>Introduced in 2017 via the paper \"Attention Is All You Need\"</li> <li>Replaced sequential RNN/LSTM processing with fully parallel architecture</li> <li>Solved long-range dependency issues and improved training speed</li> <li>Enabled large-scale model training and breakthroughs in language understanding</li> </ul>"},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#core-components-of-transformer-architecture","title":"Core Components of Transformer Architecture","text":""},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#encoder-decoder-structure","title":"Encoder-Decoder Structure","text":"<ul> <li>Encoder: Converts input text into continuous vector representations capturing context and relationships</li> <li>Decoder: Generates output text from encoder\u2019s processed information</li> <li>Enables tasks like translation, summarization, and question answering</li> </ul>"},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#attention-mechanisms","title":"Attention Mechanisms","text":"<ul> <li>Self-Attention: Weighs each word relative to others to build context-aware representations</li> <li>Scaled Dot-Product Attention: Computes dot products, scales scores, and applies softmax</li> <li>Multi-Head Attention: Uses multiple heads to capture diverse semantic/syntactic patterns</li> </ul>"},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#positional-encoding","title":"Positional Encoding","text":"<ul> <li>Compensates for lack of inherent word order in attention-only models</li> <li>Adds position-based signals to token embeddings</li> </ul>"},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#feed-forward-network-layer-normalization","title":"Feed-Forward Network &amp; Layer Normalization","text":"<ul> <li>Feed-Forward Network: Applies non-linear transformations to extract high-level features</li> <li>Layer Normalization: Stabilizes training by normalizing outputs between layers</li> </ul>"},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#full-encoder-and-decoder-block","title":"Full Encoder and Decoder Block","text":"<ul> <li>Composed of stacked layers with:</li> <li>Multi-head attention</li> <li>Feed-forward networks</li> <li>Layer normalization</li> <li>Decoder includes additional encoder-decoder attention to align output generation</li> </ul>"},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#real-world-application-example","title":"Real-World Application Example","text":"<p>Abstractive Question Answering</p> <ul> <li>Input: Paragraph + Question</li> <li>Encoder: Processes both into contextual embeddings</li> <li>Decoder: Generates an answer from the learned representation</li> </ul>"},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Transformers enabled scalable, parallel NLP processing</li> <li>Encoder-decoder architecture allows diverse tasks</li> <li>Attention mechanisms are key to understanding global context</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#2-transformer-training-paradigm-pre-training-and-fine-tuning","title":"2. Transformer Training Paradigm: Pre-training and Fine-tuning","text":"<p>This lesson outlines the two-phase training process of transformer models\u2014pre-training and fine-tuning\u2014contrasting it with traditional ML workflows.</p>"},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#training-structure-overview","title":"Training Structure Overview","text":"<ul> <li>Pre-training: General language learning from large unlabeled datasets</li> <li>Fine-tuning: Task-specific adaptation using labeled datasets</li> </ul>"},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#pre-training-phase","title":"Pre-training Phase","text":"<ul> <li>Learns grammar, context, word relationships, and long-range dependencies</li> <li>Massive-scale unsupervised training</li> <li>\ud83d\udd01 Analogy: Like learning music theory before mastering a genre</li> </ul>"},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#fine-tuning-phase","title":"Fine-tuning Phase","text":"<ul> <li>Adapts pre-trained models to tasks like NER, translation, QA, etc.</li> <li>Requires smaller supervised datasets</li> <li>Leverages transfer learning</li> <li>\ud83d\udd01 Analogy: Like a trained pianist specializing in jazz</li> </ul>"},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#combined-workflow","title":"Combined Workflow","text":"<ol> <li>Step 1: Pre-training</li> <li>Random initialization \u2192 trained on general data</li> <li>Step 2: Fine-tuning</li> <li>Task-specific data \u2192 adapted for downstream performance</li> </ol>"},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#real-world-considerations","title":"Real-world Considerations","text":"<ul> <li>Pre-training requires huge compute and data (done by orgs like Google, OpenAI)</li> <li>Most use pre-trained models and fine-tune</li> <li>Full pre-training is rare unless:</li> <li>You work with proprietary, underrepresented, or specialized domains (e.g., legal, clinical)</li> </ul>"},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#key-takeaways_1","title":"Key Takeaways","text":"<ul> <li>Pre-training + fine-tuning is the standard approach in NLP</li> <li>Enables rapid model deployment with high performance</li> <li>Specialized domains may benefit from custom pre-training</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#3-tokenization-and-embeddings-in-transformer-models","title":"3. Tokenization and Embeddings in Transformer Models","text":"<p>This lesson covers how transformers process raw text into vector representations using tokenization and embeddings.</p>"},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#tokenization","title":"Tokenization","text":""},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#purpose","title":"Purpose","text":"<ul> <li>Breaks text into smaller units called tokens</li> <li>Translates natural language into numerical input (token IDs)</li> </ul>"},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#types-of-tokenization","title":"Types of Tokenization","text":"<ul> <li>Word-level: One token per word; suffers from OOV (out-of-vocabulary) issues</li> <li>Character-level: Every character is a token; leads to longer sequences</li> <li>Subword-level (common): Breaks unknown words into known parts (e.g., Byte-Pair Encoding)</li> </ul>"},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#workflow","title":"Workflow","text":"<ol> <li>Breaks text into tokens</li> <li>Maps tokens to IDs using a predefined vocabulary</li> <li>Feeds IDs into the transformer model</li> </ol>"},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#embeddings","title":"Embeddings","text":""},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#purpose_1","title":"Purpose","text":"<ul> <li>Convert token IDs into high-dimensional dense vectors</li> <li>Capture meaning and contextual usage of tokens</li> </ul>"},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#key-concepts","title":"Key Concepts","text":"<ul> <li>Embeddings are context-aware (e.g., \"bank\" in finance vs. riverbank)</li> <li>Contextual embeddings change based on surrounding text</li> <li>Learned during pre-training</li> </ul>"},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#example","title":"Example","text":"<p>```text Sentence 1: She picked a rose. Sentence 2: The sun rose early.</p>"},{"location":"study-path/transformers-generative-ai/04-transformer-intro/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>Attention Is All You Need (Original Transformer Paper)</li> <li>The Illustrated Transformer by Jay Alammar</li> <li>Hugging Face Transformers Documentation</li> <li>Google Colab: Transformer Architecture Notebook</li> <li>Hugging Face: T5-base Model (Amazon Product Reviews)</li> <li>Hugging Face: diabloGPT Instruction Model</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/","title":"Transformer Architectures Study Hub","text":""},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. BERT: Encoder-Only Transformer Architecture</li> <li>2. Transformer &amp; GPT Evolution</li> <li>3. T5: Text-To-Text Transfer Transformer</li> </ul>"},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#1-bert-encoder-only-transformer-architecture","title":"1. BERT: Encoder-Only Transformer Architecture","text":"<p>BERT (Bidirectional Encoder Representations from Transformers) revolutionized NLP in 2018 by introducing a bidirectional, encoder-only architecture designed for deep contextual understanding of language. This section explores BERT\u2019s structure, training strategy, practical applications, and the latest advancements in its ecosystem.</p>"},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#model-overview","title":"Model Overview","text":""},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#key-characteristics","title":"Key Characteristics","text":"<ul> <li> <p>Bidirectional   BERT reads text in both directions (left-to-right and right-to-left) simultaneously to capture full context.</p> </li> <li> <p>Encoder-Only Architecture   Built entirely on stacked encoders with self-attention mechanisms.   Optimized for understanding, not generating, text.</p> </li> <li> <p>Representations   Learns dense vector embeddings that reflect token meaning in context.</p> </li> <li> <p>Transformer-Based   Leverages the original transformer architecture\u2014only the encoder side.</p> </li> </ul>"},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#pre-training-strategy","title":"Pre-training Strategy","text":""},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#datasets","title":"Datasets","text":"<ul> <li>English Wikipedia  </li> <li>10,000+ unpublished English books  </li> <li>Total: Over 3 billion words</li> </ul>"},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#pre-training-objectives","title":"Pre-training Objectives","text":"<ul> <li> <p>Masked Language Modeling (MLM)   Randomly masks 15% of tokens; the model must predict them using surrounding context.   Enables deep semantic and syntactic comprehension.</p> </li> <li> <p>Next Sentence Prediction (NSP)   Trains BERT to classify whether one sentence follows another.   Aids understanding of inter-sentence relationships.   Later models (e.g., RoBERTa) removed this due to limited benefit.</p> </li> </ul>"},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#fine-tuning-applications","title":"Fine-tuning Applications","text":""},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#text-classification","title":"Text Classification","text":"<ul> <li>Sentiment analysis, spam detection, topic categorization  </li> <li>Produces a single class label from the encoded text</li> </ul>"},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#named-entity-recognition-ner","title":"Named Entity Recognition (NER)","text":"<ul> <li>Identifies token-level entities (e.g., people, dates, organizations)  </li> <li>BERT's contextual awareness improves accuracy in boundary detection</li> </ul>"},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#extractive-question-answering","title":"Extractive Question Answering","text":"<ul> <li>Extracts answers directly from a provided context passage  </li> <li>Predicts start and end token positions  </li> <li>Used in customer service, document retrieval</li> </ul>"},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#semantic-similarity","title":"Semantic Similarity","text":"<ul> <li>Produces embeddings for entire sentences or passages  </li> <li>Used in:</li> <li>Duplicate detection  </li> <li>Paraphrase recognition  </li> <li>Semantic search  </li> <li>Vector-based retrieval systems</li> </ul>"},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#bert-model-variants","title":"BERT Model Variants","text":"Model Parameters Notes BERT-Base ~110M 12 layers, 12 heads, 768 hidden units BERT-Large ~340M 24 layers, 16 heads, 1024 hidden units DistilBERT ~66M Lightweight version by Hugging Face RoBERTa ~125M+ No NSP, trained longer, dynamic masking (Meta) ALBERT ~12M\u2013223M Weight-sharing, efficient training (Google Research) DeBERTa Varies Disentangled attention and enhanced position embeddings (Microsoft)"},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#latest-developments-as-of-2025","title":"Latest Developments (as of 2025)","text":"<ul> <li>BERT is foundational for retrieval-augmented generation (RAG) and embedding-based search systems.</li> <li>Multilingual BERT (mBERT) supports 100+ languages.</li> <li>BERT encoders are commonly paired with large decoders like GPT-4o for hybrid retrieval-generation systems.</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#2-transformer-gpt-evolution","title":"2. Transformer &amp; GPT Evolution","text":""},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#gpt-45-orion","title":"GPT-4.5 (\u201cOrion\u201d)","text":"<ul> <li>Released: Feb 27, 2025</li> <li>Enhanced instruction-following, fewer hallucinations</li> <li>API &amp; ChatGPT Pro access</li> </ul>"},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#gpt-41-family","title":"GPT-4.1 Family","text":"<ul> <li>Released: April 14, 2025</li> <li>Includes mini/nano variants supporting 1M-token context</li> <li>More efficient than GPT-4o</li> </ul>"},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#reasoning-models-o1-o3-mini-o4-mini","title":"Reasoning Models (o1, o3-mini, o4-mini)","text":"<ul> <li>Optimized for logic, math, and science</li> <li>o3-mini and o4-mini include multimodal chain-of-thought support</li> <li>Ideal for autonomous agents and structured tool use</li> </ul>"},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#gpt-5-expected-august-2025","title":"GPT-5 (Expected August 2025)","text":"<ul> <li>Will include reasoning from o3</li> <li>Multimodal + open access discussions ongoing</li> <li>Expected to set a new benchmark for general-purpose AI</li> </ul>"},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#why-it-matters","title":"Why It Matters","text":"<ul> <li>Shift from scaling parameters to scaling reasoning</li> <li>GPT-4.5/5 marks evolution toward modular, low-latency, high-accuracy models</li> </ul> Model Category Architecture Strengths Use Cases GPT\u20114.5 Instructional GPT Decoder-only Prompt-following, fewer hallucinations General NLP, coding, chatbots GPT\u20114.1 mini Efficient GPT Decoder-only 1M context, fast inference Coding, RAG o3-mini Reasoning LLM Decoder-only Logic + math + tool use Agents, science tasks GPT\u20115 Unified Multi-module Multimodal, reasoning-first Enterprise AI, general AI <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#3-t5-text-to-text-transfer-transformer","title":"3. T5: Text-To-Text Transfer Transformer","text":"<p>T5 reframes every NLP problem as a text-to-text task (e.g., input: \u201cTranslate English to German: How are you?\u201d \u2192 output: \u201cWie geht es dir?\u201d). This unified approach enables a wide range of applications across translation, QA, summarization, and more.</p>"},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#model-overview_1","title":"Model Overview","text":"<ul> <li>Encoder-decoder transformer with BERT-style encoding + GPT-style generation</li> <li>Flexible task control via text prefixes (e.g., \u201csummarize:\u201d, \u201ctranslate:\u201d)</li> <li>First model to fully embrace text-to-text multitask learning</li> </ul>"},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#pre-training-c4-dataset-fill-in-the-blank-generation","title":"Pre-training: C4 Dataset + Fill-in-the-Blank Generation","text":"<ul> <li>Uses a corrupt-and-reconstruct pre-training objective</li> <li>Learns both contextual understanding and sequence generation</li> <li>Trained on C4 (Colossal Cleaned Crawled Corpus)</li> </ul>"},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#key-use-cases","title":"Key Use Cases","text":"<ul> <li>Translation: Understands bidirectional input, generates fluent target text</li> <li>Summarization: Converts long passages into concise summaries</li> <li>Question Answering: Context-aware, generative answers</li> <li>Keyword Generation: Contextual phrase extraction</li> </ul>"},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#product-evolution-table","title":"Product Evolution Table","text":"Model Architecture Strengths Use Cases Developer T5-Base Encoder-Decoder Multitask learning, flexible Translation, QA, summarization Google AI mT5 Encoder-Decoder Multilingual model (100+ langs) Cross-lingual NLP Google AI FLAN-T5 Enc-Dec + Tuning Instruction tuning Zero-shot &amp; few-shot NLP Google Research UL2 Encoder-Decoder Supports multiple objective modes General-purpose transformer Google DeepMind Gemini 1.5 Multimodal Unified vision + text + code Multimodal reasoning, generation Google DeepMind"},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#takeaways","title":"Takeaways","text":"<ul> <li>T5 demonstrates the power of a unified framework in solving diverse NLP tasks</li> <li>Its design has influenced instruction-tuned and multimodal model families</li> <li>Continues to power a range of Google products and NLP pipelines</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers-generative-ai/05-popular-transformer-models/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Google Research)</li> <li>RoBERTa: A Robustly Optimized BERT Pretraining Approach (Meta AI)</li> <li>DeBERTa: Decoding-enhanced BERT with Disentangled Attention (Microsoft)</li> <li>DistilBERT by Hugging Face (Model Page)</li> <li>mBERT: Multilingual BERT (Google AI)</li> <li>T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</li> <li>FLAN-T5 Instruction-Tuned Models (Google Research)</li> <li>UL2: Unified Language Learning</li> <li>Gemini 1.5 Model Overview (Google DeepMind)</li> <li>GPT-4.5 and GPT-5 Updates (OpenAI Blog)</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers-generative-ai/06-using-transformers/","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Course Overview</li> <li>Key Concepts</li> <li>Tokenizer and Embeddings</li> <li>Masked Language Modeling</li> <li>Semantic Search Engine</li> <li>Model Evolution Table</li> </ul>"},{"location":"study-path/transformers-generative-ai/06-using-transformers/#course-overview","title":"Course Overview","text":"<p>This section focuses on transitioning from theoretical knowledge of transformer models to their practical implementation and engineering components, emphasizing real-world applications such as semantic search and embedding usage.</p> <ul> <li>Prepares learners to apply transformer embeddings for NLP tasks</li> <li>Covers tokenization, embeddings, model internals, and downstream tasks</li> <li>Includes practical hands-on coding with Hugging Face Transformers and PyTorch</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/06-using-transformers/#key-concepts","title":"Key Concepts","text":""},{"location":"study-path/transformers-generative-ai/06-using-transformers/#transformer-engineering-focus","title":"Transformer Engineering Focus","text":"<ul> <li>Embeddings: Represent words/sentences as dense vectors for downstream processing</li> <li>Tokenization: Converts raw text to token IDs; includes handling special tokens</li> <li>Attention Mechanism: Key to contextual representation in transformers</li> <li>Model Inputs: Includes token IDs, attention masks, and token type IDs</li> <li>Sentence Transformers: Fine-tuned models for capturing sentence-level semantics</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/06-using-transformers/#tokenizer-and-embeddings","title":"Tokenizer and Embeddings","text":""},{"location":"study-path/transformers-generative-ai/06-using-transformers/#tokenization-pipeline","title":"Tokenization Pipeline","text":"<ul> <li>Tokenizers split sentences into subword tokens</li> <li>Maintains a vocabulary of ~30k+ tokens</li> <li>Returns token IDs, attention masks, and token type IDs</li> <li>Important to use model-specific tokenizers for consistency</li> </ul>"},{"location":"study-path/transformers-generative-ai/06-using-transformers/#try-it-yourself","title":"Try It Yourself","text":"<p>Explore and run the notebook interactively using Google Colab:</p> <p>Open the Notebook in Colab</p> <p></p>"},{"location":"study-path/transformers-generative-ai/06-using-transformers/#embeddings","title":"Embeddings","text":"<ul> <li>Token IDs are converted to high-dimensional vectors</li> <li>Two key outputs:</li> <li>Last Hidden State: Embeddings for individual tokens (shape: seq_len \u00d7 hidden_dim)</li> <li>Pooled Output: Embedding for the entire sequence, used in classification</li> </ul>"},{"location":"study-path/transformers-generative-ai/06-using-transformers/#try-it-yourself_1","title":"Try It Yourself","text":"<p>You can run and explore the notebook directly in Google Colab:</p> <p>Open the Notebook in Colab</p> <p></p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/06-using-transformers/#semantic-distance","title":"Semantic Distance","text":"<ul> <li>Embeddings compared using cosine similarity</li> <li>Allows words with different meanings (e.g., \"fly\") to be distinguished contextually</li> </ul>"},{"location":"study-path/transformers-generative-ai/06-using-transformers/#masked-language-modeling","title":"Masked Language Modeling","text":"<ul> <li>Pretraining task for models like BERT</li> <li>Random tokens replaced with <code>[MASK]</code> and predicted by the model</li> <li>Output logits converted to probabilities via softmax</li> <li>Used to help the model build a strong language understanding foundation</li> </ul>"},{"location":"study-path/transformers-generative-ai/06-using-transformers/#example","title":"Example","text":"<ul> <li>Input: <code>\"I want to [MASK] pizza for tonight\"</code></li> <li>Output: <code>\"have\"</code>, <code>\"get\"</code>, <code>\"eat\"</code>, <code>\"make\"</code>, <code>\"order\"</code> as top predictions</li> </ul>"},{"location":"study-path/transformers-generative-ai/06-using-transformers/#try-it-yourself_2","title":"Try It Yourself","text":"<p>You can experiment with the code by opening the notebook in Google Colab:</p> <p>Open the Notebook in Colab</p> <p></p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/06-using-transformers/#semantic-search-engine","title":"Semantic Search Engine","text":""},{"location":"study-path/transformers-generative-ai/06-using-transformers/#goal","title":"Goal","text":"<p>Build a semantic search engine that finds the most relevant document to a query based on meaning, not keyword match.</p>"},{"location":"study-path/transformers-generative-ai/06-using-transformers/#tools-dataset","title":"Tools &amp; Dataset","text":"<ul> <li>Dataset: Multi-News (2000 article summaries)</li> <li>Model: SentenceTransformer for lightweight sentence embeddings (384-dim)</li> <li>Libraries: Hugging Face Transformers, PyTorch, Pandas</li> </ul>"},{"location":"study-path/transformers-generative-ai/06-using-transformers/#process","title":"Process","text":"<ul> <li>Embed all documents once</li> <li>Embed user\u2019s query</li> <li>Compute cosine similarity between query and all document embeddings</li> <li>Retrieve top-k relevant results using <code>torch.topk</code></li> </ul>"},{"location":"study-path/transformers-generative-ai/06-using-transformers/#example-queries","title":"Example Queries","text":"<ul> <li>\"Artificial Intelligence\": returned AI-related articles</li> <li>\"Natural Disasters\": returned disaster-related summaries</li> <li>\"Law Enforcement\", \"Politics\": worked as expected</li> </ul>"},{"location":"study-path/transformers-generative-ai/06-using-transformers/#try-it-yourself_3","title":"Try It Yourself","text":"<p>Give it a try by opening the interactive Google Colab notebook below:</p> <p>Open the Notebook in Colab</p> <p></p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/06-using-transformers/#model-evolution-table","title":"Model Evolution Table","text":"Model Name Category Architecture Strengths Ideal Use Cases Latest Version Info BERT Encoder-only Transformer Bidirectional context, strong understanding Text classification, Q&amp;A, embedding generation BERT-Base / BERT-Large GPT Decoder-only Transformer Text generation, instruction following Chatbots, creative writing, code generation GPT-4o (June 2024) T5 Encoder-Decoder Transformer Unified text-to-text architecture Translation, summarization, Q&amp;A T5.1.1, Flan-T5 Gemini Multi-modal Transformer + Vision + Memory Text + image processing, powerful LLM+VLM hybrid Multi-modal tasks, agentic reasoning Gemini 1.5 (June 2025) SentenceTransformer Encoder-only Siamese / Bi-encoder Transformer Sentence similarity, semantic search Embedding generation, retrieval, clustering <code>all-MiniLM-L6-v2</code> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/06-using-transformers/#references-further-exploration","title":"References &amp; Further Exploration","text":"<ul> <li>\ud83e\udd17 Hugging Face Models and Tools</li> <li>BERT (bert-base-uncased)</li> <li>GPT-2 (gpt2)</li> <li>T5 (t5-base)</li> <li>SentenceTransformer (all-MiniLM-L6-v2)</li> <li> <p>T5-based Amazon Product Review Generator by TheFuzzyScientist</p> </li> <li> <p>\ud83d\udcd3 Colab Notebooks (Used in This Module)</p> </li> <li>Tokenizer &amp; Embeddings Colab</li> <li>Masked Language Modeling (MLM) Demo</li> <li>Semantic Search with Transformers</li> <li> <p>Tokenizer Pipeline Walkthrough</p> </li> <li> <p>\ud83d\udcda Further Reading</p> </li> <li>Attention Is All You Need (Vaswani et al.)</li> <li>The Illustrated Transformer (Jay Alammar)</li> <li>Hugging Face Transformers Documentation</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers-generative-ai/07-real-world-scenario-llm/","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Course Overview</li> <li>Part 1: NLP &amp; Transformer Fundamentals</li> <li>Part 2: Practical LLM Applications</li> <li>Model Comparison Summary</li> <li>Key Takeaways</li> <li>References &amp; Further Exploration</li> </ul>"},{"location":"study-path/transformers-generative-ai/07-real-world-scenario-llm/#course-overview","title":"Course Overview","text":"<p>This course is a hands-on introduction to transformer-based language models, combining theoretical foundations with practical implementations. The curriculum covers BERT, GPT, and T5 models, including their use in real-world NLP tasks.</p>"},{"location":"study-path/transformers-generative-ai/07-real-world-scenario-llm/#part-1-nlp-transformer-fundamentals","title":"Part 1: NLP &amp; Transformer Fundamentals","text":""},{"location":"study-path/transformers-generative-ai/07-real-world-scenario-llm/#historical-phases-of-nlp","title":"Historical Phases of NLP","text":"<ul> <li>Rule-Based Systems: Manually defined linguistic rules</li> <li>Statistical Methods: Word co-occurrence and probabilistic models</li> <li>Machine Learning: Feature-based methods (e.g., SVM, Naive Bayes)</li> <li>Deep Learning: Dense vector embeddings and neural models</li> </ul>"},{"location":"study-path/transformers-generative-ai/07-real-world-scenario-llm/#core-transformer-concepts","title":"Core Transformer Concepts","text":"<ul> <li>Attention Mechanism: Enables global contextual representation</li> <li>Tokenization: Breaks text into subwords with positional info</li> <li>Encoder-Decoder: Structure used in models like T5, BART</li> <li>Fine-tuning: Adjusts pretrained models for specific tasks</li> </ul> <p>\ud83d\udd17 Reference: Illustrated Transformer \u2013 Jay Alammar</p>"},{"location":"study-path/transformers-generative-ai/07-real-world-scenario-llm/#part-2-practical-llm-applications","title":"Part 2: Practical LLM Applications","text":""},{"location":"study-path/transformers-generative-ai/07-real-world-scenario-llm/#bert-extractive-question-answering","title":"\ud83d\udfe2 BERT \u2013 Extractive Question Answering","text":"<ul> <li>Extracts an answer span from context using start and end logits</li> <li>Ideal for closed-domain QA</li> <li>Handles context chunks using stride</li> </ul> <p>\ud83d\udc49 Open in Colab </p> <p>\ud83d\udd17 Model: bert-base-uncased \ud83d\udcc4 Paper: BERT: Pre-training of Deep Bidirectional Transformers</p>"},{"location":"study-path/transformers-generative-ai/07-real-world-scenario-llm/#gpt-instruction-following-generation","title":"\ud83d\udd35 GPT \u2013 Instruction-Following Generation","text":"<ul> <li>Trained using causal language modeling</li> <li>Uses instruction + response prompts</li> <li>Fine-tuned with Open-Instruct dataset</li> </ul> <p>\ud83d\udc49 Open in Colab </p> <p>\ud83d\udd17 Model: DiabloGPT on Hugging Face \ud83d\udcc4 Paper: GPT-2 \ud83d\udcc4 Dataset: Open-Instruct</p>"},{"location":"study-path/transformers-generative-ai/07-real-world-scenario-llm/#t5-text-to-text-product-review-generation","title":"\ud83d\udd34 T5 \u2013 Text-to-Text Product Review Generation","text":"<ul> <li>Treats all tasks as text-to-text (e.g., <code>summarize:</code> or <code>translate:</code>)</li> <li>Pretrained on C4 corpus with span corruption</li> <li>Ideal for summarization, QA, translation, and generation</li> </ul> <p>\ud83d\udc49 Open in Colab </p> <p>\ud83d\udd17 Model: T5-base, Amazon Review Model \ud83d\udcc4 Paper: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</p>"},{"location":"study-path/transformers-generative-ai/07-real-world-scenario-llm/#model-comparison-summary","title":"Model Comparison Summary","text":"Model Architecture Directionality Pretraining Task Ideal Use Cases Limitations BERT Encoder-only Bidirectional Masked Language Modeling QA, classification, embeddings 512-token limit GPT-2 Decoder-only Unidirectional Causal Language Modeling Instruction generation, chatbots No bidirectional context T5 Encoder-Decoder Bi/Uni (input/output) Span corruption (text-to-text) Summarization, QA, translation Needs task-specific prompt Gemini Multi-modal Flexible MoE + RLHF + VLM Multimodal generation, reasoning Closed-source"},{"location":"study-path/transformers-generative-ai/07-real-world-scenario-llm/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Leverage pretrained models to reduce time and cost</li> <li>Use token chunking and stride for input limits</li> <li>Even small models like GPT-2 perform well when fine-tuned</li> <li>T5\u2019s text-to-text design enables flexibility across tasks</li> </ul>"},{"location":"study-path/transformers-generative-ai/07-real-world-scenario-llm/#references-further-exploration","title":"References &amp; Further Exploration","text":""},{"location":"study-path/transformers-generative-ai/07-real-world-scenario-llm/#foundational-papers","title":"\ud83e\udde0 Foundational Papers","text":"<ul> <li>Attention Is All You Need</li> <li>BERT: Pre-training of Deep Bidirectional Transformers</li> <li>GPT-2</li> <li>T5</li> </ul>"},{"location":"study-path/transformers-generative-ai/07-real-world-scenario-llm/#hugging-face-models","title":"\ud83e\udd17 Hugging Face Models","text":"<ul> <li>bert-base-uncased</li> <li>gpt2</li> <li>t5-base</li> <li>DiabloGPT</li> <li>Amazon T5 Review Model</li> </ul>"},{"location":"study-path/transformers-generative-ai/07-real-world-scenario-llm/#colab-notebooks","title":"\ud83e\uddea Colab Notebooks","text":"<ul> <li>BERT QA</li> <li>GPT Instruction Tuning</li> <li>T5 Product Review Generator</li> </ul>"},{"location":"study-path/transformers-generative-ai/08-llm-intro/","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Course Overview</li> <li>What is a Large Language Model?</li> <li>Decoder-Only Architecture</li> <li>Chat Templates &amp; Structured Inputs</li> <li>Model Selection on Hugging Face</li> <li>Code Demonstration: TinyLlama</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers-generative-ai/08-llm-intro/#course-overview","title":"Course Overview","text":"<p>This section explores Large Language Models (LLMs) built on the transformer architecture, their training procedures, deployment challenges, and how they are applied in real-world interactive systems. The goal is to bridge conceptual understanding with hands-on implementation.</p> <ul> <li>Core Focus:</li> <li>Decoder-only transformer models</li> <li>Tokenization &amp; input formatting</li> <li>Reinforcement Learning from Human Feedback (RLHF)</li> <li>Chat templates</li> <li>Model selection and generation parameters</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/08-llm-intro/#what-is-a-large-language-model","title":"What is a Large Language Model?","text":"<p>LLMs refer to powerful NLP models capable of generating complex, human-like text. They\u2019re built using decoder-only transformer architectures and trained at scale using massive datasets.</p> <ul> <li>Scale:</li> <li>Models like LLaMA-3 and GPT-4 have up to 70+ billion parameters.</li> <li>Small LLMs (e.g., 2\u20137B) are optimized for consumer hardware.</li> <li>Architecture:</li> <li>Modern LLMs are generally decoder-only models.</li> <li>Capabilities:</li> <li>High factual recall</li> <li>Scalable deployment</li> <li>Robust contextual understanding</li> <li>Challenges:</li> <li>Hallucination</li> <li>Deployment complexity</li> <li>High compute requirements</li> </ul> <p>\ud83e\udde0 Despite limitations, they\u2019ve surpassed average human factual knowledge.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/08-llm-intro/#decoder-only-architecture","title":"Decoder-Only Architecture","text":""},{"location":"study-path/transformers-generative-ai/08-llm-intro/#key-properties","title":"Key Properties","text":"<ul> <li>LLMs process inputs as a single concatenated sequence.</li> <li>Interaction is simulated using autoregession, where the model predicts the next token.</li> <li>Requires clever input formatting to mimic input-output behavior.</li> </ul>"},{"location":"study-path/transformers-generative-ai/08-llm-intro/#fine-tuning-techniques","title":"Fine-tuning Techniques","text":"<ul> <li>Supervised Fine-Tuning: Uses input-response pairs to guide expected outputs.</li> <li>Reinforcement Learning from Human Feedback (RLHF):</li> <li>Multiple responses are generated.</li> <li>Human annotators rank responses.</li> <li>Used to improve contextual accuracy and helpfulness.</li> </ul> <p>\ud83d\udcca Illustration:</p> <p> Source: Jay Alammar\u2019s GPT2 visual guide</p>"},{"location":"study-path/transformers-generative-ai/08-llm-intro/#applications","title":"Applications:","text":"<ul> <li>Chatbots  </li> <li>Code generation  </li> <li>Instruction following  </li> <li>Document summarization</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/08-llm-intro/#chat-templates-structured-inputs","title":"Chat Templates &amp; Structured Inputs","text":"<p>LLMs simulate dialogue using chat templates that structure user-assistant messages.</p>"},{"location":"study-path/transformers-generative-ai/08-llm-intro/#input-structure","title":"Input Structure","text":"<ul> <li>A \"conversation\" is a series of messages with:</li> <li><code>role</code>: Identifies speaker (user/assistant)</li> <li><code>content</code>: Message text</li> </ul>"},{"location":"study-path/transformers-generative-ai/08-llm-intro/#model-specific-template-examples","title":"Model-specific Template Examples","text":"Model Structure Type Special Tokens Role Awareness Instruction Capable Blenderbot Basic concat \u274c \u274c \u274c Mistral Instruction tokens \u2705 \u26a0\ufe0f (Partial) \u2705 Gemma Turn-based format \u2705\u2705 \u2705 \u2705\u2705 LLaMA 3 Header tokens \u2705\u2705\u2705 \u2705 \u2705\u2705\u2705 <p>\ud83e\udde9 These templates are essential during fine-tuning to teach models interaction patterns.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/08-llm-intro/#model-selection-on-hugging-face","title":"Model Selection on Hugging Face","text":"<p>\ud83d\udee0\ufe0f Choosing the right LLM impacts performance, cost, and resource needs.</p>"},{"location":"study-path/transformers-generative-ai/08-llm-intro/#what-to-look-for","title":"What to Look For:","text":"<ul> <li>Model Family: LLaMA, Mistral, Phi, Gemma, etc.</li> <li>Size (Parameters):</li> <li>Small: 2B\u20137B</li> <li>Medium: 13B\u201334B</li> <li>Large: 70B+</li> <li>Instruction-Following:</li> <li>Look for <code>instruct</code> or <code>chat</code> variants</li> <li>Context Length:</li> <li>Defined via <code>max_position_embeddings</code> in <code>config.json</code></li> <li>Affects how much prompt+response can be handled</li> </ul> <p>\ud83d\udca1 Hugging Face Model Hub: \ud83d\udd17 Browse Models</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/08-llm-intro/#code-demonstration-tinyllama","title":"Code Demonstration: TinyLlama","text":"<p>\ud83d\udc49 Open in Colab </p> <p>We explore TinyLlama to demonstrate basic generation and parameter tuning.</p>"},{"location":"study-path/transformers-generative-ai/08-llm-intro/#workflow","title":"Workflow","text":"<ul> <li>Load model + tokenizer  </li> <li>Prepare chat messages using templates  </li> <li>Encode as tokens  </li> <li>Generate response  </li> <li>Decode and analyze output  </li> </ul>"},{"location":"study-path/transformers-generative-ai/08-llm-intro/#key-generation-parameters","title":"Key Generation Parameters","text":"Parameter Description <code>max_new_tokens</code> Limits length of generated response <code>temperature</code> Controls creativity/randomness (higher = more) <code>top_p</code> Nucleus sampling: restricts to top % of prob. <code>do_sample</code> Enables randomness in output <p>\ud83d\udfe2 Temperature Examples:</p> <ul> <li>1.0 \u2192 Creative, varied responses  </li> <li>0.1 \u2192 Deterministic, factual outputs  </li> </ul> <p>\ud83d\udcce Prompt token count impacts total input length (important for context fitting).</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/08-llm-intro/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>\ud83d\udcdc Attention Is All You Need (Vaswani et al.)</li> <li>\ud83e\udd17 Hugging Face Transformers Documentation</li> <li>\ud83d\uddbc\ufe0f Jay Alammar\u2019s Illustrated Transformer</li> <li>\ud83e\udde0 LLaMA 3 on Hugging Face</li> <li>\ud83d\udcd8 RLHF Explained \u2013 Hugging Face Blog</li> <li>\ud83d\udcc4 OpenAI: ChatGPT Fine-tuning Guide</li> <li>\ud83d\udcda Gemma Tokenizer Guide \u2013 Google</li> <li>\ud83d\udd2c Microsoft Phi Models on Hugging Face</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/","title":"9. Preparing LLMs","text":""},{"location":"study-path/transformers-generative-ai/09-llm-prep/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Comprehensive Dive into Sequence Length</li> <li>Token Counts: Practical Intuition &amp; Impact</li> <li>Precision Matters: Numerical Precision in Training</li> <li>Navigating GPU Selection: A Guide to Hardware Platform</li> <li>Practice Fundamentals: Most Basic Form of Training LLMs</li> <li>Practice Fundamentals Part 2: Most Basic Form of Training LLMs</li> <li>Practice Fundamentals Part 3: Most Basic Form of Training LLMs</li> </ul>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#comprehensive-dive-into-sequence-length","title":"\ud83d\udcd8 Comprehensive Dive into Sequence Length","text":""},{"location":"study-path/transformers-generative-ai/09-llm-prep/#quick-navigation_1","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Understanding Sequence Length </li> <li>Why Sequence Length Matters </li> <li>Hardware Implications </li> <li>Impact on Task Suitability </li> <li>Use Cases: Short vs Long Sequences </li> <li>Guidelines for Choosing Sequence Length </li> <li>References &amp; Further Reading </li> </ul>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#understanding-sequence-length","title":"Understanding Sequence Length","text":"<p>In this foundational lesson, we examine the concept of sequence length in large language models (LLMs), particularly as it relates to fine-tuning and model design. Sequence length determines how much context a model can consider during training and inference. Once a model is trained with a specific maximum sequence length, this cannot be extended without retraining.</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#key-concepts","title":"Key Concepts","text":"<ul> <li>Sequence Length defines the number of tokens a model can process at once.  </li> <li>This parameter is fixed after pretraining.  </li> <li>You can feed shorter inputs into a longer-trained model, but not vice versa.  </li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#why-sequence-length-matters","title":"Why Sequence Length Matters","text":""},{"location":"study-path/transformers-generative-ai/09-llm-prep/#fixed-architecture-constraint","title":"Fixed Architecture Constraint","text":"<ul> <li>Pretraining fixes the maximum window size (e.g., 4K, 8K, 16K tokens).  </li> <li>Longer contexts require higher computational resources and memory.  </li> </ul>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#unidirectional-compatibility","title":"Unidirectional Compatibility","text":"<ul> <li>Models trained with longer windows can handle shorter inputs effortlessly.  </li> <li>Short-window models cannot be upgraded to handle longer contexts post hoc.  </li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#hardware-implications","title":"Hardware Implications","text":"<p>Sequence length directly impacts training and inference costs:</p> <ul> <li>Longer sequence lengths = higher VRAM requirements </li> <li>Training larger windows is exponentially slower </li> <li>Even inference (e.g., chatbots) demands more memory with longer inputs  </li> </ul> <p>Modern workarounds:</p> <ul> <li>Sparse Attention (e.g., Longformer, BigBird)  </li> <li>Memory-augmented transformers (e.g., Transformer-XL)  </li> </ul> <p>These techniques allow partial mitigation of the cost explosion from large contexts.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#impact-on-task-suitability","title":"Impact on Task Suitability","text":"<p>The sequence length determines the range and complexity of tasks that LLMs can solve. Below is a breakdown:</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#short-sequences-128-512-tokens","title":"Short Sequences (128 - 512 tokens)","text":"<ul> <li>\u2705 Sentiment Analysis  </li> <li>\u2705 Language Detection  </li> <li>\u2705 Named Entity Recognition  </li> </ul> <p>Advantages:</p> <ul> <li>Faster training  </li> <li>Lower compute overhead  </li> <li>Context is usually local and easily chunkable  </li> </ul>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#long-sequences-2048-tokens","title":"Long Sequences (2048+ tokens)","text":"<ul> <li>\u2705 Long-form QA  </li> <li>\u2705 Document Summarization  </li> <li>\u2705 Scriptwriting / Story Generation  </li> <li>\u2705 Multi-turn Dialogue Systems  </li> </ul> <p>Advantages:</p> <ul> <li>Maintains global context  </li> <li>Enables high-fidelity content generation  </li> <li>Suitable for documents, books, and extended chat history  </li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#use-cases-short-vs-long-sequences","title":"Use Cases: Short vs Long Sequences","text":""},{"location":"study-path/transformers-generative-ai/09-llm-prep/#short-sequence-use-cases","title":"Short Sequence Use Cases","text":"<ul> <li>Sentiment Analysis: Determine tone from key phrases  </li> <li>NER: Recognize entities within short contexts  </li> <li>Language Identification: Detect language using just a few words  </li> </ul>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#long-sequence-use-cases","title":"Long Sequence Use Cases","text":"<ul> <li>Conversational AI: Maintain long-term context across multiple exchanges  </li> <li>Content Generation: Write consistent long-form narratives or reports  </li> <li>Document Understanding: Answer questions or summarize content from full documents  </li> </ul> <p>\ud83d\udd39 These applications demonstrate the practical trade-offs of context length in fine-tuning.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#guidelines-for-choosing-sequence-length","title":"Guidelines for Choosing Sequence Length","text":"Task Type Suggested Sequence Benefits Limitations Classification (Sentiment, NER) 128 - 512 tokens Efficient, fast inference Limited to local context Chatbots / Assistants 2048 - 8192 tokens Maintains conversational coherence Higher cost and latency Summarization 4096 - 16000 tokens Holistic document understanding Truncation risk if too short Code Generation 2048 - 8192 tokens Handles longer code blocks Needs longer memory if multi-file <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>\ud83d\udd17 Attention Is All You Need (Vaswani et al.) </li> <li>\ud83d\udd17 Hugging Face Transformers Documentation </li> <li>\ud83d\udd17 Jay Alammar: Illustrated Transformer </li> <li>\ud83d\udd17 Google Research: Efficient Transformers </li> <li>\ud83d\udd17 OpenAI: Scaling Laws for Neural Language Models </li> <li>\ud83d\udd17 Facebook AI: Long-Range Arena Benchmark </li> <li>\ud83d\udd17 NVIDIA Megatron-LM Documentation </li> <li>\ud83d\udd17 Microsoft DeepSpeed for Long Sequence Training </li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#token-counts-practical-intuition-impact","title":"\ud83d\udcd7 Token Counts: Practical Intuition &amp; Impact","text":""},{"location":"study-path/transformers-generative-ai/09-llm-prep/#quick-navigation_2","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview</li> <li>Tokenization Mechanics</li> <li>Tokenizer Comparisons</li> <li>Vocabulary Size &amp; Token Efficiency</li> <li>Model Comparison Table</li> <li>Colab Demo</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#overview","title":"Overview","text":"<p>This lesson introduces the practical impact of token counts in generative AI, with hands-on comparisons between various tokenizer behaviors and model capacities. By examining real-world input (e.g., Wikipedia pages), learners gain intuition about:</p> <ul> <li>Sequence length constraints in models like BERT, LLaMA, and Mistral.</li> <li>Vocabulary size trade-offs.</li> <li>Tokenization efficiency and its effect on model performance and training design.</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#tokenization-mechanics","title":"Tokenization Mechanics","text":"<ul> <li>Text tokenization converts raw text into input IDs and attention masks.</li> <li>Input IDs directly affect the maximum context size a model can handle.</li> <li>For example, the phrase <code>\"fuzzy scientist\"</code> gets broken into only 5 tokens by the LLaMA3 tokenizer.</li> </ul>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#case-study-wikipedia-paragraph-on-whales","title":"Case Study: Wikipedia Paragraph on Whales","text":"<ul> <li>~170 words = ~300 tokens (using LLaMA3).</li> <li>Word-to-token ratio is approximately 1.76x.</li> <li>Demonstrates how even short paragraphs can consume large portions of traditional transformer limits.</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#tokenizer-comparisons","title":"Tokenizer Comparisons","text":""},{"location":"study-path/transformers-generative-ai/09-llm-prep/#same-paragraph-different-tokenizers","title":"Same Paragraph, Different Tokenizers:","text":"<ul> <li>BERT: ~20 more tokens than LLaMA3.</li> <li>Mistral: ~30 more than BERT, ~50 more than LLaMA3.</li> </ul>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#full-section-1000-tokens","title":"Full Section (~1,000 tokens):","text":"<ul> <li>Fits within LLaMA3 and Mistral (8K\u201332K context sizes).</li> <li>Exceeds BERT\u2019s 512-token limit.</li> </ul>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#entire-wikipedia-page","title":"Entire Wikipedia Page:","text":"<ul> <li>LLaMA3: ~21,000 tokens</li> <li>Mistral: ~26,000 tokens</li> </ul> <p>\ud83d\udfe2 Mistral fits due to 32K context, but is near capacity.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#vocabulary-size-token-efficiency","title":"Vocabulary Size &amp; Token Efficiency","text":"Model Vocabulary Size Tokens Needed for Wiki Page Notes LLaMA3 128,000 21,000 More efficient; fewer tokens per input Mistral 32,000 26,000 Less efficient but smaller vocab size <ul> <li>Trade-off:</li> <li>Larger vocab \u2192 fewer tokens \u2192 more efficient inference</li> <li> <p>Smaller vocab \u2192 easier pretraining \u2192 more tokens used</p> </li> <li> <p>Tokenizer Strategy:</p> </li> <li>LLaMA3: Breaks input into fewer, more specific tokens.</li> <li>Mistral: Uses more tokens to represent same input.</li> </ul> <p>\ud83d\udccc Takeaway: Vocabulary size directly impacts token efficiency, model generalization, and context window utilization.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#model-comparison-table","title":"Model Comparison Table","text":"Model Directionality Max Context Length Vocab Size Token Efficiency Use Case Fit BERT Encoder-only 512 tokens ~30K \ud83d\udd34 Low QA, embeddings LLaMA3 Decoder-only 8K \u2013 128K tokens 128K \ud83d\udfe2 High Chat, summarization Mistral Decoder-only 32K tokens 32K \ud83d\udfe1 Medium Long-form generation <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#colab-demo","title":"Colab Demo","text":"<p>\ud83d\udc49 Open in Colab </p> <p>This exercise allows you to: - Load LLaMA3, Mistral, and BERT tokenizers - Input arbitrary text (e.g., Wikipedia) and compare token counts - Explore vocabulary-driven tokenization behaviors</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#back-to-top","title":"Back to Top","text":""},{"location":"study-path/transformers-generative-ai/09-llm-prep/#references-further-reading_1","title":"References &amp; Further Reading","text":"<ul> <li>\u201cAttention Is All You Need\u201d \u2013 Vaswani et al. (ArXiv)</li> <li>Hugging Face Transformers Docs</li> <li>Jay Alammar \u2013 Illustrated Transformer</li> <li>LLaMA3 Model Card (Hugging Face)</li> <li>Mistral Model Card (Hugging Face)</li> <li>Google Research on Subword Tokenization</li> <li>NVIDIA LLM Efficiency Resources</li> <li>Facebook AI Blog \u2013 Mistral Architecture Insights</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#precision-matters-numerical-precision-in-training","title":"\ud83d\udcd9 Precision Matters: Numerical Precision in Training","text":""},{"location":"study-path/transformers-generative-ai/09-llm-prep/#quick-navigation_3","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview</li> <li>Numerical Precision in Machine Learning</li> <li>Precision Formats Explained</li> <li>Model Size, Memory, and Precision Trade-offs</li> <li>Hardware Limitations and Use Cases</li> <li>Lower Precision Formats (INT8, 4-bit)</li> <li>Precision vs Speed: Hardware Implications</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#overview_1","title":"Overview","text":"<p>This lesson explores the role of numerical precision in training and deploying large language models (LLMs). Understanding how floating-point representations affect performance, memory efficiency, and hardware compatibility is crucial when working with multi-billion parameter models.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#numerical-precision-in-machine-learning","title":"Numerical Precision in Machine Learning","text":"<ul> <li>In ML, parameters (weights) are represented as floating-point numbers.</li> <li>Common format: float32 (32-bit), offering high accuracy but high memory cost.</li> <li>Trade-off: higher bit precision = better accuracy but slower training &amp; higher memory.</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#precision-formats-explained","title":"Precision Formats Explained","text":""},{"location":"study-path/transformers-generative-ai/09-llm-prep/#float32-fp32","title":"\ud83d\udd35 Float32 (FP32)","text":"<ul> <li>32 bits per number \u2192 4 bytes</li> <li>High accuracy</li> <li>High memory usage</li> </ul>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#float16-fp16-mixed-precision","title":"\ud83d\udfe2 Float16 (FP16) / Mixed Precision","text":"<ul> <li>16 bits per number \u2192 2 bytes</li> <li>Slight loss of precision, but enables:</li> <li>Half the memory</li> <li>Double computation speed (if hardware supports)</li> </ul>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#bf16-brain-floating-point-16","title":"\ud83d\udfe1 BF16 (Brain Floating Point 16)","text":"<ul> <li>Also 16-bit, optimized for machine learning</li> <li>Better gradient/weight representation</li> <li>Widely adopted in modern models</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#model-size-memory-and-precision-trade-offs","title":"Model Size, Memory, and Precision Trade-offs","text":"Precision Bits Bytes 8B Param Model Memory 70B Param Model Memory FP32 32 4 32 GB 280 GB FP16 16 2 16 GB 140 GB INT8 8 1 8 GB 70 GB 4-bit 4 0.5 4 GB 35 GB <p>\ud83e\udde0 Rule of Thumb: For FP16, memory = 2 \u00d7 parameter count in GB.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#hardware-limitations-and-use-cases","title":"Hardware Limitations and Use Cases","text":""},{"location":"study-path/transformers-generative-ai/09-llm-prep/#consumer-gpus","title":"Consumer GPUs","text":"<ul> <li>RTX 4090: 24GB VRAM</li> <li>Can run 8B models in inference mode using FP16</li> <li>Cannot support full training due to memory overhead</li> </ul>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#enterprise-gpus","title":"Enterprise GPUs","text":"<ul> <li>NVIDIA A100/H100: 40\u201380GB VRAM</li> <li>Can train 7\u201313B parameter models with FP16</li> <li>Need multi-GPU setups for models \u226570B</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#lower-precision-formats-int8-4-bit","title":"Lower Precision Formats (INT8, 4-bit)","text":"<ul> <li>INT8: 1 byte per param \u2192 8B model = 8 GB</li> <li>4-bit: 0.5 bytes per param \u2192 8B model = 4 GB</li> </ul> <p>\u2705 Pros: - Drastic memory savings - Enables huge models to fit on limited VRAM</p> <p>\u26a0\ufe0f Cons: - Lower precision may reduce model accuracy - Often used in inference, not training</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#precision-vs-speed-hardware-implications","title":"Precision vs Speed: Hardware Implications","text":"<ul> <li>GPUs are optimized for FP16/FP32</li> <li>Very low-precision formats (e.g., INT4) may:</li> <li>Require internal conversions</li> <li>Lead to slower inference</li> <li>Reduce ability to utilize full GPU throughput</li> </ul> <p>\ud83d\udccc In practice: - Use FP16/BF16 for training - INT8/INT4 for memory-constrained inference</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#references-further-reading_2","title":"References &amp; Further Reading","text":"<ul> <li>Mixed Precision Training - NVIDIA</li> <li>Google TPU BF16 Overview</li> <li>\u201c8-Bit Optimizers via Block-wise Quantization\u201d \u2013 Dettmers et al.</li> <li>Hugging Face Guide to Quantization</li> <li>Jay Alammar\u2019s Illustrated Transformer</li> <li>PyTorch AMP Documentation</li> <li>INT4 Quantization - Facebook AI</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#navigating-gpu-selection-a-guide-to-hardware-platform","title":"\ud83d\udcd5 Navigating GPU Selection: A Guide to Hardware Platform","text":""},{"location":"study-path/transformers-generative-ai/09-llm-prep/#quick-navigation_4","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview</li> <li>Platform Comparison</li> <li>Consumer Platforms</li> <li>Google Colab</li> <li>RunPod</li> <li>Vast.ai</li> <li>Enterprise &amp; Cloud Platforms</li> <li>Lambda Labs</li> <li>Google Cloud Platform (GCP)</li> <li>Amazon Web Services (AWS)</li> <li>Recommended Path for Learners</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#overview_2","title":"Overview","text":"<p>Selecting the right GPU platform is essential for training and deploying large language models. This chapter provides a comparative guide to free, consumer-grade, and enterprise-level GPU options. Whether you're a student experimenting with smaller models or a researcher training multi-billion parameter LLMs, the right hardware can make all the difference.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#platform-comparison","title":"Platform Comparison","text":"Platform Type Cost Best For Notes Google Colab Consumer/Free $0\u2013$11/mo Students, Quick Experiments Limited runtime &amp; GPU availability RunPod Consumer Pay-as-you-go Developers, Researchers Access to RTX 4090 and templates Vast.ai Peer-to-peer Lowest Technical Users Requires custom setup Lambda Labs Enterprise Premium High-performance DL workloads Best for large training GCP Cloud Variable Scalable ML solutions Complicated pricing AWS Cloud Expensive Production environments Complex and costly"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#back-to-top_1","title":"Back to Top","text":""},{"location":"study-path/transformers-generative-ai/09-llm-prep/#consumer-platforms","title":"Consumer Platforms","text":""},{"location":"study-path/transformers-generative-ai/09-llm-prep/#google-colab","title":"Google Colab","text":"<ul> <li>Provides a Jupyter-based interface with Google Drive integration.</li> <li>Free Tier:</li> <li>Limited GPU types (T4, K80)</li> <li>Session timeouts (~90 mins)</li> <li>Colab Pro ($11/month):</li> <li>Access to more powerful GPUs</li> <li>Longer sessions</li> <li>Best for: prototyping, student learning, light inference workloads</li> </ul> <p>\ud83d\udc49 Try Colab</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#runpod","title":"RunPod","text":"<ul> <li>Access to high-end GPUs like RTX 4090</li> <li>Straightforward hourly pricing</li> <li>Docker templates preconfigured for DL</li> <li>No long-term commitments</li> </ul> <p>\ud83d\udc49 Explore RunPod</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#vastai","title":"Vast.ai","text":"<ul> <li>Marketplace for renting idle GPUs from other users</li> <li>Potentially lowest prices</li> <li>Requires custom setup and technical knowledge</li> <li>Performance may vary based on provider</li> </ul> <p>\ud83d\udc49 Try Vast.ai</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#enterprise-cloud-platforms","title":"Enterprise &amp; Cloud Platforms","text":""},{"location":"study-path/transformers-generative-ai/09-llm-prep/#lambda-labs","title":"Lambda Labs","text":"<ul> <li>Designed for deep learning workloads</li> <li>Offers stable infrastructure and optimized environments</li> <li>Higher cost; suitable for long-term research training</li> </ul> <p>\ud83d\udc49 Visit Lambda</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#google-cloud-platform-gcp","title":"Google Cloud Platform (GCP)","text":"<ul> <li>Highly scalable</li> <li>Wide selection of GPU types (A100, T4, V100)</li> <li>Suitable for:</li> <li>Large-scale training pipelines</li> <li>Distributed training</li> <li>Steep learning curve and complex pricing</li> </ul> <p>\ud83d\udc49 Explore GCP</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#amazon-web-services-aws","title":"Amazon Web Services (AWS)","text":"<ul> <li>Most comprehensive cloud ecosystem</li> <li>Broad GPU instance support (P4, G5, etc.)</li> <li>Very flexible, but can become prohibitively expensive</li> <li>Recommended for:</li> <li>Production deployments</li> <li>Enterprise-grade inference</li> </ul> <p>\ud83d\udc49 Visit AWS</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#recommended-path-for-learners","title":"Recommended Path for Learners","text":"<ul> <li>Start with Google Colab (Free/Pro) for initial lessons and exploration.</li> <li>As you progress to heavier training:</li> <li>Move to RunPod for access to high-end consumer GPUs.</li> <li>Consider Lambda Labs for long-term deep learning needs.</li> <li>If cost is a constraint and you're technically inclined, Vast.ai may offer unbeatable pricing.</li> <li>Use GCP or AWS only if:</li> <li>You\u2019re deploying at scale</li> <li>You\u2019re familiar with managing cloud infrastructure</li> </ul>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#back-to-top_2","title":"Back to Top","text":""},{"location":"study-path/transformers-generative-ai/09-llm-prep/#references-further-reading_3","title":"References &amp; Further Reading","text":"<ul> <li>Google Colab</li> <li>RunPod</li> <li>Vast.ai</li> <li>Lambda Labs</li> <li>Google Cloud GPU Pricing</li> <li>AWS EC2 GPU Instances</li> <li>NVIDIA Deep Learning GPU Guide</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#practice-fundamentals-most-basic-form-of-training-llms","title":"\ud83e\uddea Practice Fundamentals: Most Basic Form of Training LLMs","text":""},{"location":"study-path/transformers-generative-ai/09-llm-prep/#quick-navigation_5","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview</li> <li>Platform Comparison</li> <li>Consumer Platforms</li> <li>Google Colab</li> <li>RunPod</li> <li>Vast.ai</li> <li>Enterprise &amp; Cloud Platforms</li> <li>Lambda Labs</li> <li>Google Cloud Platform (GCP)</li> <li>Amazon Web Services (AWS)</li> <li>Recommended Path for Learners</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#overview_3","title":"Overview","text":"<p>Selecting the right GPU platform is essential for training and deploying large language models. This chapter provides a comparative guide to free, consumer-grade, and enterprise-level GPU options. Whether you're a student experimenting with smaller models or a researcher training multi-billion parameter LLMs, the right hardware can make all the difference.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#platform-comparison_1","title":"Platform Comparison","text":"Platform Type Cost Best For Notes Google Colab Consumer/Free $0\u2013$11/mo Students, Quick Experiments Limited runtime &amp; GPU availability RunPod Consumer Pay-as-you-go Developers, Researchers Access to RTX 4090 and templates Vast.ai Peer-to-peer Lowest Technical Users Requires custom setup Lambda Labs Enterprise Premium High-performance DL workloads Best for large training GCP Cloud Variable Scalable ML solutions Complicated pricing AWS Cloud Expensive Production environments Complex and costly <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#consumer-platforms_1","title":"Consumer Platforms","text":""},{"location":"study-path/transformers-generative-ai/09-llm-prep/#google-colab_1","title":"Google Colab","text":"<ul> <li>Provides a Jupyter-based interface with Google Drive integration.</li> <li>Free Tier:</li> <li>Limited GPU types (T4, K80)</li> <li>Session timeouts (~90 mins)</li> <li>Colab Pro ($11/month):</li> <li>Access to more powerful GPUs</li> <li>Longer sessions</li> <li>Best for: prototyping, student learning, light inference workloads</li> </ul> <p>\ud83d\udc49 Try Colab</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#runpod_1","title":"RunPod","text":"<ul> <li>Access to high-end GPUs like RTX 4090</li> <li>Straightforward hourly pricing</li> <li>Docker templates preconfigured for DL</li> <li>No long-term commitments</li> </ul> <p>\ud83d\udc49 Explore RunPod</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#vastai_1","title":"Vast.ai","text":"<ul> <li>Marketplace for renting idle GPUs from other users</li> <li>Potentially lowest prices</li> <li>Requires custom setup and technical knowledge</li> <li>Performance may vary based on provider</li> </ul> <p>\ud83d\udc49 Try Vast.ai</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#enterprise-cloud-platforms_1","title":"Enterprise &amp; Cloud Platforms","text":""},{"location":"study-path/transformers-generative-ai/09-llm-prep/#lambda-labs_1","title":"Lambda Labs","text":"<ul> <li>Designed for deep learning workloads</li> <li>Offers stable infrastructure and optimized environments</li> <li>Higher cost; suitable for long-term research training</li> </ul> <p>\ud83d\udc49 Visit Lambda</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#google-cloud-platform-gcp_1","title":"Google Cloud Platform (GCP)","text":"<ul> <li>Highly scalable</li> <li>Wide selection of GPU types (A100, T4, V100)</li> <li>Suitable for:</li> <li>Large-scale training pipelines</li> <li>Distributed training</li> <li>Steep learning curve and complex pricing</li> </ul> <p>\ud83d\udc49 Explore GCP</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#amazon-web-services-aws_1","title":"Amazon Web Services (AWS)","text":"<ul> <li>Most comprehensive cloud ecosystem</li> <li>Broad GPU instance support (P4, G5, etc.)</li> <li>Very flexible, but can become prohibitively expensive</li> <li>Recommended for:</li> <li>Production deployments</li> <li>Enterprise-grade inference</li> </ul> <p>\ud83d\udc49 Visit AWS</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#recommended-path-for-learners_1","title":"Recommended Path for Learners","text":"<ul> <li>Start with Google Colab (Free/Pro) for initial lessons and exploration.</li> <li>As you progress to heavier training:</li> <li>Move to RunPod for access to high-end consumer GPUs.</li> <li>Consider Lambda Labs for long-term deep learning needs.</li> <li>If cost is a constraint and you're technically inclined, Vast.ai may offer unbeatable pricing.</li> <li>Use GCP or AWS only if:</li> <li>You\u2019re deploying at scale</li> <li>You\u2019re familiar with managing cloud infrastructure</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#references-further-reading_4","title":"References &amp; Further Reading","text":"<ul> <li>Google Colab</li> <li>RunPod</li> <li>Vast.ai</li> <li>Lambda Labs</li> <li>Google Cloud GPU Pricing</li> <li>AWS EC2 GPU Instances</li> <li>NVIDIA Deep Learning GPU Guide</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#practice-fundamentals-part-2-most-basic-form-of-training-llms","title":"\ud83e\uddea Practice Fundamentals Part 2: Most Basic Form of Training LLMs","text":""},{"location":"study-path/transformers-generative-ai/09-llm-prep/#quick-navigation_6","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview</li> <li>Training Control with YAML Config</li> <li>Model Setup Parameters</li> <li>Dataset &amp; Formatting Logic</li> <li>Training Configuration File (YAML)</li> <li>Colab Integration</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#overview_4","title":"Overview","text":"<p>In this chapter, we explore how to define and control your LLM training pipeline using Axolotl's YAML-based configuration system. The focus is on training a small decoder-only model (TinyLlama) to generate short stories based on prompts using a dataset from Hugging Face.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#training-control-with-yaml-config","title":"Training Control with YAML Config","text":"<p>Axolotl leverages <code>.yml</code> configuration files to simplify LLM training orchestration. Rather than scripting logic, users can define:</p> <ul> <li>Model checkpoint and architecture</li> <li>Tokenizer type</li> <li>Dataset path and format</li> <li>Training hyperparameters</li> <li>Output and logging setup</li> </ul> <p>This allows non-programmers or fast-moving practitioners to quickly train, tune, and test models.</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#back-to-top_3","title":"Back to Top","text":""},{"location":"study-path/transformers-generative-ai/09-llm-prep/#model-setup-parameters","title":"Model Setup Parameters","text":"<p>Key fields in the YAML:</p> <ul> <li><code>base_model</code>: Pretrained checkpoint (e.g., TinyLlama 1.1B Chat)</li> <li><code>model_type</code>: Architecture class (LlamaForCausalLM)</li> <li><code>tokenizer_type</code>: Hugging Face tokenizer class (LlamaTokenizer)</li> <li><code>sequence_length</code>: Input length cap (e.g., 1024)</li> <li><code>precision</code>: Uses <code>bf16</code> (brain float 16), auto-detected if supported</li> </ul> <p>These map to common fields expected by Hugging Face models and tokenizers. Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#dataset-formatting-logic","title":"Dataset &amp; Formatting Logic","text":"<p>Dataset used: <code>jaydenccc/AI_Storyteller_Dataset</code></p> <ul> <li>Contains:</li> <li><code>synopsis</code> \u2192 serves as instruction prompt</li> <li><code>short_story</code> \u2192 target output the model learns</li> </ul>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#formatting","title":"Formatting","text":"<pre><code>&lt;|user|&gt;\n {instruction} &lt;/s&gt;\n&lt;|assistant|&gt;\n {short_story}\n</code></pre> <ul> <li>Follows the LLaMA chat template</li> <li>Ensures correct message alignment in decoder-only models</li> <li>Axolotl handles the formatting and tokenization logic internally</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#training-configuration-file-yaml","title":"Training Configuration File (YAML)","text":"<p>Below is the <code>basic_train.yml</code> referenced in this lesson. You can include this block directly in your MkDocs site or host it as a downloadable file.</p> <pre><code># model params\nbase_model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\nmodel_type: LlamaForCausalLM\ntokenizer_type: LlamaTokenizer\n\n# dataset params\ndatasets:\n  - path: jaydenccc/AI_Storyteller_Dataset\n    type: \n      system_prompt: \"\"\n      field_system: system\n      field_instruction: synopsis\n      field_output: short_story\n      format: \"&lt;|user|&gt;\\n {instruction} &lt;/s&gt;\\n&lt;|assistant|&gt;\"\n      no_input_format: \"&lt;|user|&gt; {instruction} &lt;/s&gt;\\n&lt;|assistant|&gt;\"\n\noutput_dir: ./models/TinyLlama_Storyteller\n\n# model params\nsequence_length: 1024\nbf16: auto\ntf32: false\n\n# training params\nbatch_size: 4\nmicro_batch_size: 4\nnum_epochs: 4\noptimizer: adamw_bnb_8bit\nlearning_rate: 0.0002\nlogging_steps: 1\n</code></pre> <p>\u2705 You can also link this YAML as a raw GitHub file or store it in your docs/assets folder for users to download.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#colab-integration","title":"Colab Integration","text":"<p>\ud83d\udc49 Open in Colab </p> <ul> <li>Contains environment setup and config-based training loop</li> <li>Compatible with Colab Pro for GPU-based fine-tuning</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#references-further-reading_5","title":"References &amp; Further Reading","text":"<ul> <li>Axolotl GitHub</li> <li>Colab Exercise Notebook</li> <li>TinyLlama Model Card (Hugging Face)</li> <li>JaydenCCC AI Storytelling Dataset</li> <li>YAML Config Docs from Axolotl</li> <li>Hugging Face Transformers Docs Back to Top</li> </ul>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#practice-fundamentals-part-3-most-basic-form-of-training-llms","title":"\ud83e\uddea Practice Fundamentals Part 3: Most Basic Form of Training LLMs","text":""},{"location":"study-path/transformers-generative-ai/09-llm-prep/#quick-navigation_7","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview</li> <li>Starting the Training Loop</li> <li>Monitoring Loss and Epochs</li> <li>Testing the Trained Model</li> <li>Colab Integration</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#overview_5","title":"Overview","text":"<p>This chapter concludes the first end-to-end training workflow using Axolotl and TinyLlama. We run the training script, observe the model's learning progress, and evaluate its performance with sample prompts to test generalization.</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#back-to-top_4","title":"Back to Top","text":""},{"location":"study-path/transformers-generative-ai/09-llm-prep/#starting-the-training-loop","title":"Starting the Training Loop","text":"<p>Once the YAML configuration file (<code>basic_train.yml</code>) is ready, you can launch training by running:</p> <pre><code>python -m axolotl.cli.train basic_train.yml\n</code></pre> <ul> <li>Loads the specified model (TinyLlama-1.1B-Chat-v1.0)</li> <li>Tokenizes dataset <code>jaydenccc/AI_Storyteller_Dataset</code></li> <li>Starts training loop with live logging</li> </ul> <p>\ud83d\udfe2 Axolotl automatically applies formatting, precision, and optimizer choices from the YAML.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#monitoring-loss-and-epochs","title":"Monitoring Loss and Epochs","text":"<ul> <li>Training loss is printed at each step due to <code>logging_steps: 1</code></li> <li>For a small dataset:</li> <li>Training is fast (few minutes with 4 epochs)</li> <li>Loss decreases with each batch, indicating effective learning</li> </ul> <p>\ud83e\udde0 Despite the minimal size of the dataset, the model learns the task effectively due to repetition and targeted prompts. Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#testing-the-trained-model","title":"Testing the Trained Model","text":"<p>Here's a sample Python test script:</p> <pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"./models/TinyLlama_Storyteller\", \n    torch_dtype=torch.float16, \n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"./models/TinyLlama_Storyteller\")\n\n# Prompt: Bright student working with a fuzzy scientist\nprompt = \"&lt;|user|&gt;\nA bright student was working with the fuzzy scientist on a project.&lt;/s&gt;\n&lt;|assistant|&gt;\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**inputs, max_new_tokens=512)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n</code></pre>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#results","title":"Results:","text":"<ul> <li>The model returns coherent short stories in response to prompts like:</li> <li>\"A bright student was working with a fuzzy scientist on a project.\"</li> <li>\"A global mission for humanity through overcrowded cities.\"</li> </ul> <p>\ud83c\udfaf Even with limited training, the model generalizes narrative structure well.</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#back-to-top_5","title":"Back to Top","text":""},{"location":"study-path/transformers-generative-ai/09-llm-prep/#colab-integration_1","title":"Colab Integration","text":"<p>\ud83d\udc49 Open in Colab </p> <ul> <li>Covers end-to-end steps from setup to inference</li> <li>Ideal for GPU-restricted environments</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/09-llm-prep/#references-further-reading_6","title":"References &amp; Further Reading","text":"<ul> <li>Axolotl GitHub</li> <li>TinyLlama Hugging Face Model Card</li> <li>Hugging Face Transformers</li> <li>JaydenCCC Storytelling Dataset</li> <li>Colab Exercise Notebook</li> <li>Accelerated LLM Training - NVIDIA</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/","title":"10. Advanced LLM Training","text":""},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Understanding Practical Limitations</li> <li>Boosting Efficiency: PeFT and LoRA in Depth</li> <li>Managing Data Memory: Batch Size &amp; Sequence Length</li> <li>Advanced Solutions: Gradient Accumulation &amp; Checkpointing</li> <li>Fitting Giants: Practical Introduction to LoRA for Large Models</li> <li>Expanding LoRA: Adapter Merging and Effective Evaluations</li> </ul>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#understanding-practical-limitations","title":"Understanding Practical Limitations","text":"<ul> <li>Overview</li> <li>Training Challenges with Large Models</li> <li>Memory Constraints &amp; Optimization</li> <li>Advanced Configuration Example</li> <li>Smarter Training with LoRA</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#overview","title":"Overview","text":"<p>This section builds upon the previous lessons by transitioning from small-scale LLMs to working with larger models like Meta\u2019s LLaMA 3.1 8B. The focus is on highlighting practical limitations and advanced optimization techniques to train large models with limited hardware resources.</p> <p>\ud83d\udc49 This tutorial used the <code>unsloth/Meta-Llama-3.1-8B-Instruct</code> base model and a custom storytelling dataset.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#training-challenges-with-large-models","title":"Training Challenges with Large Models","text":""},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#goals","title":"Goals:","text":"<ul> <li>Scale from small to 8B+ parameter models</li> <li>Use real-world model: <code>LLaMA 3.1 - Sloth Variant</code></li> </ul>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#observations","title":"Observations:","text":"<ul> <li>Running on consumer-grade GPU (e.g., 24GB) fails due to:</li> <li>Tokenizer misconfiguration</li> <li> <p>Out-of-memory errors even at minimal batch sizes</p> </li> <li> <p>Reducing batch size &amp; sequence length:</p> </li> <li>\u2705 Enables training</li> <li>\u274c Hurts final performance</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#memory-constraints-optimization","title":"Memory Constraints &amp; Optimization","text":""},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#issues","title":"Issues:","text":"<ul> <li>Model weights, optimizer, and dataset compete for memory</li> <li>Even smallest 8B variant cannot fit fully</li> </ul>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#solutions-explored","title":"Solutions Explored:","text":"<ul> <li>Minimal batch size = 1</li> <li>Sequence length halved</li> <li>Still fails on common GPUs</li> </ul>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#suggested-direction","title":"Suggested Direction:","text":"<ul> <li>Use smarter techniques:</li> <li>\ud83d\udfe2 LoRA (Low-Rank Adaptation)</li> <li>\ud83d\udfe2 Gradient Checkpointing</li> <li>\ud83d\udfe2 Mixed Precision (bf16/8bit)</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#advanced-configuration-example","title":"Advanced Configuration Example","text":"<pre><code># Source: advanced_train.yml\nbase_model: unsloth/Meta-Llama-3.1-8B-Instruct\ndatasets:\n  - path: jaydenccc/AI_Storyteller_Dataset\n    type:\n      system_prompt: \"You are an amazing storyteller. From the following synopsis, create an engaging story.\"\n      field_instruction: synopsis\n      field_output: short_story\noutput_dir: ./models/Llama3_Storyteller2\nsequence_length: 1024\nmicro_batch_size: 4\noptimizer: adamw_bnb_8bit\nlearning_rate: 0.0002\nadapter: lora\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\ngradient_checkpointing: true\n</code></pre> <p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#smarter-training-with-lora","title":"Smarter Training with LoRA","text":"<p>LoRA is introduced as a lightweight fine-tuning mechanism.</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#benefits","title":"Benefits:","text":"<ul> <li>Reduces GPU memory footprint</li> <li>Only updates a few trainable parameters</li> <li>Compatible with large models like LLaMA</li> </ul>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#combined-with","title":"Combined With:","text":"<ul> <li>bf16 or tf32 mixed precision</li> <li>Gradient Checkpointing</li> <li>8bit optimizer (bnb)</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>Meta LLaMA 3.1 on Hugging Face</li> <li>LoRA: Low-Rank Adaptation of Large Language Models (ArXiv)</li> <li>Hugging Face Transformers Docs</li> <li>Jay Alammar's Blog: The Illustrated Transformer</li> <li>Google Colab Guide</li> <li>OpenAI Cookbook</li> <li>NVIDIA: Memory-Efficient Training</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#boosting-efficiency-peft-and-lora-in-depth","title":"Boosting Efficiency: PeFT and LoRA in Depth","text":""},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#quick-navigation_1","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview: The Problem with Full Fine-Tuning</li> <li>Memory Usage Breakdown</li> <li>Introduction to Parameter-Efficient Fine-Tuning</li> <li>LoRA: Low-Rank Adaptation Explained</li> <li>Hyperparameters in LoRA</li> <li>Benefits and Applications</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#overview-the-problem-with-full-fine-tuning","title":"Overview: The Problem with Full Fine-Tuning","text":"<p>Fine-tuning large language models (LLMs) often exceeds hardware capabilities due to massive memory demands. The key goals of improving training efficiency are:</p> <ul> <li>\ud83d\udfe2 Lower memory requirements.</li> <li>\ud83d\udfe2 Maintain model performance and accuracy.</li> <li>\ud83d\udfe2 Democratize access to LLM customization on limited hardware.</li> </ul> <p>Full fine-tuning is resource-intensive due to the need to update all model parameters, gradients, and optimizer states for every step.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#memory-usage-breakdown","title":"Memory Usage Breakdown","text":"<p>Training large models consumes memory across four key areas:</p> Component Description Memory Usage Model Parameters Learned weights of the model 2 \u00d7 N GB (for N billion params @ FP16) Gradients Gradients for backpropagation 2 \u00d7 N GB Optimizer States Additional states like moment estimates in Adam ~4\u20138 \u00d7 N GB Training Data Input sequences + embeddings per batch Variable (based on batch/seq length) <p>\u27a1\ufe0f Total: 8\u201312\u00d7 the model parameter memory.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#introduction-to-parameter-efficient-fine-tuning","title":"Introduction to Parameter-Efficient Fine-Tuning","text":"<p>Rather than updating all parameters, Parameter-Efficient Fine-Tuning (PEFT) updates only a targeted subset of the model. This reduces resource requirements while maintaining task-specific performance.</p> <p>One of the most prominent PEFT techniques is:</p> <ul> <li>\ud83d\udd35 LoRA (Low-Rank Adaptation) \u2014 inserts trainable, low-rank matrices into layers of a frozen pre-trained model.</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#lora-low-rank-adaptation-explained","title":"LoRA: Low-Rank Adaptation Explained","text":"<p>LoRA modifies only a small number of parameters by introducing additional matrices into each layer of the frozen model.</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#key-concepts","title":"\ud83c\udfaf Key Concepts","text":"<ul> <li>Fine-tunes LLMs by updating low-rank matrices (<code>A</code> and <code>B</code>) instead of full weight matrices <code>W</code>.</li> <li>Adds <code>\u0394W = A \u00d7 B</code> to <code>W</code> during forward pass.</li> <li>Enables efficient updates and reduces memory overhead.</li> </ul>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#lora-parameter-and-its-usage","title":"\ud83e\udde0 LoRA - Parameter and its usage","text":"Parameter Description Influence on Memory Influence on Runtime r (rank) The rank of the LoRA matrices. Lower values reduce memory and computation cost. \u2705 Lower rank = less memory usage \u2705 Lower rank = faster computation alpha Scaling factor applied to the LoRA output. Usually alpha / r is the effective scale. \ud83d\udd01 No direct memory impact, but may influence scale of activations \u2796 May affect gradient scale, but not runtime dropout Dropout probability applied to the LoRA layers to regularize during training. \u2796 Slight additional memory usage due to dropout mask \u2796 Slight slowdown during training bias Whether to include bias terms. Can be 'none', 'all', or 'lora_only'. \u2796 Adds small amount of memory if bias is included \u2796 Minor impact if biases are added target_modules List of module names where LoRA adapters should be inserted (e.g., 'q_proj', 'v_proj'). \u2705 Selective targeting reduces memory footprint \u2705 Reduces compute by targeting specific layers merge_weights If True, merges LoRA weights with the original model weights during inference. \u2705 Merging removes need for separate LoRA weights at inference \u2705 Faster inference by removing adapter layers"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#hugging-face-arxiv","title":"\ud83d\udd17 Hugging Face + ArXiv","text":"<ul> <li>LoRA Model on Hugging Face</li> <li>Original Paper - LoRA: Low-Rank Adaptation</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#hyperparameters-in-lora","title":"Hyperparameters in LoRA","text":"Hyperparameter Role Typical Value(s) <code>rank</code> (r) Size of the low-rank matrices 8, 16, 32 <code>alpha</code> Scaling factor for \u0394W = \u03b1 \u00d7 A \u00d7 B 16 (default) <code>dropout</code> Regularization to prevent overfitting 0.0 \u2013 0.1 <code>target_modules</code> Model layers to apply LoRA (e.g., <code>query</code>, <code>value</code>, etc.) Varies <p>\ud83d\udccc Higher <code>rank</code> means better task adaptation, but at a cost to memory savings. \ud83d\udccc Lower <code>rank</code> helps prevent catastrophic forgetting during fine-tuning.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#benefits-and-applications","title":"Benefits and Applications","text":""},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#advantages-of-lora","title":"\ud83d\udfe2 Advantages of LoRA","text":"<ul> <li>Reduces GPU memory footprint by &gt;80%</li> <li>Avoids catastrophic forgetting</li> <li>Accelerates training time</li> <li>Easily pluggable into existing transformer architectures</li> <li>Works well on small datasets</li> </ul>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#example-use-cases","title":"\ud83d\udee0\ufe0f Example Use Cases","text":"<ul> <li>Personalizing a chatbot without retraining a full LLM</li> <li>Domain-specific adaptation (e.g., legal, healthcare)</li> <li>Multilingual extensions using LoRA adapters per language</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#references-further-reading_1","title":"References &amp; Further Reading","text":"<ul> <li>LoRA: Low-Rank Adaptation of Large Language Models (ArXiv)</li> <li>Hugging Face PEFT Documentation</li> <li>Jay Alammar \u2013 LoRA Illustrated</li> <li>NVIDIA \u2013 Efficient Fine-Tuning Techniques</li> <li>Google \u2013 Parameter-Efficient Transfer Learning</li> <li>Meta AI Research \u2013 PEFT Methods</li> <li>OpenAI \u2013 Scaling Laws and Efficient Training</li> <li>Microsoft \u2013 LoRA Integration in DeepSpeed</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#managing-data-memory-batch-size-sequence-length","title":"Managing Data Memory: Batch Size &amp; Sequence Length","text":""},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#quick-navigation_2","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Introduction to Memory Efficiency</li> <li>Training Analogy: Book, Pages, and Feedback</li> <li>Core Training Parameters</li> <li>Batch Size</li> <li>Sequence Length</li> <li>Trade-offs in Training Efficiency</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#introduction-to-memory-efficiency","title":"Introduction to Memory Efficiency","text":"<p>In the previous lesson, we explored LoRA (Low-Rank Adaptation) as a method to reduce memory usage during fine-tuning of Large Language Models (LLMs) by modifying only a subset of model parameters.</p> <p>In this session, we shift our focus to the memory consumption from data\u2014specifically, how the structure of the input data (e.g., batch size and sequence length) affects training efficiency, cost, and feasibility.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#training-analogy-book-pages-and-feedback","title":"Training Analogy: Book, Pages, and Feedback","text":"<p>To illustrate how models learn from data, the lesson uses an analogy:</p> <ul> <li>Book \u2192 The complete dataset</li> <li>Page \u2192 A single training example</li> <li>Reading a few pages then testing \u2192 A training step</li> <li>Reading the full book once \u2192 One epoch</li> <li>Number of pages per step \u2192 Batch size</li> <li>Words per page \u2192 Sequence length</li> </ul> <p> </p> <p>This incremental reading process enables: - Frequent model updates - Improved generalization - Reduced memory usage per training step</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#core-training-parameters","title":"Core Training Parameters","text":""},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#batch-size","title":"Batch Size","text":"<ul> <li>Determines how many training examples are processed before a model update.</li> <li>Larger batches:</li> <li>Require more memory</li> <li>Yield more accurate gradients (faster convergence)</li> <li>Are often limited by GPU/TPU capacity</li> <li>Smaller batches:</li> <li>Reduce memory consumption</li> <li>Introduce noisier gradients, which may help generalization</li> </ul>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#sequence-length","title":"Sequence Length","text":"<ul> <li>Number of tokens per training example.</li> <li>Longer sequences:</li> <li>Require more memory and compute</li> <li>Contain richer contextual information</li> <li>Shorter sequences:</li> <li>Allow for bigger batch sizes</li> <li>Reduce computation time</li> <li>May lack enough context for learning</li> </ul> <p>\ud83d\udcc9 Trade-off: You often reduce batch size to accommodate longer sequences within memory limits.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#trade-offs-in-training-efficiency","title":"Trade-offs in Training Efficiency","text":"<p>When training LLMs, we often cannot afford large batch sizes due to memory limits.</p> <ul> <li>Rarely does batch size get \u201ctoo large\u201d</li> <li>Commonly, batch size becomes \u201ctoo small\u201d due to memory constraints</li> <li>The key challenge: <p>How to gain the benefits of large batch training without overwhelming memory resources?</p> </li> </ul> <p>Solutions to these constraints will be covered in the next lesson.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#references-further-reading_2","title":"References &amp; Further Reading","text":"<ul> <li>LoRA: Low-Rank Adaptation of Large Language Models (arXiv)</li> <li>Jay Alammar \u2013 Illustrated Transformer</li> <li>Hugging Face \u2013 Transformers Documentation</li> <li>OpenAI Blog</li> <li>Google Research</li> <li>Microsoft Research \u2013 DeepSpeed</li> <li>NVIDIA Developer Blog</li> <li>Facebook AI Research (FAIR)</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#advanced-solutions-gradient-accumulation-checkpointing","title":"Advanced Solutions: Gradient Accumulation &amp; Checkpointing","text":""},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#quick-navigation_3","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Micro-Batching and Gradient Accumulation</li> <li>Gradient Checkpointing</li> <li>Efficiency Method Comparison Table</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#micro-batching-and-gradient-accumulation","title":"Micro-Batching and Gradient Accumulation","text":"<p>Training large language models (LLMs) often requires careful memory management. Two primary techniques help in this regard: Micro-batching and Gradient Accumulation.</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#micro-batching","title":"Micro-Batching","text":"<ul> <li>A large batch is split into micro-batches to fit in limited memory.</li> <li>Each micro-batch is processed sequentially.</li> <li>Gradients are accumulated to simulate a larger batch.</li> <li>Enables efficient training on hardware with limited GPU memory.</li> </ul> <p>Effective Batch Size Formula: <pre><code>Effective Batch Size = Micro Batch Size \u00d7 Gradient Accumulation Steps \u00d7 Number of GPUs\n</code></pre></p> <p>\ud83d\uddbc\ufe0f </p> <p>Alt text: Diagram showing micro-batch pages filling a book one by one, representing how gradient accumulation simulates a large batch.</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#gradient-accumulation","title":"Gradient Accumulation","text":"<ul> <li>Performs multiple forward/backward passes per optimization step.</li> <li>Gradients are aggregated before updating the model.</li> <li>Reduces memory usage but increases training time.</li> </ul> <p>\ud83d\udd27 Tip: Choose the largest micro-batch size that fits your GPU and increase accumulation steps only when necessary.</p> <p>\ud83d\udcc9 Example:</p> Micro Batch Size Accumulation Steps GPUs Effective Batch Size 4 2 1 8 1 16 4 64 <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#gradient-checkpointing","title":"Gradient Checkpointing","text":""},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#the-problem","title":"The Problem","text":"<ul> <li>Training LLMs requires storing many activations during the forward pass.</li> <li>These activations are needed for computing gradients in the backward pass.</li> <li>Storing all of them consumes significant GPU memory.</li> </ul>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#the-solution-checkpointing","title":"The Solution: Checkpointing","text":"<ul> <li>Gradient Checkpointing stores only selected activations during forward pass.</li> <li>During backward pass, missing activations are recomputed.</li> <li>Balances memory savings with added computation.</li> </ul> <p>\ud83d\uddbc\ufe0f </p> <p>Alt text: Diagram showing checkpoint blocks within the model to reduce activation memory.</p> <p>\ud83d\udcca Benefits: - Substantial memory savings - Trade-off: Slight increase in training time</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#efficiency-method-comparison-table","title":"Efficiency Method Comparison Table","text":"Method Memory Usage Training Speed Accuracy Impact Notes \ud83d\udfe2 LoRA High savings Moderate Neutral to Slight loss Trains subset of weights \ud83d\udd35 Small Batch Size Medium Faster (large batch) Neutral Limited if model needs larger batches \ud83d\udd35 Gradient Accumulation High savings Slower Slightly Better Allows larger effective batch \ud83d\udfe2 Gradient Checkpointing High savings Slower Neutral Strategic activation savings \ud83d\udd34 Mixed Precision Very High Faster (with FP16) Slight loss Risk of instability at low precision <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#references-further-reading_3","title":"References &amp; Further Reading","text":"<ul> <li>Attention is All You Need \u2013 Vaswani et al. (ArXiv)</li> <li>Hugging Face Transformers Documentation</li> <li>Jay Alammar\u2019s Illustrated Transformer</li> <li>NVIDIA \u2013 Gradient Accumulation &amp; Mixed Precision</li> <li>OpenAI Research Papers</li> <li>Microsoft DeepSpeed Memory Optimization</li> <li>Google AI Blog: Efficient Training Techniques</li> <li>Facebook AI Gradient Checkpointing</li> </ul> <p>Back to Top</p> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#fitting-giants-practical-introduction-to-lora-for-large-models","title":"Fitting Giants: Practical Introduction to LoRA for Large Models","text":""},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#quick-navigation_4","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Micro-Batching &amp; Gradient Accumulation</li> <li>Gradient Checkpointing</li> <li>LoRA (Low-Rank Adaptation)</li> <li>Batch Size Trade-offs</li> <li>Mixed Precision Training</li> <li>Technique Comparison Summary</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#micro-batching-gradient-accumulation","title":"Micro-Batching &amp; Gradient Accumulation","text":"<p>Micro-batching enables large batch benefits on memory-constrained hardware. By splitting a large batch into smaller \"micro-batches\", the system accumulates gradients across them before a single optimizer update.</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#gradient-accumulation_1","title":"\ud83d\udd01 Gradient Accumulation","text":"<ul> <li>Forward and backward passes are done over smaller micro-batches.</li> <li>Gradients are accumulated in memory across steps.</li> <li>A single optimizer update is performed after N steps.</li> </ul>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#formula-for-effective-batch-size","title":"Formula for Effective Batch Size","text":"<p>[ \\text{Effective Batch Size} = \\text{Micro Batch Size} \\times \\text{Accumulation Steps} \\times \\text{# of GPUs} ]</p> <ul> <li>Example 1: Micro batch = 4, steps = 2, 1 GPU \u2192 Effective Batch Size = 8</li> <li>Example 2: Micro batch = 1, steps = 16, 4 GPUs \u2192 Effective Batch Size = 64</li> </ul> <p>\ud83d\udfe2 Pros: - Enables large effective batch sizes on small GPUs - Good generalization performance</p> <p>\ud83d\udd34 Cons: - Slower training due to repeated forward/backward passes</p> <p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#gradient-checkpointing_1","title":"Gradient Checkpointing","text":"<p>Gradient checkpointing saves memory by selectively storing activations during the forward pass.</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#concept","title":"\u26d3\ufe0f Concept","text":"<ul> <li>Store only key activations (\"checkpoints\") during the forward pass</li> <li>During backpropagation, re-compute non-stored activations as needed</li> </ul> <p>\ud83d\udfe2 Pros: - Significant memory savings - Feasible for training large models on limited hardware</p> <p>\ud83d\udd34 Cons: - Slower training due to partial recomputation</p> <p>\ud83d\udcd8 Hugging Face Docs \ud83d\udcc4 Gradient Checkpointing ArXiv Paper</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#lora-low-rank-adaptation","title":"LoRA (Low-Rank Adaptation)","text":"<p>LoRA reduces memory usage by freezing the base model and training only a small number of injected low-rank matrices.</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#how-it-works","title":"\u2699\ufe0f How It Works","text":"<ul> <li>Inject trainable low-rank adapters into transformer layers</li> <li>Only adapters are updated during fine-tuning</li> </ul> <p>\ud83d\udfe2 Memory: - Saves memory by reducing optimizer state size</p> <p>\ud83d\udd35 Speed: - Comparable to full fine-tuning</p> <p>\ud83d\udd34 Accuracy: - Can cap performance for some tasks - May help in preventing catastrophic forgetting</p> <p>\ud83d\udcd8 LoRA on Hugging Face \ud83d\udcc4 LoRA Paper on ArXiv</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#batch-size-trade-offs","title":"Batch Size Trade-offs","text":"<p>Batch size directly impacts training memory, speed, and generalization.</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#trade-off-spectrum","title":"\ud83e\uddee Trade-off Spectrum","text":"<ul> <li>Small batch size \u2192 Lower memory, slower speed, better generalization</li> <li>Large batch size \u2192 Higher memory, faster training, risk of overfitting</li> </ul> <p>\ud83d\udfe2 Rule of Thumb:</p> <p>Use the largest batch size that fits in GPU memory.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#mixed-precision-training","title":"Mixed Precision Training","text":"<p>Training with lower precision (e.g., FP16, INT8, 4-bit) can greatly reduce memory usage and increase speed.</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#floating-point-formats","title":"\ud83e\uddca Floating Point Formats","text":"Format Memory Usage Speed Accuracy Impact FP32 High Standard None FP16 Medium Fast Minor INT8 Low Slower Moderate 4-bit Very Low Slower Noticeable <p>\ud83d\udfe2 Pros: - Huge memory savings - Speed boost on supporting hardware (e.g., A100, H100)</p> <p>\ud83d\udd34 Cons: - Minor accuracy loss at extreme bit reduction</p> <p>\ud83d\udcd8 Mixed Precision on NVIDIA \ud83d\udcd8 Hugging Face Docs</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#technique-comparison-summary","title":"Technique Comparison Summary","text":"Technique Memory Usage Speed Impact Accuracy Impact \ud83d\udfe2 LoRA Excellent Neutral Neutral/Task Dependent \ud83d\udd35 Batch Size Tuning Moderate High Task Dependent \ud83d\udfe1 Gradient Accumulation High Slightly Slower Positive \ud83d\udd34 Gradient Checkpointing High Slower Neutral \ud83d\udfe2 Mixed Precision Excellent Faster (if supported) Slightly Negative"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#advanced-configuration-example_1","title":"Advanced Configuration Example","text":"<pre><code>#Source: advanced_train2.yml\n# model params\nbase_model: unsloth/Meta-Llama-3.1-8B-Instruct\n\n# dataset params\ndatasets:\n  - path: jaydenccc/AI_Storyteller_Dataset\n    type: \n      system_prompt: \"You are an amazing storyteller. From the following synopsis, create an engaging story.\"\n      field_system: system\n      field_instruction: synopsis\n      field_output: short_story\n      format: \"&lt;|user|&gt;\\n {instruction} &lt;/s&gt;\\n&lt;|assistant|&gt;\"\n      no_input_format: \"&lt;|user|&gt; {instruction} &lt;/s&gt;\\n&lt;|assistant|&gt;\"\n\noutput_dir: ./models/Llama3_Storyteller2\n\n\n# model params\nsequence_length: 1024\nbf16: auto\ntf32: false\n\n# training params\nmicro_batch_size: 4\nnum_epochs: 4\noptimizer: adamw_bnb_8bit\nlearning_rate: 0.0002\n\nlogging_steps: 1\n\n\n# LoRA\nadapter: lora\n\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\n\nlora_target_linear: true\n\n# Gradient Accumulation\ngradient_accumulation_steps: 1\n\n# Gradient Checkpointing\ngradient_checkpointing: true\n</code></pre>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#try-it-yourself","title":"Try It Yourself","text":"<p>Explore and run the notebook interactively using Google Colab:</p> <p>Open the Notebook in Colab</p> <p></p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#references-further-reading_4","title":"References &amp; Further Reading","text":"<ul> <li>Attention Is All You Need (Vaswani et al.)</li> <li>LoRA: Low-Rank Adaptation of Large Language Models</li> <li>Gradient Checkpointing for Memory Optimization</li> <li>Jay Alammar\u2019s Illustrated Transformer</li> <li>NVIDIA Mixed Precision Guide</li> <li>Hugging Face Transformers Documentation</li> <li>Google Research: Efficient Training Techniques</li> <li>OpenAI Research Blog</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#expanding-lora-adapter-merging-and-effective-evaluations","title":"Expanding LoRA: Adapter Merging and Effective Evaluations","text":""},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#quick-navigation_5","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview: Adapter Merging in LoRA</li> <li>What Is an Adapter File?</li> <li>Using <code>xolotl.merge_lora</code></li> <li>Best Practices in LoRA Fine-Tuning</li> <li>Instructional Prompting</li> <li>Effective Batch Size</li> <li>Model Evaluation and Loss Comparison</li> <li>Final Merge and Output</li> <li>Colab Notebook</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#overview-adapter-merging-in-lora","title":"Overview: Adapter Merging in LoRA","text":"<p>In this session, we explore how to merge the lightweight adapter produced by LoRA-based training with the base model. We review best practices to optimize accuracy and stability, and correct common mistakes in the fine-tuning process.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#what-is-an-adapter-file","title":"What Is an Adapter File?","text":"<ul> <li>After LoRA training, only a small diff file is produced\u2014this is the adapter.</li> <li>It contains the modified 1% of weights from the original model.</li> <li>When merged with the base model, it reconstructs the fully fine-tuned model.</li> <li>This saves disk space and makes training more efficient.</li> </ul> <p>\ud83e\udde0 Key Concept: Adapter = Delta weights (not the full model)</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#using-xolotlmerge_lora","title":"Using <code>xolotl.merge_lora</code>","text":"<p>To merge the adapter with the base model:</p> <pre><code>from xolotl import merge_lora\n\nmerge_lora(\n  config_path=\"path/to/config.yaml\",\n  adapter_path=\"path/to/adapter\"\n)\n</code></pre> <ul> <li>The process creates a new model directory with the full merged weights.</li> <li>You can then prompt the model as usual for inference.</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#best-practices-in-lora-fine-tuning","title":"Best Practices in LoRA Fine-Tuning","text":"<p>Despite a successful training, multiple common issues were present.</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#instructional-prompting","title":"Instructional Prompting","text":"<p>\u274c Mistake: - No explicit task prompt was given to the instruction-following model.</p> <p>\u2705 Fix: - Add a system prompt like:</p> <pre><code>You are an amazing storyteller. From the following synopsis, write an engaging story.\n</code></pre> <ul> <li>Helps align model behavior with instruction-tuned expectations.</li> </ul>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#effective-batch-size","title":"Effective Batch Size","text":"<p>\u274c Mistake: - Micro batch size = 4 - Gradient accumulation steps = 4 \u2192 Effective batch size = 16</p> <ul> <li>Too small for a meaningful update over a small dataset.</li> </ul> <p>\u2705 Fix: - Increase micro batch size to maximum that fits in memory - Reduce accumulation steps to speed up training</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#model-evaluation-and-loss-comparison","title":"Model Evaluation and Loss Comparison","text":"<ul> <li>After training with improved prompts and optimized batch size:</li> <li>Training ran for more steps</li> <li>Loss values were significantly lower</li> <li>Indicates improved convergence and generalization</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#final-merge-and-output","title":"Final Merge and Output","text":"<ul> <li>The merged model was tested on a story generation prompt.</li> <li>Result: A more coherent and structured output, with better alignment to storytelling instructions.</li> <li>This validates the importance of correct prompts and batch size tuning.</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#colab-notebook","title":"Colab Notebook","text":"<p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/10-llm-advanced/#references-further-reading_5","title":"References &amp; Further Reading","text":"<ul> <li>LoRA: Low-Rank Adaptation of LLMs (arXiv)</li> <li>Hugging Face \u2013 Parameter Efficient Fine-Tuning Guide</li> <li>OpenAI Cookbook</li> <li>Jay Alammar \u2013 Visualizing Transformers</li> <li>Google AI Blog</li> <li>XAI \u2013 Explainable AI Projects</li> <li>Microsoft Research \u2013 DeepSpeed</li> </ul> <p>\ud83d\uddd3 Generated on: July 27, 2025</p> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/","title":"11. Specialized LLM Training","text":""},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#quick-navigation","title":"Quick Navigation","text":"<ul> <li>Level-Up Giants: 8-bit Training for Massive Models</li> <li>Task-Focused Training: Aim for Better Learning - Part 1</li> <li>Task-Focused Training: Aim for Better Learning - Part 2</li> <li>Edge of Hardware Limits: Scaling Inputs with Flash Attention 2</li> <li>Edge of Hardware Limits: Reaching 4bit Training with QLoRA</li> </ul>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#level-up-giants-8-bit-training-for-massive-models","title":"Level-Up Giants: 8-bit Training for Massive Models","text":""},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#quick-navigation_1","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Training Phi-3: Scaling to Larger LLMs</li> <li>Memory Challenges and 8-Bit Quantization</li> <li>Performance, Trade-offs, and Results</li> <li>Using the SQuAD Dataset for LLM QA</li> <li>Phi-3 Configuration: <code>specialised_phi.yml</code></li> <li>Gemma 2-27B Configuration: <code>specialised_gemma.yml</code></li> <li>Colab Notebook</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#training-phi-3-scaling-to-larger-llms","title":"Training Phi-3: Scaling to Larger LLMs","text":"<p>This session demonstrates the training of Microsoft\u2019s Phi-3 Medium, a 14B parameter model with a 128k context window, on a single 24GB GPU using Axolotl and LoRA. Techniques such as gradient accumulation, checkpointing, and 8-bit quantization enable training without sacrificing batch size or sequence length.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#memory-challenges-and-8-bit-quantization","title":"Memory Challenges and 8-Bit Quantization","text":"<p>Despite tuning batch size and sequence length, the Phi-3 model exceeded available memory due to its ~30GB weight size. To overcome this:</p> <ul> <li>Quantization to 8-bit (<code>load_in_8bit: true</code>) was applied.</li> <li>This halved memory usage, allowing training to proceed.</li> <li>Axolotl supports this via a simple YAML flag.</li> </ul> <p>\ud83d\udca1 Note: Precision is traded for capacity; however, the gain in model size (14B vs. 7B) outweighs the loss from quantization.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#performance-trade-offs-and-results","title":"Performance, Trade-offs, and Results","text":"<ul> <li>Training succeeded with no OOM errors and room to spare.</li> <li>Sequence length and batch size compromises were reversed.</li> <li>Training was slower than LLaMA 8B due to the model's size.</li> <li>Despite quantization, Phi-3 handled the dataset well and generated fluent outputs.</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#using-the-squad-dataset-for-llm-qa","title":"Using the SQuAD Dataset for LLM QA","text":"<p>The model was fine-tuned on the SQuAD (Stanford Question Answering Dataset), adapted for generative LLMs.</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#prompt-format","title":"Prompt Format:","text":"<pre><code>&lt;|user|&gt;\n {input} {instruction} &lt;/s&gt;\n&lt;|assistant|&gt;\n</code></pre> <ul> <li>Context and question are combined into a system-level prompt.</li> <li>Model learns to generate precise answers based on instruction tuning.</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#phi-3-configuration-specialised_phiyml","title":"Phi-3 Configuration: <code>specialised_phi.yml</code>","text":"<pre><code>base_model: microsoft/Phi-3-medium-128k-instruct\ndatasets:\n  - path: TheFuzzyScientist/squad-for-llms\n    type:\n      system_prompt: \"Read the following context and concisely answer my question.\"\n      field_system: system\n      field_instruction: question\n      field_input: context\n      field_output: output\n      format: \"&lt;|user|&gt;\n {input} {instruction} &lt;/s&gt;\n&lt;|assistant|&gt;\"\n      no_input_format: \"&lt;|user|&gt; {instruction} &lt;/s&gt;\n&lt;|assistant|&gt;\"\noutput_dir: ./models/Phi3_Storyteller\nsequence_length: 8172\nbf16: auto\ntf32: false\nmicro_batch_size: 4\nnum_epochs: 1\noptimizer: adamw_bnb_8bit\nlearning_rate: 0.0002\nlogging_steps: 1\nadapter: lora\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear: true\ngradient_accumulation_steps: 1\ngradient_checkpointing: true\nload_in_8bit: true\n</code></pre> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#gemma-2-27b-configuration-specialised_gemmayml","title":"Gemma 2-27B Configuration: <code>specialised_gemma.yml</code>","text":"<pre><code>base_model: unsloth/gemma-2-27b-it\ndatasets:\n  - path: Yukang/LongAlpaca-12k\n    type: alpaca\noutput_dir: ./models/gemma-LongAlpaca\nsequence_length: 1024\nbf16: auto\ntf32: false\nmicro_batch_size: 1\nnum_epochs: 1\noptimizer: adamw_bnb_8bit\nlearning_rate: 0.0002\nlogging_steps: 1\nadapter: qlora\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear: true\ngradient_accumulation_steps: 1\ngradient_checkpointing: true\nload_in_8bit: false\nload_in_4bit: true\nflash_attention: true\n</code></pre> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#colab-notebook","title":"Colab Notebook","text":"<p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>Microsoft Phi-3 Medium Model Card</li> <li>Phi-3 Technical Report (Microsoft)</li> <li>Quantization in Transformers (Hugging Face)</li> <li>Gradient Checkpointing \u2013 PyTorch Docs</li> <li>LoRA: Low-Rank Adaptation (arXiv)</li> <li>Axolotl Fine-Tuning Framework</li> <li>Unsloth \u2013 Efficient Training</li> <li>Flash Attention 2 \u2013 Paper</li> </ul> <p>\ud83d\uddd3 Generated on: July 27, 2025</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#task-focused-training-aim-for-better-learning-part-1","title":"Task-Focused Training: Aim for Better Learning - Part 1","text":""},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#quick-navigation_2","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Dataset Overview</li> <li>Data Preprocessing</li> <li>Training Configuration</li> <li>Loss Calculation Challenges</li> <li>Colab Notebook</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#dataset-overview","title":"Dataset Overview","text":"<p>The lesson transitions from a simple dataset to the more complex SQuAD dataset, aiming to train a decoder-only large language model (LLM). SQuAD (Stanford Question Answering Dataset) consists of:</p> <ul> <li>Context: A passage of text</li> <li>Question: Related to the context</li> <li>Answer: A short, extractable string from the context</li> </ul> <p>Dataset Statistics: - Training Set: ~87,000 samples - Validation Set: ~10,000 samples</p> <p>\ud83d\udd17 SQuAD on Hugging Face</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#data-preprocessing","title":"Data Preprocessing","text":""},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#tools-used","title":"\ud83d\udd27 Tools Used","text":"<ul> <li>Python</li> <li>Hugging Face Datasets</li> <li>Pandas</li> <li>Parquet file storage</li> </ul>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#steps","title":"\ud83e\udde9 Steps:","text":"<ol> <li>Load SQuAD using <code>datasets</code> library.</li> <li>Convert to pandas DataFrame.</li> <li>Simplify Answers: Use only the first answer string.</li> <li>Drop unnecessary columns.</li> <li>Save the processed dataset as a <code>.parquet</code> file.</li> <li>Update the YAML configuration to use the local dataset.</li> <li>Adjust Prompts:</li> <li>Instruction: The question</li> <li>Input: The context</li> <li>Output: The answer</li> <li>System prompt: \u201cRead the following context and concisely answer my question.\u201d</li> </ol> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#training-configuration","title":"Training Configuration","text":""},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#model-microsoftphi-3-medium-128k-instruct","title":"\ud83e\udde0 Model: <code>microsoft/Phi-3-medium-128k-instruct</code>","text":"<ul> <li>Direction: Decoder-only</li> <li>Max Token Sequence: 8,172 (extended due to load_in_8bit optimization)</li> <li>Optimizer: <code>adamw_bnb_8bit</code></li> <li>Precision: <code>bf16</code>, <code>8bit</code>, <code>tf32: false</code></li> </ul>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#lora-setup","title":"\ud83d\udee0\ufe0f LoRA Setup","text":"<ul> <li><code>adapter: lora</code></li> <li><code>r: 32</code>, <code>alpha: 16</code>, <code>dropout: 0.05</code></li> <li>Target Linear Layers</li> </ul>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#yaml-config-snippet","title":"\ud83c\udf00 YAML Config Snippet","text":"<pre><code>base_model: microsoft/Phi-3-medium-128k-instruct\ndatasets:\n  - path: TheFuzzyScientist/squad-for-llms\n    type: \n      system_prompt: \"Read the following context and concisely answer my question.\"\n      field_system: system\n      field_instruction: question\n      field_input: context\n      field_output: output\n      format: \"&lt;|user|&gt;\\n {input} {instruction} &lt;/s&gt;\\n&lt;|assistant|&gt;\"\noutput_dir: ./models/Phi3_Storyteller\nsequence_length: 8172\nmicro_batch_size: 4\nlearning_rate: 0.0002\nadapter: lora\ngradient_checkpointing: true\nload_in_8bit: true\n\n\ud83d\udcc4 [Model on Hugging Face](https://huggingface.co/microsoft/Phi-3-medium-128k-instruct)  \n\ud83d\udcdc [ArXiv Paper (if available)](https://arxiv.org/abs/2404.14219)  \n[Back to Top](#quick-navigation)\n\n---\n\n## Task-Focused Training: Aim for Better Learning - Part 2\n\n\n## \ud83d\udccc Quick Navigation\n\n- [Output-Focused Training](#output-focused-training)\n- [Training Behavior Comparison](#training-behavior-comparison)\n- [YAML Configuration Updates](#yaml-configuration-updates)\n- [Colab Notebook](#colab-notebook)\n- [References &amp; Further Reading](#references--further-reading)\n\n---\n\n## Output-Focused Training\n\nIn this lesson, we explore how to modify our training setup so that the model is rewarded and punished based **only** on its ability to generate the output \u2014 not the input.\n\nThis is especially important for tasks like **question answering**, where:\n- The **input (context + question)** is significantly longer than the **output (answer)**.\n- The model may waste learning capacity modeling irrelevant parts of the input.\n\n### \u2705 Key Change:\n- In the Axolotl framework, set:\n\n  ```yaml\n  train_on_inputs: false\n  ```\n\nThis disables gradient calculation over the input portion of the sequence.\n\n### \ud83d\udccc When to Use\n- Ideal for tasks with short, deterministic outputs (e.g., QA, summarization).\n- **Not recommended** for conversational datasets or tasks with intertwined dialog.\n\n[Back to Top](#quick-navigation)\n\n---\n\n## Training Behavior Comparison\n\nTwo identical models are trained:\n- **Left**: `train_on_inputs = false` (loss computed only on output)\n- **Right**: `train_on_inputs = true` (loss computed on full input + output)\n\n### Observations:\n- Early performance is similar due to SQuAD's simplicity.\n- Models with `train_on_inputs = false` tend to:\n  - Converge **faster**\n  - Achieve **higher final accuracy**\n  - Require **fewer steps**\n  - Be **more stable** (less prone to divergence)\n\nThis adjustment improves focus and prevents overfitting to irrelevant tokens.\n\n[Back to Top](#quick-navigation)\n\n---\n\n## YAML Configuration Updates\n\nBelow is the modified configuration snippet used for this lesson:\n\n```yaml\nbase_model: microsoft/Phi-3-medium-128k-instruct\ndatasets:\n  - path: TheFuzzyScientist/squad-for-llms\n    type: \n      system_prompt: \"Read the following context and concisely answer my question.\"\n      field_system: system\n      field_instruction: question\n      field_input: context\n      field_output: output\n      format: \"&lt;|user|&gt;\n {input} {instruction} &lt;/s&gt;\n&lt;|assistant|&gt;\"\n      no_input_format: \"&lt;|user|&gt; {instruction} &lt;/s&gt;\n&lt;|assistant|&gt;\"\ntrain_on_inputs: false\noutput_dir: ./models/Phi3_Storyteller\nsequence_length: 8172\nbf16: auto\ntf32: false\nmicro_batch_size: 4\nnum_epochs: 1\noptimizer: adamw_bnb_8bit\nlearning_rate: 0.0002\nadapter: lora\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear: true\ngradient_accumulation_steps: 1\ngradient_checkpointing: true\nload_in_8bit: true\n</code></pre> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#colab-notebook_1","title":"Colab Notebook","text":"<p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#references-further-reading_1","title":"References &amp; Further Reading","text":"<ul> <li>Phi-3 Model on Hugging Face</li> <li>ArXiv: Phi-3 Model Paper</li> <li>Training on Outputs Only - Hugging Face Discussion</li> <li>LoRA: Low-Rank Adaptation of Large Language Models</li> <li>Transformers Docs \u2013 Hugging Face</li> <li>Axolotl GitHub</li> <li>Google Colab Guide</li> </ul> <p>Back to Top</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#edge-of-hardware-limits-scaling-inputs-with-flash-attention-2","title":"Edge of Hardware Limits: Scaling Inputs with Flash Attention 2","text":""},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#quick-navigation_3","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Scaling Model Training</li> <li>Flash Attention: Theory</li> <li>Implementing Flash Attention</li> <li>Long Context Dataset Training</li> <li>Colab Notebook</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#scaling-model-training","title":"Scaling Model Training","text":"<p>In this section, the focus shifts to pushing the boundaries of what is possible in training large language models (LLMs) without upgrading hardware.</p> <p>Key goals: - Train on longer sequences - Reduce GPU memory footprint - Improve training throughput and stability</p> <p>Challenges: - Transformer attention is quadratic in complexity - Long sequences demand more compute and memory</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#flash-attention-theory","title":"Flash Attention: Theory","text":"<p>Flash Attention is a highly efficient attention mechanism designed to optimize: - Memory transfers - Computation efficiency</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#key-optimizations","title":"\ud83e\udde0 Key Optimizations:","text":"<ul> <li>Minimizes memory I/O by loading queries, keys, and values once</li> <li>Operates on GPU SRAM instead of relying on frequent memory access</li> <li>Enables longer sequence training with same hardware</li> </ul>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#outcomes","title":"\ud83d\udd0d Outcomes:","text":"<ul> <li>Up to 1GB memory saved</li> <li>Up to 12% training speed improvement</li> <li>Higher max sequence lengths and batch sizes achievable</li> </ul> <p>\ud83d\udcd8 Flash Attention Paper: ArXiv: 2205.14135</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#implementing-flash-attention","title":"Implementing Flash Attention","text":"<p>In the <code>Axolotl</code> framework, enabling Flash Attention is very simple:</p> <pre><code>flash_attention: true\n</code></pre> <p>Once enabled, the attention mechanism becomes more memory-efficient without any change to model architecture or tokenization.</p> <p>\ud83d\udee0\ufe0f Other Configuration Highlights: - Base Model: <code>unsloth/gemma-2-27b-it</code> - Adapter: <code>qLoRA</code> - Mixed precision: <code>bf16</code>, <code>load_in_4bit</code></p> <pre><code># model params\nbase_model: unsloth/gemma-2-27b-it\n\n# dataset params\ndatasets:\n  - path: Yukang/LongAlpaca-12k\n    type: alpaca\n\noutput_dir: ./models/gemma-LongAlpaca\n\nsequence_length: 1024\nflash_attention: true\nadapter: qlora\nload_in_4bit: true\n</code></pre> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#long-context-dataset-training","title":"Long Context Dataset Training","text":"<p>To test Flash Attention and longer contexts, the model was fine-tuned using:</p> <p>\ud83d\uddc2\ufe0f Dataset: <code>LongAlpaca-12k</code> \ud83e\uddfe Type: Instruction-following conversations with long inputs and outputs \ud83d\udd22 Max Sequence Length: 16,000 tokens (extended from 8k) \ud83d\udce6 Micro Batch Size: 1 \ud83e\uddea Padding: Applied to ensure all sequences are uniform \ud83d\udcc9 Observation: - 5% lower memory usage - 10\u201312% training speedup</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#colab-notebook_2","title":"Colab Notebook","text":"<p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#references-further-reading_2","title":"References &amp; Further Reading","text":"<ul> <li>Flash Attention (ArXiv)</li> <li>LongAlpaca Dataset \u2013 Hugging Face</li> <li>Gemma-2-27B Model \u2013 Hugging Face</li> <li>Axolotl Fine-Tuning GitHub</li> <li>LoRA: Low-Rank Adaptation</li> <li>QLoRA: Efficient Finetuning of LLMs</li> <li>Transformers Documentation \u2013 Hugging Face</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#edge-of-hardware-limits-reaching-4bit-training-with-qlora","title":"Edge of Hardware Limits: Reaching 4bit Training with QLoRA","text":""},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#quick-navigation_4","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Overview of 4-bit and qLoRA Training</li> <li>Memory Efficiency via Quantization</li> <li>Gemma-2 27B Model Training</li> <li>YAML Configuration</li> <li>Colab Notebook</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#overview-of-4-bit-and-qlora-training","title":"Overview of 4-bit and qLoRA Training","text":"<p>As we push the boundaries of training ever-larger models on limited hardware, the next evolution involves precision optimization:</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#topics-covered","title":"\ud83d\udd0d Topics Covered:","text":"<ul> <li>4-bit training via double quantization</li> <li>qLoRA (quantized Low-Rank Adaptation)</li> <li>Using <code>Gemma-2-27B</code> on a 24GB GPU</li> </ul> <p>These techniques allow us to maximize model size without scaling hardware further.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#memory-efficiency-via-quantization","title":"Memory Efficiency via Quantization","text":""},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#4-bit-quantization","title":"\ud83d\udcc9 4-bit Quantization","text":"<ul> <li>Reduces model weights from 16/32 bits to 4 bits</li> <li>Uses double quantization: 8-bit quantization followed by 4-bit compression</li> <li>Implements NF4 (Normalized Float) for high precision retention</li> <li>Memory savings allow fitting massive models on smaller GPUs</li> </ul>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#qlora","title":"\ud83e\udde0 qLoRA","text":"<ul> <li>A variant of LoRA integrating quantization into adapter training</li> <li>Propagates gradients only through low-rank matrices, while freezing the backbone</li> <li>Enables training models like <code>Gemma-2-27B</code> efficiently</li> </ul> <p>\ud83d\ude80 Outcome: - Fits 27B parameters on a single 24GB GPU - Matches or exceeds performance of full-precision fine-tuning</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#gemma-2-27b-model-training","title":"Gemma-2 27B Model Training","text":"<p>We moved from Microsoft's Phi-3 to Google's <code>Gemma-2-27B</code> for long-instruction fine-tuning.</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#training-setup","title":"\ud83e\uddfe Training Setup:","text":"<ul> <li>Model: <code>unsloth/gemma-2-27b-it</code></li> <li>Dataset: <code>LongAlpaca-12k</code></li> <li>Precision: 4-bit (<code>load_in_4bit: true</code>)</li> <li>Adapter: <code>qlora</code></li> <li>Flash Attention: Enabled</li> <li>Hardware: Single 24GB GPU</li> </ul>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#challenges","title":"\u26a0\ufe0f Challenges:","text":"<ul> <li>Training failed at 8-bit due to OOM</li> <li>4-bit loading + qLoRA enabled training to proceed</li> <li>Achieved full convergence and speed on low-resource setup</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#yaml-configuration","title":"YAML Configuration","text":"<pre><code># model params\nbase_model: unsloth/gemma-2-27b-it\n\n# dataset params\ndatasets:\n  - path: Yukang/LongAlpaca-12k\n    type: alpaca\n\noutput_dir: ./models/gemma-LongAlpaca\n\n# training setup\nsequence_length: 1024\nbf16: auto\ntf32: false\nmicro_batch_size: 1\nnum_epochs: 1\noptimizer: adamw_bnb_8bit\nlearning_rate: 0.0002\nlogging_steps: 1\n\n# adapter and quantization\nadapter: qlora\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear: true\nload_in_8bit: false\nload_in_4bit: true\ngradient_accumulation_steps: 1\ngradient_checkpointing: true\nflash_attention: true\n</code></pre> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#colab-notebook_3","title":"Colab Notebook","text":"<p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/11-llm-specialized/#references-further-reading_3","title":"References &amp; Further Reading","text":"<ul> <li>Gemma 2-27B on Hugging Face</li> <li>LongAlpaca Dataset \u2013 Hugging Face</li> <li>QLoRA: Efficient Finetuning of LLMs</li> <li>LoRA: Low-Rank Adaptation</li> <li>NF4 Quantization Paper</li> <li>Transformers Documentation \u2013 Hugging Face</li> <li>Flash Attention (ArXiv)</li> <li>Axolotl Fine-Tuning GitHub</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/","title":"12. Final Deployment","text":""},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#quick-navigation","title":"Quick Navigation","text":"<ul> <li>Introductory Concepts for Scale Training</li> <li>Understanding DeepSpeed Theoretically</li> <li>Implementing DeepSpeed Practically</li> <li>FSDP Theoretical Insights</li> <li>Applying FSDP in Practice</li> <li>Course Conclusion and Future Directions</li> <li>Practice Test 2: LLM Mastery</li> </ul>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#introductory-concepts-for-scale-training","title":"Introductory Concepts for Scale Training","text":""},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#quick-navigation_1","title":"Quick Navigation","text":"<ul> <li>Introduction to Multi-GPU Scaling</li> <li>Why Scaling is Necessary</li> <li>Strategic Goals of Scaling</li> <li>From Optimization to Hardware Expansion</li> <li>Preview of Scaling Frameworks</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#introduction-to-multi-gpu-scaling","title":"Introduction to Multi-GPU Scaling","text":"<p>In this final section, we shift our focus from single-machine optimization to scaling across multiple GPUs or nodes. While techniques such as quantization, LoRA, and flash attention helped maximize limited resources, we now explore strategies that increase total computational power by adding hardware.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#why-scaling-is-necessary","title":"Why Scaling is Necessary","text":"<p>Scaling allows us to break free from the physical constraints of a single GPU. There are two primary motivations for doing so:</p> <ul> <li>\ud83e\udde0 Training larger models: Eventually, no matter how optimized, a model won't fit into GPU memory.</li> <li>\u26a1 Faster training: Even for smaller models, scaling enables faster iteration and experimentation by parallelizing the training workload.</li> </ul> <p>State-of-the-art LLMs (like GPT-4, Gemini, Claude) are trained across hundreds to thousands of GPUs simultaneously.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#strategic-goals-of-scaling","title":"Strategic Goals of Scaling","text":"<p>Scaling isn\u2019t only about \u201cgoing bigger.\u201d It\u2019s also about going faster, or both.</p> <ul> <li>Goal 1: Train larger models that wouldn't fit in memory otherwise.</li> <li>Goal 2: Train faster, reducing the time per epoch or iteration.</li> <li>Hybrid Goal: Train moderately larger models faster, using distributed compute efficiently.</li> </ul> <p>\ud83d\udd01 The balance between size, speed, and hardware availability guides the scaling strategy.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#from-optimization-to-hardware-expansion","title":"From Optimization to Hardware Expansion","text":"<p>Previously, we relied on: - 4-bit quantization - Flash Attention - QLoRA - Efficient batch and sequence tuning</p> <p>These maximize utilization of a single GPU, often squeezing 27B+ parameter models into 24GB cards.</p> <p>Now, we explore hardware scaling, where: - Multiple GPUs share memory and gradient updates - Compute is parallelized for greater throughput</p> <p>This introduces complexity in terms of synchronization, communication overhead, and model sharding, but unlocks next-level capabilities.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#preview-of-scaling-frameworks","title":"Preview of Scaling Frameworks","text":"<p>We will explore and compare two major frameworks for distributed LLM training:</p> <ol> <li>DeepSpeed (by Microsoft):</li> <li>Efficient training of very large models</li> <li> <p>Includes ZeRO optimizer and communication-efficient primitives</p> </li> <li> <p>Fully Sharded Data Parallel (FSDP) (by PyTorch):</p> </li> <li>Parameter, gradient, and optimizer sharding</li> <li>Native PyTorch integration with strong memory savings</li> </ol> <p>Both frameworks enable: - Multi-GPU training - Reduced memory load per device - High-speed training with scalable architecture</p> <p>Next, we begin hands-on work with DeepSpeed.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>DeepSpeed GitHub</li> <li>DeepSpeed Docs</li> <li>FSDP Documentation \u2013 PyTorch</li> <li>Zero Redundancy Optimizer (ZeRO)</li> <li>Scaling Laws for Neural Language Models</li> <li>Accelerating Training with Flash Attention</li> <li>LoRA and QLoRA Papers</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#understanding-deepspeed-theoretically","title":"Understanding DeepSpeed Theoretically","text":""},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#quick-navigation_2","title":"Quick Navigation","text":"<ul> <li>What is DeepSpeed</li> <li>Why Use DeepSpeed</li> <li>Zero Optimization Stages</li> <li>Comparison of Zero Stages</li> <li>When to Use Each Stage</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#what-is-deepspeed","title":"What is DeepSpeed","text":"<p>DeepSpeed is an open-source deep learning optimization library developed by Microsoft. It is designed to: - Speed up training - Enable training of very large models - Efficiently utilize multiple GPUs through advanced parallelization strategies</p> <p>Key features include: - Model parallelism - Gradient and parameter partitioning - Memory and compute optimizations</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#why-use-deepspeed","title":"Why Use DeepSpeed","text":"<p>DeepSpeed is built for high-scale model training. It helps: - Distribute models across multiple GPUs - Lower memory footprint per GPU - Improve throughput in both research and production-scale workloads</p> <p>It is ideal for: - Training large LLMs efficiently - Use cases where hardware and time are critical resources</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#zero-optimization-stages","title":"Zero Optimization Stages","text":"<p>The core innovation of DeepSpeed is the ZeRO (Zero Redundancy Optimizer) framework, which is broken down into three stages:</p>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#stage-1-zero-1","title":"Stage 1: ZeRO-1","text":"<ul> <li>Partitions optimizer states across GPUs</li> <li>Provides slight memory savings</li> <li>Fastest among all stages</li> </ul>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#stage-2-zero-2","title":"Stage 2: ZeRO-2","text":"<ul> <li>Partitions both optimizer states and gradients</li> <li>Reduces memory load further</li> <li>Slower than ZeRO-1 due to added overhead</li> </ul>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#stage-3-zero-3","title":"Stage 3: ZeRO-3","text":"<ul> <li>Partitions optimizer states, gradients, and model parameters</li> <li>Offers maximum memory savings</li> <li>Slowest stage due to high communication overhead</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#comparison-of-zero-stages","title":"Comparison of Zero Stages","text":"Stage Speed Memory Efficiency Components Partitioned ZeRO-1 \ud83d\udfe2 Fastest \ud83d\udd34 Lowest Optimizer states ZeRO-2 \ud83d\udfe1 Moderate \ud83d\udfe1 Moderate Optimizer states + Gradients ZeRO-3 \ud83d\udd34 Slowest \ud83d\udfe2 Highest Optimizer states + Gradients + Model Parameters <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#when-to-use-each-stage","title":"When to Use Each Stage","text":"<ul> <li>ZeRO-1: Use when training time is the main constraint and memory is not a major issue.</li> <li>ZeRO-2: Use when training moderately large models with balanced speed/memory tradeoff.</li> <li>ZeRO-3: Use when maximum model size is required, even at the cost of training speed.</li> </ul> <p>These stages give developers flexibility to choose based on their needs and available hardware.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#references-further-reading_1","title":"References &amp; Further Reading","text":"<ul> <li>DeepSpeed GitHub</li> <li>ZeRO Paper \u2013 Optimizer Redundancy Elimination</li> <li>DeepSpeed Documentation</li> <li>PyTorch FSDP Docs (for comparison)</li> <li>Efficient Training Techniques by Microsoft</li> </ul> <p>Back to Top</p> <p>--</p>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#implementing-deepspeed-practically","title":"Implementing DeepSpeed Practically","text":""},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#quick-navigation_3","title":"Quick Navigation","text":"<ul> <li>Setting Up the Environment</li> <li>Training LLaMA 3-8B Without DeepSpeed</li> <li>Enabling DeepSpeed Integration</li> <li>Running Training on Multiple GPUs</li> <li>Zero-1 JSON Configuration</li> <li>DeepSpeed Training YAML Configuration</li> <li>Colab Notebook</li> <li>References &amp; Further Reading</li> </ul>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#setting-up-the-environment","title":"Setting Up the Environment","text":"<p>In this lesson, we apply DeepSpeed practically to train the <code>Meta-LLaMA-3.1-8B-Instruct</code> model using two GPUs. The goal is to optimize training speed rather than memory efficiency.</p> <ul> <li>Starting from an Axolotl-based setup</li> <li>Using the SQuAD dataset</li> <li>Measuring single-GPU vs. multi-GPU time comparisons</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#training-llama-3-8b-without-deepspeed","title":"Training LLaMA 3-8B Without DeepSpeed","text":"<p>Initial benchmark: - Run on 1 GPU - Training time: ~1 hour 40 minutes - CUDA visibility set to restrict GPU usage - Dataset: <code>squad-for-llms</code> - Model: <code>Meta-LLaMA-3.1-8B-Instruct</code></p> <p>This baseline helps quantify the gains from enabling DeepSpeed later.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#enabling-deepspeed-integration","title":"Enabling DeepSpeed Integration","text":"<p>DeepSpeed support is built into Axolotl. It provides default JSON configs for: - ZeRO-1 - ZeRO-2 - ZeRO-3</p>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#how-to-enable","title":"How to Enable:","text":"<ol> <li>Copy the desired config (e.g., <code>zero1.json</code>) from Axolotl DeepSpeed Configs</li> <li>Set it in your training command via <code>deepspeed</code> parameter</li> <li>Launch Axolotl with multiple GPUs (remove <code>CUDA_VISIBLE_DEVICES</code> restriction)</li> </ol> <pre><code>CUDA_VISIBLE_DEVICES=0,1 accelerate launch train.py --deepspeed=zero1.json ...\n</code></pre> <p>\ud83d\udfe2 Outcome: Multi-GPU training initiated with backend set to DeepSpeed</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#running-training-on-multiple-gpus","title":"Running Training on Multiple GPUs","text":"<ul> <li>Training is re-launched using both GPUs</li> <li>ZeRO-1 stage selected (focused on speed, not memory saving)</li> <li>Each GPU runs a full copy of the model</li> <li>Results:</li> <li>New training time: ~1 hour</li> <li>~40% speed improvement with 2 GPUs</li> </ul> <p>\ud83d\udd01 Although not linear, the scaling is significant and useful in production and experimentation workflows.</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#zero-1-json-configuration","title":"Zero-1 JSON Configuration","text":"<pre><code>{\n  \"zero_optimization\": {\n    \"stage\": 1,\n    \"overlap_comm\": true\n  },\n  \"bf16\": {\n    \"enabled\": \"auto\"\n  },\n  \"fp16\": {\n    \"enabled\": \"auto\",\n    \"auto_cast\": false,\n    \"loss_scale\": 0,\n    \"initial_scale_power\": 32,\n    \"loss_scale_window\": 1000,\n    \"hysteresis\": 2,\n    \"min_loss_scale\": 1\n  },\n  \"gradient_accumulation_steps\": \"auto\",\n  \"gradient_clipping\": \"auto\",\n  \"train_batch_size\": \"auto\",\n  \"train_micro_batch_size_per_gpu\": \"auto\",\n  \"wall_clock_breakdown\": false\n}\n</code></pre> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#deepspeed-training-yaml-configuration","title":"DeepSpeed Training YAML Configuration","text":"<pre><code>base_model: unsloth/Meta-Llama-3.1-8B-Instruct\n\ndatasets:\n  - path: TheFuzzyScientist/squad-for-llms\n    type: \n      system_prompt: \"Read the following context and concisely answer my question.\"\n      field_system: system\n      field_instruction: question\n      field_input: context\n      field_output: output\n      format: \"&lt;|user|&gt;\\n {input} {instruction} &lt;/s&gt;\\n&lt;|assistant|&gt;\"\n      no_input_format: \"&lt;|user|&gt; {instruction} &lt;/s&gt;\\n&lt;|assistant|&gt;\"\n\noutput_dir: ./models/Llama3_squad\n\nsequence_length: 2048\nbf16: auto\ntf32: false\nmicro_batch_size: 4\nnum_epochs: 4\noptimizer: adamw_bnb_8bit\nlearning_rate: 0.0002\nlogging_steps: 1\n\nadapter: lora\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear: true\ngradient_accumulation_steps: 1\ngradient_checkpointing: true\n</code></pre> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#colab-notebook","title":"Colab Notebook","text":"<p>\ud83d\udc49 Open in Colab </p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#references-further-reading_2","title":"References &amp; Further Reading","text":"<ul> <li>DeepSpeed GitHub</li> <li>Axolotl DeepSpeed Configs</li> <li>Hugging Face DeepSpeed Guide</li> <li>ZeRO Optimizer Paper</li> <li>Axolotl Project</li> <li>SQuAD Dataset \u2013 Hugging Face</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#fsdp-theoretical-insights","title":"FSDP Theoretical Insights","text":"<p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#applying-fsdp-in-practice","title":"Applying FSDP in Practice","text":"<p>--</p> <p>Back to Top</p>"},{"location":"study-path/transformers-generative-ai/12-llm-deployment/#course-conclusion-and-future-directions","title":"Course Conclusion and Future Directions","text":"<p>Back to Top</p>"}]}