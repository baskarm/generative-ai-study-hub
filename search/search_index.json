{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Generative AI Study Hub","text":"<p>Last updated: July 26, 2025</p> <p>This is your central hub for learning and referencing Generative AI concepts.  Choose from the Study Path to go through structured topics or explore the Reference Hub for focused research and tooling insights.</p>"},{"location":"reference-hub/","title":"Reference Hub","text":"<p>Welcome to the Reference Hub! Here you'll find foundational topics, research tools, and hands-on practices for working with Generative AI.</p>"},{"location":"study-path/","title":"Study Path","text":"<p>Welcome to the Study Path! Explore structured lessons on Transformers, LLMOps, and Evaluation Frameworks.</p>"},{"location":"study-path/transformers/01-intro/","title":"What You\u2019ll Learn","text":"<p>\ud83d\udccc Quick Navigation</p> <ul> <li>What You\u2019ll Learn</li> <li>Part 1: Fundamentals of NLP and Transformers<ul> <li>Transformer Foundations</li> </ul> </li> <li>Part 2: Working with Large Language Models (LLMs)<ul> <li>Key Model Architectures</li> <li>Real-World Tasks You\u2019ll Implement</li> <li>Advanced Tooling and Techniques</li> </ul> </li> </ul>"},{"location":"study-path/transformers/01-intro/#part-1-fundamentals-of-nlp-and-transformers","title":"Part 1: Fundamentals of NLP and Transformers","text":"<p>You\u2019ll explore the evolution of natural language processing (NLP) through four historical phases:</p> <ul> <li>Rule-Based Systems </li> <li> <p>Manually defined rules for parsing, tagging, and other language tasks.</p> </li> <li> <p>Statistical Methods </p> </li> <li> <p>Used mathematical probability and co-occurrence to model language.</p> </li> <li> <p>Machine Learning Era </p> </li> <li> <p>Leveraged labeled data for training classifiers like SVMs and Naive Bayes.</p> </li> <li> <p>Deep Learning &amp; Embeddings </p> </li> <li>Enabled dense semantic understanding and contextual word representations.</li> </ul>"},{"location":"study-path/transformers/01-intro/#transformer-foundations","title":"Transformer Foundations","text":"<p>You\u2019ll also build a deep understanding of the architecture that powers modern LLMs:</p> <ul> <li>Attention Mechanism </li> <li> <p>Allows models to focus on important parts of the input.</p> </li> <li> <p>Encoder-Decoder Structure </p> </li> <li> <p>Enables tasks like translation, summarization, and text generation.</p> </li> <li> <p>Tokenization &amp; Embeddings </p> </li> <li> <p>Converts text into vectors, enabling model computation.</p> </li> <li> <p>Pretraining and Fine-Tuning </p> </li> <li>Learn how general-purpose models adapt to specific tasks.</li> </ul>"},{"location":"study-path/transformers/01-intro/#part-2-working-with-large-language-models-llms","title":"Part 2: Working with Large Language Models (LLMs)","text":"<p>This section focuses on state-of-the-art transformer-based models and their practical applications.</p>"},{"location":"study-path/transformers/01-intro/#key-model-architectures","title":"Key Model Architectures","text":"<ul> <li>BERT (Encoder-only) </li> <li> <p>Learns bidirectional context; ideal for understanding tasks like classification or Q&amp;A.</p> </li> <li> <p>GPT (Decoder-only) </p> </li> <li> <p>Generates coherent, fluent text; the backbone of tools like ChatGPT.</p> </li> <li> <p>T5 (Encoder-Decoder) </p> </li> <li>Treats all problems as a text-to-text task, offering maximum flexibility.</li> </ul>"},{"location":"study-path/transformers/01-intro/#real-world-tasks-youll-implement","title":"Real-World Tasks You\u2019ll Implement","text":"<ul> <li>Masked Language Modeling (MLM)  </li> <li>Semantic Search with embeddings  </li> <li>Document-Based Question Answering  </li> <li>Instruction-Following Text Generation  </li> <li>Product Review Generation (prompt-based)</li> </ul>"},{"location":"study-path/transformers/01-intro/#advanced-tooling-and-techniques","title":"Advanced Tooling and Techniques","text":"<p>You\u2019ll gain hands-on experience with modern model optimization strategies:</p> <ul> <li>LoRA and PeFT (parameter-efficient fine-tuning)  </li> <li>8-bit / 4-bit quantization for faster, smaller models  </li> <li>FlashAttention, DeepSpeed, and FSDP for accelerated training  </li> <li>Chat templates and RLHF (Reinforcement Learning from Human Feedback)</li> </ul> <p>By completing this course, you\u2019ll be equipped to build, fine-tune, and deploy transformer-based models in real-world NLP applications.</p>"},{"location":"study-path/transformers/02-getting-started/","title":"Course Orientation Summary","text":"<p>Back to Top</p>"},{"location":"study-path/transformers/02-getting-started/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Course Orientation Summary</li> <li>Course Structure</li> <li>Who Should Take This Course</li> <li>Learning Recommendations</li> <li>Tools and Coding Approach</li> <li>Practical Learning Tips</li> <li>Outcomes and Goals</li> <li>Environment Setup for Practical LLM Work</li> <li>Recommended Editor</li> <li>Required Extensions</li> <li>Jupyter Notebooks and Google Colab</li> <li>GPU Access</li> <li>Remote Machine Setup (Optional)</li> <li>Best Practices for Practical Lessons</li> <li>Resources and Downloads</li> </ul>"},{"location":"study-path/transformers/02-getting-started/#course-structure","title":"Course Structure","text":"<ul> <li>Introductory Sections </li> <li>Set expectations and explain course format  </li> <li>Theory-Focused Modules </li> <li>Three foundational sections on NLP and transformer theory  </li> <li>Densely packed with essential concepts and examples  </li> <li>Hands-On Practice Modules </li> <li>Later sections demonstrate applied use of transformers  </li> <li>Real code examples for solving NLP problems  </li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/02-getting-started/#who-should-take-this-course","title":"Who Should Take This Course","text":"<ul> <li>Beginners </li> <li>Will gain solid foundational knowledge before diving into advanced techniques  </li> <li>Experienced NLP Practitioners </li> <li>Can reinforce understanding of core concepts and discover modern tools and strategies  </li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/02-getting-started/#learning-recommendations","title":"Learning Recommendations","text":"<ul> <li>Complete the full course if you're new to transformers and NLP  </li> <li>Jump to advanced sections if you're already experienced and want to apply concepts  </li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/02-getting-started/#tools-and-coding-approach","title":"Tools and Coding Approach","text":"<ul> <li>Editor: Visual Studio Code (VS Code)</li> <li>Interactive Environment: Google Colab for browser-based execution</li> <li>Source Access:</li> <li>Linked Python scripts and code snippets</li> <li>Model weights hosted via Hugging Face Model Hub  </li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/02-getting-started/#practical-learning-tips","title":"Practical Learning Tips","text":"<ul> <li>Code Along </li> <li>Type and run code during the lessons for active learning  </li> <li>Top-Down Learning </li> <li>Begin with complete examples and adapt them for your own use cases  </li> <li>Use Provided Resources </li> <li>Lessons provide all tools and links to minimize setup time  </li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/02-getting-started/#outcomes-and-goals","title":"Outcomes and Goals","text":"<ul> <li>Grasp the evolution of NLP from rule-based systems to large-scale transformers  </li> <li>Understand transformer model architecture, including BERT, GPT, and T5  </li> <li>Gain practical experience with modern LLM techniques </li> <li>Build the skills to deploy and adapt LLMs for real-world problems  </li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/02-getting-started/#environment-setup-for-practical-llm-work","title":"Environment Setup for Practical LLM Work","text":"<p>Back to Top</p>"},{"location":"study-path/transformers/02-getting-started/#recommended-editor","title":"Recommended Editor","text":"<ul> <li>Visual Studio Code (VS Code) </li> <li>Free and lightweight  </li> <li>Open-source  </li> <li>Excellent Python and remote development support  </li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/02-getting-started/#required-extensions","title":"Required Extensions","text":"<ul> <li>Python Extension </li> <li>Enables writing and running Python code interactively  </li> <li>Activate interactive mode via:<ul> <li><code>Cmd+Shift+P</code> (Mac) or <code>Ctrl+Shift+P</code> (Windows/Linux)</li> <li>Search: <code>\"Send to Interactive\"</code> </li> </ul> </li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/02-getting-started/#jupyter-notebooks-and-google-colab","title":"Jupyter Notebooks and Google Colab","text":"<ul> <li>Most lessons include Google Colab notebooks </li> <li>No local setup needed  </li> <li>Ideal for users without high-performance hardware  </li> <li>Links provided in each lesson\u2019s Resources tab </li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/02-getting-started/#gpu-access","title":"GPU Access","text":"<ul> <li>Necessary for heavy training and inference</li> <li>Options:</li> <li>Local machine with GPU</li> <li>Google Colab with GPU runtime (<code>Runtime \u2192 Change runtime type \u2192 GPU</code>)</li> <li>Cloud-based machines (e.g., AWS, GCP, LambdaLabs)</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/02-getting-started/#remote-machine-setup-optional","title":"Remote Machine Setup (Optional)","text":"<ul> <li>Install Remote SSH extension in VS Code</li> <li>Add your server info to <code>~/.ssh/config</code>:   ```bash   Host myserver     HostName your.ip.address     User username     IdentityFile ~/.ssh/id_rsa</li> </ul>"},{"location":"study-path/transformers/03-nlp-overview/","title":"NLP Evolution Timeline","text":""},{"location":"study-path/transformers/03-nlp-overview/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Historical NLP Techniques</li> <li>2. Statistical NLP Era</li> <li>3. Machine Learning Era in NLP</li> <li>4. Embedding Era in NLP</li> </ul>"},{"location":"study-path/transformers/03-nlp-overview/#1-historical-nlp-techniques","title":"1. Historical NLP Techniques","text":"<p>Understanding the evolution of NLP techniques provides critical context for modern advancements like transformers. This section explores foundational rule-based systems.</p>"},{"location":"study-path/transformers/03-nlp-overview/#rule-based-nlp-era","title":"Rule-Based NLP Era","text":"<ul> <li>Built on manually crafted linguistic rules</li> <li>Focused on syntactic analysis:</li> <li>Parsing: Grammatical structure and relationships</li> <li>Part-of-Speech Tagging: Identifying grammatical roles</li> <li>Applications:</li> <li>Syntax analysis</li> <li>Text summarization</li> <li>Machine translation</li> </ul>"},{"location":"study-path/transformers/03-nlp-overview/#key-limitations","title":"Key Limitations","text":"<ul> <li>Ambiguity: Poor context awareness</li> <li>Scalability: Rule creation and maintenance were not feasible at scale</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/03-nlp-overview/#2-statistical-nlp-era","title":"2. Statistical NLP Era","text":"<p>The transition to data-driven statistical techniques marked a turning point in NLP.</p>"},{"location":"study-path/transformers/03-nlp-overview/#key-innovations","title":"Key Innovations","text":"<ul> <li>Data-Driven Shift: Replaced rules with learned probabilities</li> <li>Probabilistic Language Models: Modeled word likelihoods and co-occurrence patterns</li> <li>n-Grams: Captured word sequences (e.g., bigrams, trigrams)</li> <li>Hidden Markov Models (HMMs):</li> <li>Used for sequence tasks (POS tagging, NER)</li> <li>Modeled state transitions for linguistic structure</li> </ul>"},{"location":"study-path/transformers/03-nlp-overview/#applications","title":"Applications","text":"<ul> <li>POS Tagging: Predict tags using probability sequences</li> <li>Named Entity Recognition (NER): Detect names, dates, organizations</li> </ul>"},{"location":"study-path/transformers/03-nlp-overview/#limitations","title":"Limitations","text":"<ul> <li>Data Sparsity: Rare word combinations weakened predictions</li> <li>Shallow Semantics: Couldn\u2019t truly \u201cunderstand\u201d meaning</li> </ul>"},{"location":"study-path/transformers/03-nlp-overview/#evolution","title":"Evolution","text":"<p>These limitations led to machine learning and neural models, enabling more scalable, adaptive solutions.</p> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/03-nlp-overview/#3-machine-learning-era-in-nlp","title":"3. Machine Learning Era in NLP","text":"<p>Machine learning enabled NLP systems to generalize from data without extensive rules or handcrafted features.</p>"},{"location":"study-path/transformers/03-nlp-overview/#key-advancements","title":"Key Advancements","text":"<ul> <li>Naive Bayes: Probabilistic classifier for text classification (e.g., spam detection)</li> <li>Support Vector Machines (SVMs):</li> <li>Effective for sentiment analysis</li> <li>Worked well on high-dimensional text vectors</li> </ul>"},{"location":"study-path/transformers/03-nlp-overview/#rise-of-neural-networks","title":"Rise of Neural Networks","text":"<ul> <li>Reduced Feature Engineering: Learned features from raw data</li> <li>Applications: Summarization, translation, sentiment detection</li> </ul>"},{"location":"study-path/transformers/03-nlp-overview/#specialized-architectures","title":"Specialized Architectures","text":"<ul> <li>RNNs:</li> <li>Process text sequentially</li> <li>Preserve past input using hidden state</li> <li> <p>Limitations: Weak on long-term dependencies</p> </li> <li> <p>LSTMs:</p> </li> <li>Enhanced RNNs with memory cells</li> <li>Better handling of long-range context</li> <li>Enabled language modeling and generation</li> </ul>"},{"location":"study-path/transformers/03-nlp-overview/#milestones","title":"Milestones","text":"<ul> <li>Shifted to end-to-end learning</li> <li>More flexible and powerful than statistical models</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/03-nlp-overview/#4-embedding-era-in-nlp","title":"4. Embedding Era in NLP","text":"<p>Dense vector embeddings enabled models to capture word meaning and similarity, surpassing sparse representations like one-hot encoding.</p>"},{"location":"study-path/transformers/03-nlp-overview/#key-concepts","title":"Key Concepts","text":"<ul> <li>Word Embeddings:</li> <li>Low-dimensional, dense vectors for each word</li> <li> <p>Capture meaning through context-based learning</p> </li> <li> <p>Benefits Over One-Hot Encoding:</p> </li> <li>Smaller dimensionality</li> <li>Encoded meaning and similarity</li> </ul>"},{"location":"study-path/transformers/03-nlp-overview/#popular-embedding-techniques","title":"Popular Embedding Techniques","text":"Technique Developer Method Highlights Word2Vec Google Skip-gram, CBOW Context prediction via local word windows GloVe Stanford Co-occurrence + global stats Combines frequency and semantics FastText Facebook AI Subword n-grams Handles rare and OOV words better"},{"location":"study-path/transformers/03-nlp-overview/#applications_1","title":"Applications","text":"<ul> <li>Semantic Similarity: Text comparison</li> <li>Text Classification: Improved input features</li> <li>Translation, QA: Foundation for neural systems</li> <li>Input to Deep Models: Used in RNNs, LSTMs, and later transformers</li> </ul>"},{"location":"study-path/transformers/03-nlp-overview/#limitations_1","title":"Limitations","text":"<ul> <li>Static Embeddings: One vector per word, no context awareness</li> <li>No Polysemy Handling: Same vector for multiple meanings (e.g., \u201cbank\u201d)</li> </ul> <p>These drawbacks triggered the rise of contextualized embeddings (e.g., ELMo, BERT), marking the start of the Transformer Era.</p> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/04-transformer-intro/","title":"Transformer Fundamentals","text":""},{"location":"study-path/transformers/04-transformer-intro/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. Transformer Architecture Overview</li> <li>2. Transformer Training Paradigm: Pre-training and Fine-tuning</li> <li>3. Tokenization and Embeddings in Transformer Models</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#1-transformer-architecture-overview","title":"1. Transformer Architecture Overview","text":"<p>This lesson introduces the transformer model architecture, emphasizing its structural innovations, key mechanisms, and how it revolutionized NLP by overcoming the limitations of RNNs and LSTMs.</p>"},{"location":"study-path/transformers/04-transformer-intro/#origins-and-significance","title":"Origins and Significance","text":"<ul> <li>Introduced in 2017 via the paper \"Attention Is All You Need\"</li> <li>Replaced sequential RNN/LSTM processing with fully parallel architecture</li> <li>Solved long-range dependency issues and improved training speed</li> <li>Enabled large-scale model training and breakthroughs in language understanding</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#core-components-of-transformer-architecture","title":"Core Components of Transformer Architecture","text":""},{"location":"study-path/transformers/04-transformer-intro/#encoder-decoder-structure","title":"Encoder-Decoder Structure","text":"<ul> <li>Encoder: Converts input text into continuous vector representations capturing context and relationships</li> <li>Decoder: Generates output text from encoder\u2019s processed information</li> <li>Enables tasks like translation, summarization, and question answering</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#attention-mechanisms","title":"Attention Mechanisms","text":"<ul> <li>Self-Attention: Weighs each word relative to others to build context-aware representations</li> <li>Scaled Dot-Product Attention: Computes dot products, scales scores, and applies softmax</li> <li>Multi-Head Attention: Uses multiple heads to capture diverse semantic/syntactic patterns</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#positional-encoding","title":"Positional Encoding","text":"<ul> <li>Compensates for lack of inherent word order in attention-only models</li> <li>Adds position-based signals to token embeddings</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#feed-forward-network-layer-normalization","title":"Feed-Forward Network &amp; Layer Normalization","text":"<ul> <li>Feed-Forward Network: Applies non-linear transformations to extract high-level features</li> <li>Layer Normalization: Stabilizes training by normalizing outputs between layers</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#full-encoder-and-decoder-block","title":"Full Encoder and Decoder Block","text":"<ul> <li>Composed of stacked layers with:</li> <li>Multi-head attention</li> <li>Feed-forward networks</li> <li>Layer normalization</li> <li>Decoder includes additional encoder-decoder attention to align output generation</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#real-world-application-example","title":"Real-World Application Example","text":"<p>Abstractive Question Answering</p> <ul> <li>Input: Paragraph + Question</li> <li>Encoder: Processes both into contextual embeddings</li> <li>Decoder: Generates an answer from the learned representation</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Transformers enabled scalable, parallel NLP processing</li> <li>Encoder-decoder architecture allows diverse tasks</li> <li>Attention mechanisms are key to understanding global context</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/04-transformer-intro/#2-transformer-training-paradigm-pre-training-and-fine-tuning","title":"2. Transformer Training Paradigm: Pre-training and Fine-tuning","text":"<p>This lesson outlines the two-phase training process of transformer models\u2014pre-training and fine-tuning\u2014contrasting it with traditional ML workflows.</p>"},{"location":"study-path/transformers/04-transformer-intro/#training-structure-overview","title":"Training Structure Overview","text":"<ul> <li>Pre-training: General language learning from large unlabeled datasets</li> <li>Fine-tuning: Task-specific adaptation using labeled datasets</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#pre-training-phase","title":"Pre-training Phase","text":"<ul> <li>Learns grammar, context, word relationships, and long-range dependencies</li> <li>Massive-scale unsupervised training</li> <li>\ud83d\udd01 Analogy: Like learning music theory before mastering a genre</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#fine-tuning-phase","title":"Fine-tuning Phase","text":"<ul> <li>Adapts pre-trained models to tasks like NER, translation, QA, etc.</li> <li>Requires smaller supervised datasets</li> <li>Leverages transfer learning</li> <li>\ud83d\udd01 Analogy: Like a trained pianist specializing in jazz</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#combined-workflow","title":"Combined Workflow","text":"<ol> <li>Step 1: Pre-training</li> <li>Random initialization \u2192 trained on general data</li> <li>Step 2: Fine-tuning</li> <li>Task-specific data \u2192 adapted for downstream performance</li> </ol>"},{"location":"study-path/transformers/04-transformer-intro/#real-world-considerations","title":"Real-world Considerations","text":"<ul> <li>Pre-training requires huge compute and data (done by orgs like Google, OpenAI)</li> <li>Most use pre-trained models and fine-tune</li> <li>Full pre-training is rare unless:</li> <li>You work with proprietary, underrepresented, or specialized domains (e.g., legal, clinical)</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#key-takeaways_1","title":"Key Takeaways","text":"<ul> <li>Pre-training + fine-tuning is the standard approach in NLP</li> <li>Enables rapid model deployment with high performance</li> <li>Specialized domains may benefit from custom pre-training</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/04-transformer-intro/#3-tokenization-and-embeddings-in-transformer-models","title":"3. Tokenization and Embeddings in Transformer Models","text":"<p>This lesson covers how transformers process raw text into vector representations using tokenization and embeddings.</p>"},{"location":"study-path/transformers/04-transformer-intro/#tokenization","title":"Tokenization","text":""},{"location":"study-path/transformers/04-transformer-intro/#purpose","title":"Purpose","text":"<ul> <li>Breaks text into smaller units called tokens</li> <li>Translates natural language into numerical input (token IDs)</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#types-of-tokenization","title":"Types of Tokenization","text":"<ul> <li>Word-level: One token per word; suffers from OOV (out-of-vocabulary) issues</li> <li>Character-level: Every character is a token; leads to longer sequences</li> <li>Subword-level (common): Breaks unknown words into known parts (e.g., Byte-Pair Encoding)</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#workflow","title":"Workflow","text":"<ol> <li>Breaks text into tokens</li> <li>Maps tokens to IDs using a predefined vocabulary</li> <li>Feeds IDs into the transformer model</li> </ol>"},{"location":"study-path/transformers/04-transformer-intro/#embeddings","title":"Embeddings","text":""},{"location":"study-path/transformers/04-transformer-intro/#purpose_1","title":"Purpose","text":"<ul> <li>Convert token IDs into high-dimensional dense vectors</li> <li>Capture meaning and contextual usage of tokens</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#key-concepts","title":"Key Concepts","text":"<ul> <li>Embeddings are context-aware (e.g., \"bank\" in finance vs. riverbank)</li> <li>Contextual embeddings change based on surrounding text</li> <li>Learned during pre-training</li> </ul>"},{"location":"study-path/transformers/04-transformer-intro/#example","title":"Example","text":"<p>```text Sentence 1: She picked a rose. Sentence 2: The sun rose early.</p>"},{"location":"study-path/transformers/05-popular-transformer-models/","title":"Transformer Architectures Study Hub","text":""},{"location":"study-path/transformers/05-popular-transformer-models/#quick-navigation","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>1. BERT: Encoder-Only Transformer Architecture</li> <li>2. Transformer &amp; GPT Evolution</li> <li>3. T5: Text-To-Text Transfer Transformer</li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#1-bert-encoder-only-transformer-architecture","title":"1. BERT: Encoder-Only Transformer Architecture","text":"<p>BERT (Bidirectional Encoder Representations from Transformers) revolutionized NLP in 2018 by introducing a bidirectional, encoder-only architecture designed for deep contextual understanding of language. This section explores BERT\u2019s structure, training strategy, practical applications, and the latest advancements in its ecosystem.</p>"},{"location":"study-path/transformers/05-popular-transformer-models/#model-overview","title":"Model Overview","text":""},{"location":"study-path/transformers/05-popular-transformer-models/#key-characteristics","title":"Key Characteristics","text":"<ul> <li> <p>Bidirectional   BERT reads text in both directions (left-to-right and right-to-left) simultaneously to capture full context.</p> </li> <li> <p>Encoder-Only Architecture   Built entirely on stacked encoders with self-attention mechanisms.   Optimized for understanding, not generating, text.</p> </li> <li> <p>Representations   Learns dense vector embeddings that reflect token meaning in context.</p> </li> <li> <p>Transformer-Based   Leverages the original transformer architecture\u2014only the encoder side.</p> </li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#pre-training-strategy","title":"Pre-training Strategy","text":""},{"location":"study-path/transformers/05-popular-transformer-models/#datasets","title":"Datasets","text":"<ul> <li>English Wikipedia  </li> <li>10,000+ unpublished English books  </li> <li>Total: Over 3 billion words</li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#pre-training-objectives","title":"Pre-training Objectives","text":"<ul> <li> <p>Masked Language Modeling (MLM)   Randomly masks 15% of tokens; the model must predict them using surrounding context.   Enables deep semantic and syntactic comprehension.</p> </li> <li> <p>Next Sentence Prediction (NSP)   Trains BERT to classify whether one sentence follows another.   Aids understanding of inter-sentence relationships.   Later models (e.g., RoBERTa) removed this due to limited benefit.</p> </li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#fine-tuning-applications","title":"Fine-tuning Applications","text":""},{"location":"study-path/transformers/05-popular-transformer-models/#text-classification","title":"Text Classification","text":"<ul> <li>Sentiment analysis, spam detection, topic categorization  </li> <li>Produces a single class label from the encoded text</li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#named-entity-recognition-ner","title":"Named Entity Recognition (NER)","text":"<ul> <li>Identifies token-level entities (e.g., people, dates, organizations)  </li> <li>BERT's contextual awareness improves accuracy in boundary detection</li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#extractive-question-answering","title":"Extractive Question Answering","text":"<ul> <li>Extracts answers directly from a provided context passage  </li> <li>Predicts start and end token positions  </li> <li>Used in customer service, document retrieval</li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#semantic-similarity","title":"Semantic Similarity","text":"<ul> <li>Produces embeddings for entire sentences or passages  </li> <li>Used in:</li> <li>Duplicate detection  </li> <li>Paraphrase recognition  </li> <li>Semantic search  </li> <li>Vector-based retrieval systems</li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#bert-model-variants","title":"BERT Model Variants","text":"Model Parameters Notes BERT-Base ~110M 12 layers, 12 heads, 768 hidden units BERT-Large ~340M 24 layers, 16 heads, 1024 hidden units DistilBERT ~66M Lightweight version by Hugging Face RoBERTa ~125M+ No NSP, trained longer, dynamic masking (Meta) ALBERT ~12M\u2013223M Weight-sharing, efficient training (Google Research) DeBERTa Varies Disentangled attention and enhanced position embeddings (Microsoft)"},{"location":"study-path/transformers/05-popular-transformer-models/#latest-developments-as-of-2025","title":"Latest Developments (as of 2025)","text":"<ul> <li>BERT is foundational for retrieval-augmented generation (RAG) and embedding-based search systems.</li> <li>Multilingual BERT (mBERT) supports 100+ languages.</li> <li>BERT encoders are commonly paired with large decoders like GPT-4o for hybrid retrieval-generation systems.</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/05-popular-transformer-models/#2-transformer-gpt-evolution","title":"2. Transformer &amp; GPT Evolution","text":""},{"location":"study-path/transformers/05-popular-transformer-models/#gpt-45-orion","title":"GPT-4.5 (\u201cOrion\u201d)","text":"<ul> <li>Released: Feb 27, 2025</li> <li>Enhanced instruction-following, fewer hallucinations</li> <li>API &amp; ChatGPT Pro access</li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#gpt-41-family","title":"GPT-4.1 Family","text":"<ul> <li>Released: April 14, 2025</li> <li>Includes mini/nano variants supporting 1M-token context</li> <li>More efficient than GPT-4o</li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#reasoning-models-o1-o3-mini-o4-mini","title":"Reasoning Models (o1, o3-mini, o4-mini)","text":"<ul> <li>Optimized for logic, math, and science</li> <li>o3-mini and o4-mini include multimodal chain-of-thought support</li> <li>Ideal for autonomous agents and structured tool use</li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#gpt-5-expected-august-2025","title":"GPT-5 (Expected August 2025)","text":"<ul> <li>Will include reasoning from o3</li> <li>Multimodal + open access discussions ongoing</li> <li>Expected to set a new benchmark for general-purpose AI</li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#why-it-matters","title":"Why It Matters","text":"<ul> <li>Shift from scaling parameters to scaling reasoning</li> <li>GPT-4.5/5 marks evolution toward modular, low-latency, high-accuracy models</li> </ul> Model Category Architecture Strengths Use Cases GPT\u20114.5 Instructional GPT Decoder-only Prompt-following, fewer hallucinations General NLP, coding, chatbots GPT\u20114.1 mini Efficient GPT Decoder-only 1M context, fast inference Coding, RAG o3-mini Reasoning LLM Decoder-only Logic + math + tool use Agents, science tasks GPT\u20115 Unified Multi-module Multimodal, reasoning-first Enterprise AI, general AI <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/05-popular-transformer-models/#3-t5-text-to-text-transfer-transformer","title":"3. T5: Text-To-Text Transfer Transformer","text":"<p>T5 reframes every NLP problem as a text-to-text task (e.g., input: \u201cTranslate English to German: How are you?\u201d \u2192 output: \u201cWie geht es dir?\u201d). This unified approach enables a wide range of applications across translation, QA, summarization, and more.</p>"},{"location":"study-path/transformers/05-popular-transformer-models/#model-overview_1","title":"Model Overview","text":"<ul> <li>Encoder-decoder transformer with BERT-style encoding + GPT-style generation</li> <li>Flexible task control via text prefixes (e.g., \u201csummarize:\u201d, \u201ctranslate:\u201d)</li> <li>First model to fully embrace text-to-text multitask learning</li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#pre-training-c4-dataset-fill-in-the-blank-generation","title":"Pre-training: C4 Dataset + Fill-in-the-Blank Generation","text":"<ul> <li>Uses a corrupt-and-reconstruct pre-training objective</li> <li>Learns both contextual understanding and sequence generation</li> <li>Trained on C4 (Colossal Cleaned Crawled Corpus)</li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#key-use-cases","title":"Key Use Cases","text":"<ul> <li>Translation: Understands bidirectional input, generates fluent target text</li> <li>Summarization: Converts long passages into concise summaries</li> <li>Question Answering: Context-aware, generative answers</li> <li>Keyword Generation: Contextual phrase extraction</li> </ul>"},{"location":"study-path/transformers/05-popular-transformer-models/#product-evolution-table","title":"Product Evolution Table","text":"Model Architecture Strengths Use Cases Developer T5-Base Encoder-Decoder Multitask learning, flexible Translation, QA, summarization Google AI mT5 Encoder-Decoder Multilingual model (100+ langs) Cross-lingual NLP Google AI FLAN-T5 Enc-Dec + Tuning Instruction tuning Zero-shot &amp; few-shot NLP Google Research UL2 Encoder-Decoder Supports multiple objective modes General-purpose transformer Google DeepMind Gemini 1.5 Multimodal Unified vision + text + code Multimodal reasoning, generation Google DeepMind"},{"location":"study-path/transformers/05-popular-transformer-models/#takeaways","title":"Takeaways","text":"<ul> <li>T5 demonstrates the power of a unified framework in solving diverse NLP tasks</li> <li>Its design has influenced instruction-tuned and multimodal model families</li> <li>Continues to power a range of Google products and NLP pipelines</li> </ul> <p>\u27a1\ufe0f Back to Top</p>"},{"location":"study-path/transformers/06-using-transformers/","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Course Overview</li> <li>Key Concepts</li> <li>Tokenizer and Embeddings</li> <li>Masked Language Modeling</li> <li>Semantic Search Engine</li> <li>Model Evolution Table</li> </ul>"},{"location":"study-path/transformers/06-using-transformers/#course-overview","title":"Course Overview","text":"<p>This section focuses on transitioning from theoretical knowledge of transformer models to their practical implementation and engineering components, emphasizing real-world applications such as semantic search and embedding usage.</p> <ul> <li>Prepares learners to apply transformer embeddings for NLP tasks</li> <li>Covers tokenization, embeddings, model internals, and downstream tasks</li> <li>Includes practical hands-on coding with Hugging Face Transformers and PyTorch</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/06-using-transformers/#key-concepts","title":"Key Concepts","text":""},{"location":"study-path/transformers/06-using-transformers/#transformer-engineering-focus","title":"Transformer Engineering Focus","text":"<ul> <li>Embeddings: Represent words/sentences as dense vectors for downstream processing</li> <li>Tokenization: Converts raw text to token IDs; includes handling special tokens</li> <li>Attention Mechanism: Key to contextual representation in transformers</li> <li>Model Inputs: Includes token IDs, attention masks, and token type IDs</li> <li>Sentence Transformers: Fine-tuned models for capturing sentence-level semantics</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/06-using-transformers/#tokenizer-and-embeddings","title":"Tokenizer and Embeddings","text":""},{"location":"study-path/transformers/06-using-transformers/#tokenization-pipeline","title":"Tokenization Pipeline","text":"<ul> <li>Tokenizers split sentences into subword tokens</li> <li>Maintains a vocabulary of ~30k+ tokens</li> <li>Returns token IDs, attention masks, and token type IDs</li> <li>Important to use model-specific tokenizers for consistency</li> </ul>"},{"location":"study-path/transformers/06-using-transformers/#try-it-yourself","title":"Try It Yourself","text":"<p>Explore and run the notebook interactively using Google Colab:</p> <p>Open the Notebook in Colab</p> <p></p>"},{"location":"study-path/transformers/06-using-transformers/#embeddings","title":"Embeddings","text":"<ul> <li>Token IDs are converted to high-dimensional vectors</li> <li>Two key outputs:</li> <li>Last Hidden State: Embeddings for individual tokens (shape: seq_len \u00d7 hidden_dim)</li> <li>Pooled Output: Embedding for the entire sequence, used in classification</li> </ul>"},{"location":"study-path/transformers/06-using-transformers/#try-it-yourself_1","title":"Try It Yourself","text":"<p>You can run and explore the notebook directly in Google Colab:</p> <p>Open the Notebook in Colab</p> <p></p> <p>Back to Top</p>"},{"location":"study-path/transformers/06-using-transformers/#semantic-distance","title":"Semantic Distance","text":"<ul> <li>Embeddings compared using cosine similarity</li> <li>Allows words with different meanings (e.g., \"fly\") to be distinguished contextually</li> </ul>"},{"location":"study-path/transformers/06-using-transformers/#masked-language-modeling","title":"Masked Language Modeling","text":"<ul> <li>Pretraining task for models like BERT</li> <li>Random tokens replaced with <code>[MASK]</code> and predicted by the model</li> <li>Output logits converted to probabilities via softmax</li> <li>Used to help the model build a strong language understanding foundation</li> </ul>"},{"location":"study-path/transformers/06-using-transformers/#example","title":"Example","text":"<ul> <li>Input: <code>\"I want to [MASK] pizza for tonight\"</code></li> <li>Output: <code>\"have\"</code>, <code>\"get\"</code>, <code>\"eat\"</code>, <code>\"make\"</code>, <code>\"order\"</code> as top predictions</li> </ul>"},{"location":"study-path/transformers/06-using-transformers/#try-it-yourself_2","title":"Try It Yourself","text":"<p>You can experiment with the code by opening the notebook in Google Colab:</p> <p>Open the Notebook in Colab</p> <p></p> <p>Back to Top</p>"},{"location":"study-path/transformers/06-using-transformers/#semantic-search-engine","title":"Semantic Search Engine","text":""},{"location":"study-path/transformers/06-using-transformers/#goal","title":"Goal","text":"<p>Build a semantic search engine that finds the most relevant document to a query based on meaning, not keyword match.</p>"},{"location":"study-path/transformers/06-using-transformers/#tools-dataset","title":"Tools &amp; Dataset","text":"<ul> <li>Dataset: Multi-News (2000 article summaries)</li> <li>Model: SentenceTransformer for lightweight sentence embeddings (384-dim)</li> <li>Libraries: Hugging Face Transformers, PyTorch, Pandas</li> </ul>"},{"location":"study-path/transformers/06-using-transformers/#process","title":"Process","text":"<ul> <li>Embed all documents once</li> <li>Embed user\u2019s query</li> <li>Compute cosine similarity between query and all document embeddings</li> <li>Retrieve top-k relevant results using <code>torch.topk</code></li> </ul>"},{"location":"study-path/transformers/06-using-transformers/#example-queries","title":"Example Queries","text":"<ul> <li>\"Artificial Intelligence\": returned AI-related articles</li> <li>\"Natural Disasters\": returned disaster-related summaries</li> <li>\"Law Enforcement\", \"Politics\": worked as expected</li> </ul>"},{"location":"study-path/transformers/06-using-transformers/#try-it-yourself_3","title":"Try It Yourself","text":"<p>Give it a try by opening the interactive Google Colab notebook below:</p> <p>Open the Notebook in Colab</p> <p></p> <p>Back to Top</p>"},{"location":"study-path/transformers/06-using-transformers/#model-evolution-table","title":"Model Evolution Table","text":"Model Name Category Architecture Strengths Ideal Use Cases Latest Version Info BERT Encoder-only Transformer Bidirectional context, strong understanding Text classification, Q&amp;A, embedding generation BERT-Base / BERT-Large GPT Decoder-only Transformer Text generation, instruction following Chatbots, creative writing, code generation GPT-4o (June 2024) T5 Encoder-Decoder Transformer Unified text-to-text architecture Translation, summarization, Q&amp;A T5.1.1, Flan-T5 Gemini Multi-modal Transformer + Vision + Memory Text + image processing, powerful LLM+VLM hybrid Multi-modal tasks, agentic reasoning Gemini 1.5 (June 2025) SentenceTransformer Encoder-only Siamese / Bi-encoder Transformer Sentence similarity, semantic search Embedding generation, retrieval, clustering <code>all-MiniLM-L6-v2</code> <p>Back to Top</p>"},{"location":"study-path/transformers/07-real-world-scenario-llm/","title":"\ud83d\udccc Quick Navigation","text":"<ul> <li>Course Overview</li> <li>Part 1: NLP &amp; Transformer Fundamentals</li> <li>Part 2: Practical LLM Applications</li> <li>Model Comparison Summary</li> <li>Key Takeaways</li> </ul>"},{"location":"study-path/transformers/07-real-world-scenario-llm/#course-overview","title":"Course Overview","text":"<p>This course is a hands-on introduction to transformer-based language models, combining theoretical foundations with practical implementations. The curriculum covers BERT, GPT, and T5 models, including their use in real-world NLP tasks.</p> <p>Back to Top</p>"},{"location":"study-path/transformers/07-real-world-scenario-llm/#part-1-nlp-transformer-fundamentals","title":"Part 1: NLP &amp; Transformer Fundamentals","text":""},{"location":"study-path/transformers/07-real-world-scenario-llm/#historical-phases-of-nlp","title":"Historical Phases of NLP","text":"<ul> <li>Rule-Based Systems: Manually defined linguistic rules.</li> <li>Statistical Methods: Probabilistic modeling based on word co-occurrence.</li> <li>Machine Learning: Classifiers like Naive Bayes and SVMs trained on features.</li> <li>Deep Learning &amp; Embeddings: Introduced context-aware dense vector representations.</li> </ul>"},{"location":"study-path/transformers/07-real-world-scenario-llm/#core-transformer-concepts","title":"Core Transformer Concepts","text":"<ul> <li>Attention Mechanism: Learns relationships between input tokens.</li> <li>Tokenization &amp; Embedding: Converts text into numerical representations.</li> <li>Encoder-Decoder Framework: Foundation for many transformer tasks.</li> <li>Pretraining vs. Fine-tuning: General training vs. task-specific adaptation.</li> </ul> <p>Back to Top</p>"},{"location":"study-path/transformers/07-real-world-scenario-llm/#part-2-practical-llm-applications","title":"Part 2: Practical LLM Applications","text":""},{"location":"study-path/transformers/07-real-world-scenario-llm/#bert-extractive-question-answering","title":"\ud83d\udfe2 BERT \u2013 Extractive Question Answering","text":"<ul> <li>Input: Context + Question</li> <li>Output: Extracted span from the context</li> <li>Limitations: 512-token input limit</li> <li>Solution: Chunked context with stride for long inputs</li> <li>Tools: Hugging Face Transformers, Plotly for token score visualization</li> </ul>"},{"location":"study-path/transformers/07-real-world-scenario-llm/#try-it-yourself","title":"Try It Yourself","text":"<p>You can try the code yourself by opening the notebook in Google Colab:</p> <p>Open the Notebook in Colab</p> <p></p>"},{"location":"study-path/transformers/07-real-world-scenario-llm/#gpt-instruction-following-generation","title":"\ud83d\udd35 GPT \u2013 Instruction-Following Generation","text":"<ul> <li>Fine-tuned GPT-2 using the <code>open-instruct</code> dataset</li> <li>Format: Instruction + Response prompt for generative training</li> <li>Approach: Causal language modeling with manually added pad token</li> <li>Output: Instruction-compliant completions (e.g., travel plans, recipes)</li> </ul>"},{"location":"study-path/transformers/07-real-world-scenario-llm/#try-it-yourself_1","title":"Try It Yourself","text":"<p>Want to test the code hands-on? You can run the full notebook in Google Colab.</p> <p>\ud83d\udd17 Open the notebook in Colab</p> <p></p>"},{"location":"study-path/transformers/07-real-world-scenario-llm/#try-it-on-hugging-face","title":"Try It on Hugging Face","text":"<p>Explore and experiment with the model directly on Hugging Face:</p> <p>\ud83d\udd17 DiabloGPT on Hugging Face</p>"},{"location":"study-path/transformers/07-real-world-scenario-llm/#t5-text-to-text-product-review-generation","title":"\ud83d\udd34 T5 \u2013 Text-to-Text Product Review Generation","text":"<ul> <li>Input: Product Title + Star Rating (e.g., <code>review: HDMI cable 3 stars</code>)</li> <li>Output: Review Headline and Body</li> <li>Preprocessing: Stratified sampling, review formatting</li> <li>Training: Conditional generation with truncation and early stopping</li> </ul>"},{"location":"study-path/transformers/07-real-world-scenario-llm/#try-it-yourself_2","title":"Try It Yourself","text":"<p>Explore and run the interactive code in Google Colab to deepen your understanding of the topic:</p> <p>\ud83d\udc49 Open the Notebook in Google Colab</p> <p>Or launch it directly using the badge below:</p> <p> </p>"},{"location":"study-path/transformers/07-real-world-scenario-llm/#try-it-on-hugging-face_1","title":"Try It on Hugging Face","text":"<p>Curious to explore the data? You can try it directly on Hugging Face below:</p> <p>\ud83d\udd17 View on Hugging Face</p> <p>Back to Top</p>"},{"location":"study-path/transformers/07-real-world-scenario-llm/#model-comparison-summary","title":"Model Comparison Summary","text":"Model Architecture Directionality Pretraining Task Best Suited For Limitations BERT Encoder-only Bidirectional Masked Language Modeling Classification, QA (extractive) 512-token context limit GPT-2 Decoder-only Unidirectional Causal Language Modeling Instruction-following generation Weak on structured input T5 Encoder-Decoder Bidirectional (input) / Decoder (output) Text-to-Text Generation Any NLP task (summarization, QA, generation) Needs task-specific prompts Gemini (Google) Multimodal Transformer Flexible (Encoder/Decoder) Mixture-of-Experts, RLHF, Vision-Language Chatbots, vision, code, reasoning Not fully open-source <p>Back to Top</p>"},{"location":"study-path/transformers/07-real-world-scenario-llm/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Always leverage pretrained models when possible to save resources.</li> <li>Use chunking + stride for models with token input limits (e.g., BERT).</li> <li>Fine-tuning even small models (e.g., GPT-2) can yield powerful behavior with limited data.</li> <li>Encoder-decoder models like T5 excel at diverse, generative tasks.</li> <li>Tools used: Hugging Face Transformers, Plotly, PyTorch Lightning</li> </ul> <p>\u2705 For scalable LLM deployment and optimization techniques, refer to the instructor's follow-up course on LLaMA, Mistral, and Gemma.</p> <p>Back to Top</p>"}]}