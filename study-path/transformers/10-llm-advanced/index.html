
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://baskarm.github.io/generative-ai-study-hub/study-path/transformers/10-llm-advanced/">
      
      
        <link rel="prev" href="../09-llm-prep/">
      
      
        <link rel="next" href="../11-llm-specialized/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.15">
    
    
      
        <title>10. Advanced LLM Training - Generative AI Study Hub</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../styles/logo.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="deep-purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#quick-navigation" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Generative AI Study Hub" class="md-header__button md-logo" aria-label="Generative AI Study Hub" data-md-component="logo">
      
  <img src="/generative-ai-study-hub/images/adhana.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Generative AI Study Hub
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              10. Advanced LLM Training
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../01-intro/" class="md-tabs__link">
          
  
  
    
  
  Study Path

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../reference-hub/" class="md-tabs__link">
          
  
  
    
  
  Reference Hub

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Generative AI Study Hub" class="md-nav__button md-logo" aria-label="Generative AI Study Hub" data-md-component="logo">
      
  <img src="/generative-ai-study-hub/images/adhana.png" alt="logo">

    </a>
    Generative AI Study Hub
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Study Path
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Study Path
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" checked>
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Transformers & Generative AI
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Transformers & Generative AI
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01-intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Intro
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02-getting-started/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Getting Started
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03-nlp-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. NLP Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04-transformer-intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Transformer Intro
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../05-popular-transformer-models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Popular Transformer Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06-using-transformers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Using Transformers (Practical)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../07-real-world-scenario-llm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. Real World Scenario with LLMs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../08-llm-intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Intro to LLMs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../09-llm-prep/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    9. Preparing LLMs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    10. Advanced LLM Training
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    10. Advanced LLM Training
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#quick-navigation" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Quick Navigation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#understanding-practical-limitations" class="md-nav__link">
    <span class="md-ellipsis">
      Understanding Practical Limitations
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    <span class="md-ellipsis">
      Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-challenges-with-large-models" class="md-nav__link">
    <span class="md-ellipsis">
      Training Challenges with Large Models
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Training Challenges with Large Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#goals" class="md-nav__link">
    <span class="md-ellipsis">
      Goals:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#observations" class="md-nav__link">
    <span class="md-ellipsis">
      Observations:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#memory-constraints-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Memory Constraints &amp; Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Memory Constraints &amp; Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#issues" class="md-nav__link">
    <span class="md-ellipsis">
      Issues:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#solutions-explored" class="md-nav__link">
    <span class="md-ellipsis">
      Solutions Explored:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#suggested-direction" class="md-nav__link">
    <span class="md-ellipsis">
      Suggested Direction:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#advanced-configuration-example" class="md-nav__link">
    <span class="md-ellipsis">
      Advanced Configuration Example
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#smarter-training-with-lora" class="md-nav__link">
    <span class="md-ellipsis">
      Smarter Training with LoRA
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Smarter Training with LoRA">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#benefits" class="md-nav__link">
    <span class="md-ellipsis">
      Benefits:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#combined-with" class="md-nav__link">
    <span class="md-ellipsis">
      Combined With:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references-further-reading" class="md-nav__link">
    <span class="md-ellipsis">
      References &amp; Further Reading
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#boosting-efficiency-peft-and-lora-in-depth" class="md-nav__link">
    <span class="md-ellipsis">
      Boosting Efficiency: PeFT and LoRA in Depth
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#quick-navigation_1" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Quick Navigation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#overview-the-problem-with-full-fine-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      Overview: The Problem with Full Fine-Tuning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#memory-usage-breakdown" class="md-nav__link">
    <span class="md-ellipsis">
      Memory Usage Breakdown
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction-to-parameter-efficient-fine-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction to Parameter-Efficient Fine-Tuning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lora-low-rank-adaptation-explained" class="md-nav__link">
    <span class="md-ellipsis">
      LoRA: Low-Rank Adaptation Explained
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LoRA: Low-Rank Adaptation Explained">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#key-concepts" class="md-nav__link">
    <span class="md-ellipsis">
      🎯 Key Concepts
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lora-parameter-and-its-usage" class="md-nav__link">
    <span class="md-ellipsis">
      🧠 LoRA - Parameter and its usage
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hugging-face-arxiv" class="md-nav__link">
    <span class="md-ellipsis">
      🔗 Hugging Face + ArXiv
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hyperparameters-in-lora" class="md-nav__link">
    <span class="md-ellipsis">
      Hyperparameters in LoRA
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#benefits-and-applications" class="md-nav__link">
    <span class="md-ellipsis">
      Benefits and Applications
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Benefits and Applications">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#advantages-of-lora" class="md-nav__link">
    <span class="md-ellipsis">
      🟢 Advantages of LoRA
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-use-cases" class="md-nav__link">
    <span class="md-ellipsis">
      🛠️ Example Use Cases
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references-further-reading_1" class="md-nav__link">
    <span class="md-ellipsis">
      References &amp; Further Reading
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#managing-data-memory-batch-size-sequence-length" class="md-nav__link">
    <span class="md-ellipsis">
      Managing Data Memory: Batch Size &amp; Sequence Length
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#quick-navigation_2" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Quick Navigation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction-to-memory-efficiency" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction to Memory Efficiency
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-analogy-book-pages-and-feedback" class="md-nav__link">
    <span class="md-ellipsis">
      Training Analogy: Book, Pages, and Feedback
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#core-training-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Core Training Parameters
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Core Training Parameters">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#batch-size" class="md-nav__link">
    <span class="md-ellipsis">
      Batch Size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sequence-length" class="md-nav__link">
    <span class="md-ellipsis">
      Sequence Length
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#trade-offs-in-training-efficiency" class="md-nav__link">
    <span class="md-ellipsis">
      Trade-offs in Training Efficiency
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references-further-reading_2" class="md-nav__link">
    <span class="md-ellipsis">
      References &amp; Further Reading
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#advanced-solutions-gradient-accumulation-checkpointing" class="md-nav__link">
    <span class="md-ellipsis">
      Advanced Solutions: Gradient Accumulation &amp; Checkpointing
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#quick-navigation_3" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Quick Navigation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#micro-batching-and-gradient-accumulation" class="md-nav__link">
    <span class="md-ellipsis">
      Micro-Batching and Gradient Accumulation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Micro-Batching and Gradient Accumulation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#micro-batching" class="md-nav__link">
    <span class="md-ellipsis">
      Micro-Batching
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-accumulation" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Accumulation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gradient-checkpointing" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Checkpointing
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Gradient Checkpointing">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-problem" class="md-nav__link">
    <span class="md-ellipsis">
      The Problem
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-solution-checkpointing" class="md-nav__link">
    <span class="md-ellipsis">
      The Solution: Checkpointing
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#efficiency-method-comparison-table" class="md-nav__link">
    <span class="md-ellipsis">
      Efficiency Method Comparison Table
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references-further-reading_3" class="md-nav__link">
    <span class="md-ellipsis">
      References &amp; Further Reading
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fitting-giants-practical-introduction-to-lora-for-large-models" class="md-nav__link">
    <span class="md-ellipsis">
      Fitting Giants: Practical Introduction to LoRA for Large Models
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#quick-navigation_4" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Quick Navigation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#micro-batching-gradient-accumulation" class="md-nav__link">
    <span class="md-ellipsis">
      Micro-Batching &amp; Gradient Accumulation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Micro-Batching &amp; Gradient Accumulation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gradient-accumulation_1" class="md-nav__link">
    <span class="md-ellipsis">
      🔁 Gradient Accumulation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="🔁 Gradient Accumulation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#formula-for-effective-batch-size" class="md-nav__link">
    <span class="md-ellipsis">
      Formula for Effective Batch Size
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gradient-checkpointing_1" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Checkpointing
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Gradient Checkpointing">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#concept" class="md-nav__link">
    <span class="md-ellipsis">
      ⛓️ Concept
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lora-low-rank-adaptation" class="md-nav__link">
    <span class="md-ellipsis">
      LoRA (Low-Rank Adaptation)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LoRA (Low-Rank Adaptation)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-it-works" class="md-nav__link">
    <span class="md-ellipsis">
      ⚙️ How It Works
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#batch-size-trade-offs" class="md-nav__link">
    <span class="md-ellipsis">
      Batch Size Trade-offs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Batch Size Trade-offs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#trade-off-spectrum" class="md-nav__link">
    <span class="md-ellipsis">
      🧮 Trade-off Spectrum
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mixed-precision-training" class="md-nav__link">
    <span class="md-ellipsis">
      Mixed Precision Training
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Mixed Precision Training">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#floating-point-formats" class="md-nav__link">
    <span class="md-ellipsis">
      🧊 Floating Point Formats
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#technique-comparison-summary" class="md-nav__link">
    <span class="md-ellipsis">
      Technique Comparison Summary
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#advanced-configuration-example_1" class="md-nav__link">
    <span class="md-ellipsis">
      Advanced Configuration Example
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#try-it-yourself" class="md-nav__link">
    <span class="md-ellipsis">
      Try It Yourself
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references-further-reading_4" class="md-nav__link">
    <span class="md-ellipsis">
      References &amp; Further Reading
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#expanding-lora-adapter-merging-and-effective-evaluations" class="md-nav__link">
    <span class="md-ellipsis">
      Expanding LoRA: Adapter Merging and Effective Evaluations
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#quick-navigation_5" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Quick Navigation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#overview-adapter-merging-in-lora" class="md-nav__link">
    <span class="md-ellipsis">
      Overview: Adapter Merging in LoRA
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-is-an-adapter-file" class="md-nav__link">
    <span class="md-ellipsis">
      What Is an Adapter File?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#using-xolotlmerge_lora" class="md-nav__link">
    <span class="md-ellipsis">
      Using xolotl.merge_lora
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#best-practices-in-lora-fine-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      Best Practices in LoRA Fine-Tuning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Best Practices in LoRA Fine-Tuning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#instructional-prompting" class="md-nav__link">
    <span class="md-ellipsis">
      Instructional Prompting
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#effective-batch-size" class="md-nav__link">
    <span class="md-ellipsis">
      Effective Batch Size
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-evaluation-and-loss-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      Model Evaluation and Loss Comparison
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#final-merge-and-output" class="md-nav__link">
    <span class="md-ellipsis">
      Final Merge and Output
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#colab-notebook" class="md-nav__link">
    <span class="md-ellipsis">
      Colab Notebook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references-further-reading_5" class="md-nav__link">
    <span class="md-ellipsis">
      References &amp; Further Reading
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../11-llm-specialized/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    11. Specialized LLM Training
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../12-llm-deployment/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    12. Final Deployment
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    LLM Ops
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            LLM Ops
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../llmops/01-getting-started/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Getting Started
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../llmops/02-pre-deployment/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Pre-Deployment Strategies
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../llmops/03-mlops-foundations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. MLOps Foundations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../llmops/04-advanced-deployment/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Advanced Model Deployment
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../llmops/05-inference-optimization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. ML Inference Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../llmops/06-economics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Economics of Inference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../llmops/07-cluster-management/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. Cluster Management
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../llmops/08-real-time-api/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Real-Time API Deployment
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    AI Evals for Engineers & PMs
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            AI Evals for Engineers & PMs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ai-eval/01-fundamentals.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    1. Fundamentals & Lifecycle
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ai-eval/02-error-analysis.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2. Systematic Error Analysis
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ai-eval/03-automated-evaluators.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3. Automated Evaluators
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ai-eval/04-architecture-strategies.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    4. Architecture Strategies
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ai-eval/05-production-monitoring.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    5. Production Monitoring
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ai-eval/06-human-review.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    6. Human Review Systems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ai-eval/07-cost-optimization.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    7. Cost Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ai-eval/08-wrap-up.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    8. Final Wrap-Up
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../reference-hub/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Reference Hub
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Reference Hub
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Foundation
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Foundation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../reference-hub/foundation/nlp-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    NLP Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../reference-hub/foundation/transformers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transformers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../reference-hub/foundation/models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Models
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3" >
        
          
          <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Practice
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3">
            <span class="md-nav__icon md-icon"></span>
            Practice
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../reference-hub/practice/tokenization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tokenization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../reference-hub/practice/embeddings/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Embeddings
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../reference-hub/practice/fine-tuning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Fine Tuning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../reference-hub/practice/efficiency/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Efficiency
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_4" >
        
          
          <label class="md-nav__link" for="__nav_3_4" id="__nav_3_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Applications
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_4">
            <span class="md-nav__icon md-icon"></span>
            Applications
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../reference-hub/applications/semantic-search/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Semantic Search
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../reference-hub/applications/qa-over-docs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    QA from Docs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../reference-hub/applications/instruction-tuning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Instruction Tuning
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_5" >
        
          
          <label class="md-nav__link" for="__nav_3_5" id="__nav_3_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    LLMs
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_5">
            <span class="md-nav__icon md-icon"></span>
            LLMs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../reference-hub/llms/intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../reference-hub/llms/rlhf/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RLHF
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../reference-hub/llms/scaling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Scaling
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_6" >
        
          
          <label class="md-nav__link" for="__nav_3_6" id="__nav_3_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Knowledge
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_6">
            <span class="md-nav__icon md-icon"></span>
            Knowledge
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../reference-hub/knowledge/prompt-engineering/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Prompt Engineering
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../reference-hub/knowledge/huggingface/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Hugging Face
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../reference-hub/knowledge/research/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Research Notes
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../reference-hub/knowledge/tools/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tools & Libraries
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../reference-hub/knowledge/learning-links/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Learning Links
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



  <h1>10. Advanced LLM Training</h1>

<h2 id="quick-navigation">📌 Quick Navigation</h2>
<ul>
<li><a href="#understanding-practical-limitations">Understanding Practical Limitations</a></li>
<li><a href="#boosting-efficiency-peft-and-lora-in-depth">Boosting Efficiency: PeFT and LoRA in Depth</a></li>
<li><a href="#managing-data-memory-batch-size-sequence-length">Managing Data Memory: Batch Size &amp; Sequence Length</a></li>
<li><a href="#advanced-solutions-gradient-accumulation-checkpointing">Advanced Solutions: Gradient Accumulation &amp; Checkpointing</a></li>
<li><a href="#fitting-giants-practical-introduction-to-lora-for-large-models">Fitting Giants: Practical Introduction to LoRA for Large Models</a></li>
<li><a href="#expanding-lora-adapter-merging-and-effective-evaluations">Expanding LoRA: Adapter Merging and Effective Evaluations</a></li>
</ul>
<h2 id="understanding-practical-limitations">Understanding Practical Limitations</h2>
<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#training-challenges-with-large-models">Training Challenges with Large Models</a></li>
<li><a href="#memory-constraints--optimization">Memory Constraints &amp; Optimization</a></li>
<li><a href="#advanced-configuration-example">Advanced Configuration Example</a></li>
<li><a href="#smarter-training-with-lora">Smarter Training with LoRA</a></li>
<li><a href="#references--further-reading">References &amp; Further Reading</a></li>
</ul>
<hr />
<h2 id="overview">Overview</h2>
<p>This section builds upon the previous lessons by transitioning from small-scale LLMs to working with <strong>larger models</strong> like Meta’s <em>LLaMA 3.1 8B</em>. The focus is on highlighting <strong>practical limitations</strong> and <strong>advanced optimization techniques</strong> to train large models with <strong>limited hardware resources</strong>.</p>
<p>👉 This tutorial used the <code>unsloth/Meta-Llama-3.1-8B-Instruct</code> base model and a custom storytelling dataset.</p>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="training-challenges-with-large-models">Training Challenges with Large Models</h2>
<h3 id="goals">Goals:</h3>
<ul>
<li>Scale from small to 8B+ parameter models</li>
<li>Use real-world model: <code>LLaMA 3.1 - Sloth Variant</code></li>
</ul>
<h3 id="observations">Observations:</h3>
<ul>
<li>Running on consumer-grade GPU (e.g., 24GB) fails due to:</li>
<li>Tokenizer misconfiguration</li>
<li>
<p>Out-of-memory errors even at minimal batch sizes</p>
</li>
<li>
<p>Reducing batch size &amp; sequence length:</p>
</li>
<li>✅ Enables training</li>
<li>❌ Hurts final performance</li>
</ul>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="memory-constraints-optimization">Memory Constraints &amp; Optimization</h2>
<h3 id="issues">Issues:</h3>
<ul>
<li>Model weights, optimizer, and dataset compete for memory</li>
<li>Even smallest 8B variant cannot fit fully</li>
</ul>
<h3 id="solutions-explored">Solutions Explored:</h3>
<ul>
<li>Minimal batch size = 1</li>
<li>Sequence length halved</li>
<li>Still fails on common GPUs</li>
</ul>
<h3 id="suggested-direction">Suggested Direction:</h3>
<ul>
<li>Use smarter techniques:</li>
<li>🟢 LoRA (Low-Rank Adaptation)</li>
<li>🟢 Gradient Checkpointing</li>
<li>🟢 Mixed Precision (bf16/8bit)</li>
</ul>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="advanced-configuration-example">Advanced Configuration Example</h2>
<pre><code class="language-yaml"># Source: advanced_train.yml
base_model: unsloth/Meta-Llama-3.1-8B-Instruct
datasets:
  - path: jaydenccc/AI_Storyteller_Dataset
    type:
      system_prompt: &quot;You are an amazing storyteller. From the following synopsis, create an engaging story.&quot;
      field_instruction: synopsis
      field_output: short_story
output_dir: ./models/Llama3_Storyteller2
sequence_length: 1024
micro_batch_size: 4
optimizer: adamw_bnb_8bit
learning_rate: 0.0002
adapter: lora
lora_r: 32
lora_alpha: 16
lora_dropout: 0.05
gradient_checkpointing: true
</code></pre>
<p>👉 <a href="https://colab.research.google.com/drive/1ntPiYbPZ8VomtKuIpEKUEKqdvJiSR3bH?usp=sharing">Open in Colab</a><br />
<a href="https://colab.research.google.com/drive/1ntPiYbPZ8VomtKuIpEKUEKqdvJiSR3bH?usp=sharing"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="smarter-training-with-lora">Smarter Training with LoRA</h2>
<p>LoRA is introduced as a <strong>lightweight fine-tuning</strong> mechanism.</p>
<h3 id="benefits">Benefits:</h3>
<ul>
<li>Reduces GPU memory footprint</li>
<li>Only updates a few trainable parameters</li>
<li>Compatible with large models like LLaMA</li>
</ul>
<h3 id="combined-with">Combined With:</h3>
<ul>
<li>bf16 or tf32 mixed precision</li>
<li>Gradient Checkpointing</li>
<li>8bit optimizer (bnb)</li>
</ul>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="references-further-reading">References &amp; Further Reading</h2>
<ul>
<li><a href="https://huggingface.co/meta-llama">Meta LLaMA 3.1 on Hugging Face</a></li>
<li><a href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models (ArXiv)</a></li>
<li><a href="https://huggingface.co/docs/transformers/index">Hugging Face Transformers Docs</a></li>
<li><a href="https://jalammar.github.io/illustrated-transformer/">Jay Alammar's Blog: The Illustrated Transformer</a></li>
<li><a href="https://colab.research.google.com/">Google Colab Guide</a></li>
<li><a href="https://github.com/openai/openai-cookbook">OpenAI Cookbook</a></li>
<li><a href="https://developer.nvidia.com/blog">NVIDIA: Memory-Efficient Training</a></li>
</ul>
<p><a href="#quick-navigation">Back to Top</a></p>
<h2 id="boosting-efficiency-peft-and-lora-in-depth">Boosting Efficiency: PeFT and LoRA in Depth</h2>
<h2 id="quick-navigation_1">📌 Quick Navigation</h2>
<ul>
<li><a href="#overview-the-problem-with-full-fine-tuning">Overview: The Problem with Full Fine-Tuning</a></li>
<li><a href="#memory-usage-breakdown">Memory Usage Breakdown</a></li>
<li><a href="#introduction-to-parameter-efficient-fine-tuning">Introduction to Parameter-Efficient Fine-Tuning</a></li>
<li><a href="#lora-low-rank-adaptation-explained">LoRA: Low-Rank Adaptation Explained</a></li>
<li><a href="#hyperparameters-in-lora">Hyperparameters in LoRA</a></li>
<li><a href="#benefits-and-applications">Benefits and Applications</a></li>
<li><a href="#references--further-reading">References &amp; Further Reading</a></li>
</ul>
<hr />
<h2 id="overview-the-problem-with-full-fine-tuning">Overview: The Problem with Full Fine-Tuning</h2>
<p>Fine-tuning large language models (LLMs) often exceeds hardware capabilities due to massive memory demands. The key goals of improving training efficiency are:</p>
<ul>
<li>🟢 Lower memory requirements.</li>
<li>🟢 Maintain model performance and accuracy.</li>
<li>🟢 Democratize access to LLM customization on limited hardware.</li>
</ul>
<p>Full fine-tuning is resource-intensive due to the need to update all model parameters, gradients, and optimizer states for every step.</p>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="memory-usage-breakdown">Memory Usage Breakdown</h2>
<p>Training large models consumes memory across four key areas:</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Description</th>
<th>Memory Usage</th>
</tr>
</thead>
<tbody>
<tr>
<td>Model Parameters</td>
<td>Learned weights of the model</td>
<td>2 × N GB (for N billion params @ FP16)</td>
</tr>
<tr>
<td>Gradients</td>
<td>Gradients for backpropagation</td>
<td>2 × N GB</td>
</tr>
<tr>
<td>Optimizer States</td>
<td>Additional states like moment estimates in Adam</td>
<td>~4–8 × N GB</td>
</tr>
<tr>
<td>Training Data</td>
<td>Input sequences + embeddings per batch</td>
<td>Variable (based on batch/seq length)</td>
</tr>
</tbody>
</table>
<p>➡️ Total: <strong>8–12×</strong> the model parameter memory.</p>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="introduction-to-parameter-efficient-fine-tuning">Introduction to Parameter-Efficient Fine-Tuning</h2>
<p>Rather than updating all parameters, <strong>Parameter-Efficient Fine-Tuning (PEFT)</strong> updates only a targeted subset of the model. This reduces resource requirements while maintaining task-specific performance.</p>
<p>One of the most prominent PEFT techniques is:</p>
<ul>
<li>🔵 <strong>LoRA (Low-Rank Adaptation)</strong> — inserts trainable, low-rank matrices into layers of a frozen pre-trained model.</li>
</ul>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="lora-low-rank-adaptation-explained">LoRA: Low-Rank Adaptation Explained</h2>
<p>LoRA modifies only a small number of parameters by introducing additional matrices into each layer of the frozen model.</p>
<h3 id="key-concepts">🎯 Key Concepts</h3>
<ul>
<li>Fine-tunes LLMs by updating low-rank matrices (<code>A</code> and <code>B</code>) instead of full weight matrices <code>W</code>.</li>
<li>Adds <code>ΔW = A × B</code> to <code>W</code> during forward pass.</li>
<li>Enables efficient updates and reduces memory overhead.</li>
</ul>
<h3 id="lora-parameter-and-its-usage">🧠 LoRA - Parameter and its usage</h3>
<table>
<thead>
<tr>
<th style="text-align: left;">Parameter</th>
<th style="text-align: left;">Description</th>
<th style="text-align: left;">Influence on Memory</th>
<th style="text-align: left;">Influence on Runtime</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">r (rank)</td>
<td style="text-align: left;">The rank of the LoRA matrices. Lower values reduce memory and computation cost.</td>
<td style="text-align: left;">✅ Lower rank = less memory usage</td>
<td style="text-align: left;">✅ Lower rank = faster computation</td>
</tr>
<tr>
<td style="text-align: left;">alpha</td>
<td style="text-align: left;">Scaling factor applied to the LoRA output. Usually alpha / r is the effective scale.</td>
<td style="text-align: left;">🔁 No direct memory impact, but may influence scale of activations</td>
<td style="text-align: left;">➖ May affect gradient scale, but not runtime</td>
</tr>
<tr>
<td style="text-align: left;">dropout</td>
<td style="text-align: left;">Dropout probability applied to the LoRA layers to regularize during training.</td>
<td style="text-align: left;">➖ Slight additional memory usage due to dropout mask</td>
<td style="text-align: left;">➖ Slight slowdown during training</td>
</tr>
<tr>
<td style="text-align: left;">bias</td>
<td style="text-align: left;">Whether to include bias terms. Can be 'none', 'all', or 'lora_only'.</td>
<td style="text-align: left;">➖ Adds small amount of memory if bias is included</td>
<td style="text-align: left;">➖ Minor impact if biases are added</td>
</tr>
<tr>
<td style="text-align: left;">target_modules</td>
<td style="text-align: left;">List of module names where LoRA adapters should be inserted (e.g., 'q_proj', 'v_proj').</td>
<td style="text-align: left;">✅ Selective targeting reduces memory footprint</td>
<td style="text-align: left;">✅ Reduces compute by targeting specific layers</td>
</tr>
<tr>
<td style="text-align: left;">merge_weights</td>
<td style="text-align: left;">If True, merges LoRA weights with the original model weights during inference.</td>
<td style="text-align: left;">✅ Merging removes need for separate LoRA weights at inference</td>
<td style="text-align: left;">✅ Faster inference by removing adapter layers</td>
</tr>
</tbody>
</table>
<h3 id="hugging-face-arxiv">🔗 Hugging Face + ArXiv</h3>
<ul>
<li><a href="https://huggingface.co/docs/peft/index">LoRA Model on Hugging Face</a></li>
<li><a href="https://arxiv.org/abs/2106.09685">Original Paper - LoRA: Low-Rank Adaptation</a></li>
</ul>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="hyperparameters-in-lora">Hyperparameters in LoRA</h2>
<table>
<thead>
<tr>
<th>Hyperparameter</th>
<th>Role</th>
<th>Typical Value(s)</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>rank</code> (r)</td>
<td>Size of the low-rank matrices</td>
<td>8, 16, 32</td>
</tr>
<tr>
<td><code>alpha</code></td>
<td>Scaling factor for ΔW = α × A × B</td>
<td>16 (default)</td>
</tr>
<tr>
<td><code>dropout</code></td>
<td>Regularization to prevent overfitting</td>
<td>0.0 – 0.1</td>
</tr>
<tr>
<td><code>target_modules</code></td>
<td>Model layers to apply LoRA (e.g., <code>query</code>, <code>value</code>, etc.)</td>
<td>Varies</td>
</tr>
</tbody>
</table>
<blockquote>
<p>📌 Higher <code>rank</code> means better task adaptation, but at a cost to memory savings.
📌 Lower <code>rank</code> helps prevent <strong>catastrophic forgetting</strong> during fine-tuning.</p>
</blockquote>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="benefits-and-applications">Benefits and Applications</h2>
<h3 id="advantages-of-lora">🟢 Advantages of LoRA</h3>
<ul>
<li>Reduces GPU memory footprint by &gt;80%</li>
<li>Avoids catastrophic forgetting</li>
<li>Accelerates training time</li>
<li>Easily pluggable into existing transformer architectures</li>
<li>Works well on small datasets</li>
</ul>
<h3 id="example-use-cases">🛠️ Example Use Cases</h3>
<ul>
<li>Personalizing a chatbot without retraining a full LLM</li>
<li>Domain-specific adaptation (e.g., legal, healthcare)</li>
<li>Multilingual extensions using LoRA adapters per language</li>
</ul>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="references-further-reading_1">References &amp; Further Reading</h2>
<ul>
<li><a href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models (ArXiv)</a></li>
<li><a href="https://huggingface.co/docs/peft/index">Hugging Face PEFT Documentation</a></li>
<li><a href="https://jalammar.github.io">Jay Alammar – LoRA Illustrated</a></li>
<li><a href="https://developer.nvidia.com/blog">NVIDIA – Efficient Fine-Tuning Techniques</a></li>
<li><a href="https://ai.googleblog.com/">Google – Parameter-Efficient Transfer Learning</a></li>
<li><a href="https://ai.facebook.com/research/">Meta AI Research – PEFT Methods</a></li>
<li><a href="https://openai.com/research">OpenAI – Scaling Laws and Efficient Training</a></li>
<li><a href="https://www.microsoft.com/en-us/research/project/deepspeed/">Microsoft – LoRA Integration in DeepSpeed</a></li>
</ul>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="managing-data-memory-batch-size-sequence-length">Managing Data Memory: Batch Size &amp; Sequence Length</h2>
<h2 id="quick-navigation_2">📌 Quick Navigation</h2>
<ul>
<li><a href="#introduction-to-memory-efficiency">Introduction to Memory Efficiency</a></li>
<li><a href="#training-analogy-book-pages-and-feedback">Training Analogy: Book, Pages, and Feedback</a></li>
<li><a href="#core-training-parameters">Core Training Parameters</a></li>
<li><a href="#batch-size">Batch Size</a></li>
<li><a href="#sequence-length">Sequence Length</a></li>
<li><a href="#trade-offs-in-training-efficiency">Trade-offs in Training Efficiency</a></li>
<li><a href="#references--further-reading">References &amp; Further Reading</a></li>
</ul>
<hr />
<h2 id="introduction-to-memory-efficiency">Introduction to Memory Efficiency</h2>
<p>In the previous lesson, we explored <strong>LoRA (Low-Rank Adaptation)</strong> as a method to reduce memory usage during fine-tuning of Large Language Models (LLMs) by modifying only a subset of model parameters.</p>
<p>In this session, we shift our focus to the <strong>memory consumption from data</strong>—specifically, how the structure of the input data (e.g., batch size and sequence length) affects training efficiency, cost, and feasibility.</p>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="training-analogy-book-pages-and-feedback">Training Analogy: Book, Pages, and Feedback</h2>
<p>To illustrate how models learn from data, the lesson uses an analogy:</p>
<ul>
<li><strong>Book</strong> → The complete dataset</li>
<li><strong>Page</strong> → A single training example</li>
<li><strong>Reading a few pages then testing</strong> → A <strong>training step</strong></li>
<li><strong>Reading the full book once</strong> → One <strong>epoch</strong></li>
<li><strong>Number of pages per step</strong> → <strong>Batch size</strong></li>
<li><strong>Words per page</strong> → <strong>Sequence length</strong></li>
</ul>
<p><img alt="LLM Training Analogy Diagram" src="../../../images/LLM_Batch_Analogy.png" />
<img alt="LLM Training Analogy Memory Usage " src="../../../images/LLM_Batch_Chart.png" /></p>
<p>This incremental reading process enables:
- Frequent model updates
- Improved generalization
- Reduced memory usage per training step</p>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="core-training-parameters">Core Training Parameters</h2>
<h3 id="batch-size">Batch Size</h3>
<ul>
<li>Determines how many training examples are processed before a model update.</li>
<li>Larger batches:</li>
<li>Require more memory</li>
<li>Yield more accurate gradients (faster convergence)</li>
<li>Are often limited by GPU/TPU capacity</li>
<li>Smaller batches:</li>
<li>Reduce memory consumption</li>
<li>Introduce noisier gradients, which may help generalization</li>
</ul>
<h3 id="sequence-length">Sequence Length</h3>
<ul>
<li>Number of tokens per training example.</li>
<li>Longer sequences:</li>
<li>Require more memory and compute</li>
<li>Contain richer contextual information</li>
<li>Shorter sequences:</li>
<li>Allow for bigger batch sizes</li>
<li>Reduce computation time</li>
<li>May lack enough context for learning</li>
</ul>
<p>📉 <strong>Trade-off</strong>: You often reduce batch size to accommodate longer sequences within memory limits.</p>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="trade-offs-in-training-efficiency">Trade-offs in Training Efficiency</h2>
<p>When training LLMs, we often <strong>cannot afford large batch sizes</strong> due to memory limits.</p>
<ul>
<li>Rarely does batch size get “too large”</li>
<li>Commonly, batch size becomes “too small” due to memory constraints</li>
<li>The key challenge:<blockquote>
<p><em>How to gain the benefits of large batch training without overwhelming memory resources?</em></p>
</blockquote>
</li>
</ul>
<p>Solutions to these constraints will be covered in the next lesson.</p>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="references-further-reading_2">References &amp; Further Reading</h2>
<ul>
<li><a href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models (arXiv)</a></li>
<li><a href="https://jalammar.github.io/illustrated-transformer/">Jay Alammar – Illustrated Transformer</a></li>
<li><a href="https://huggingface.co/docs/transformers/index">Hugging Face – Transformers Documentation</a></li>
<li><a href="https://openai.com/blog/">OpenAI Blog</a></li>
<li><a href="https://research.google/">Google Research</a></li>
<li><a href="https://www.microsoft.com/en-us/research/project/deepspeed/">Microsoft Research – DeepSpeed</a></li>
<li><a href="https://developer.nvidia.com/blog/">NVIDIA Developer Blog</a></li>
<li><a href="https://ai.facebook.com/">Facebook AI Research (FAIR)</a></li>
</ul>
<hr />
<p>➡️ <a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="advanced-solutions-gradient-accumulation-checkpointing">Advanced Solutions: Gradient Accumulation &amp; Checkpointing</h2>
<h2 id="quick-navigation_3">📌 Quick Navigation</h2>
<ul>
<li><a href="#micro-batching-and-gradient-accumulation">Micro-Batching and Gradient Accumulation</a></li>
<li><a href="#gradient-checkpointing">Gradient Checkpointing</a></li>
<li><a href="#efficiency-method-comparison-table">Efficiency Method Comparison Table</a></li>
<li><a href="#references--further-reading">References &amp; Further Reading</a></li>
</ul>
<hr />
<h2 id="micro-batching-and-gradient-accumulation">Micro-Batching and Gradient Accumulation</h2>
<p>Training large language models (LLMs) often requires careful memory management. Two primary techniques help in this regard: <strong>Micro-batching</strong> and <strong>Gradient Accumulation</strong>.</p>
<h3 id="micro-batching">Micro-Batching</h3>
<ul>
<li>A large batch is split into <strong>micro-batches</strong> to fit in limited memory.</li>
<li>Each micro-batch is processed sequentially.</li>
<li>Gradients are accumulated to simulate a larger batch.</li>
<li>Enables efficient training on hardware with limited GPU memory.</li>
</ul>
<p><strong>Effective Batch Size Formula</strong>:</p>
<pre><code>Effective Batch Size = Micro Batch Size × Gradient Accumulation Steps × Number of GPUs
</code></pre>
<p>🖼️ <img alt="Batch Brick Analogy" src="../../../images/Gradient_Accumlation.png" /></p>
<blockquote>
<p><em>Alt text</em>: Diagram showing micro-batch pages filling a book one by one, representing how gradient accumulation simulates a large batch.</p>
</blockquote>
<hr />
<h3 id="gradient-accumulation">Gradient Accumulation</h3>
<ul>
<li>Performs multiple forward/backward passes per optimization step.</li>
<li>Gradients are aggregated before updating the model.</li>
<li>Reduces memory usage but increases training time.</li>
</ul>
<p>🔧 <strong>Tip</strong>: Choose the largest micro-batch size that fits your GPU and increase accumulation steps only when necessary.</p>
<p>📉 <strong>Example</strong>:</p>
<table>
<thead>
<tr>
<th>Micro Batch Size</th>
<th>Accumulation Steps</th>
<th>GPUs</th>
<th>Effective Batch Size</th>
</tr>
</thead>
<tbody>
<tr>
<td>4</td>
<td>2</td>
<td>1</td>
<td>8</td>
</tr>
<tr>
<td>1</td>
<td>16</td>
<td>4</td>
<td>64</td>
</tr>
</tbody>
</table>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="gradient-checkpointing">Gradient Checkpointing</h2>
<h3 id="the-problem">The Problem</h3>
<ul>
<li>Training LLMs requires storing many <strong>activations</strong> during the forward pass.</li>
<li>These activations are needed for computing gradients in the backward pass.</li>
<li>Storing all of them consumes significant GPU memory.</li>
</ul>
<h3 id="the-solution-checkpointing">The Solution: Checkpointing</h3>
<ul>
<li><strong>Gradient Checkpointing</strong> stores only selected activations during forward pass.</li>
<li>During backward pass, missing activations are recomputed.</li>
<li>Balances memory savings with added computation.</li>
</ul>
<p>🖼️ <img alt="Checkpointing Diagram" src="https://jalammar.github.io/images/gpt3/GPT3-Checkpointing.png" /></p>
<blockquote>
<p><em>Alt text</em>: Diagram showing checkpoint blocks within the model to reduce activation memory.</p>
</blockquote>
<p>📊 <strong>Benefits</strong>:
- Substantial memory savings
- Trade-off: Slight increase in training time</p>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="efficiency-method-comparison-table">Efficiency Method Comparison Table</h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Memory Usage</th>
<th>Training Speed</th>
<th>Accuracy Impact</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>🟢 <strong>LoRA</strong></td>
<td>High savings</td>
<td>Moderate</td>
<td>Neutral to Slight loss</td>
<td>Trains subset of weights</td>
</tr>
<tr>
<td>🔵 <strong>Small Batch Size</strong></td>
<td>Medium</td>
<td>Faster (large batch)</td>
<td>Neutral</td>
<td>Limited if model needs larger batches</td>
</tr>
<tr>
<td>🔵 <strong>Gradient Accumulation</strong></td>
<td>High savings</td>
<td>Slower</td>
<td>Slightly Better</td>
<td>Allows larger effective batch</td>
</tr>
<tr>
<td>🟢 <strong>Gradient Checkpointing</strong></td>
<td>High savings</td>
<td>Slower</td>
<td>Neutral</td>
<td>Strategic activation savings</td>
</tr>
<tr>
<td>🔴 <strong>Mixed Precision</strong></td>
<td>Very High</td>
<td>Faster (with FP16)</td>
<td>Slight loss</td>
<td>Risk of instability at low precision</td>
</tr>
</tbody>
</table>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="references-further-reading_3">References &amp; Further Reading</h2>
<ul>
<li><a href="https://arxiv.org/abs/1706.03762">Attention is All You Need – Vaswani et al. (ArXiv)</a></li>
<li><a href="https://huggingface.co/docs/transformers/index">Hugging Face Transformers Documentation</a></li>
<li><a href="https://jalammar.github.io/illustrated-transformer/">Jay Alammar’s Illustrated Transformer</a></li>
<li><a href="https://developer.nvidia.com/blog/">NVIDIA – Gradient Accumulation &amp; Mixed Precision</a></li>
<li><a href="https://openai.com/research">OpenAI Research Papers</a></li>
<li><a href="https://www.microsoft.com/en-us/research/project/deepspeed/">Microsoft DeepSpeed Memory Optimization</a></li>
<li><a href="https://ai.googleblog.com/">Google AI Blog: Efficient Training Techniques</a></li>
<li><a href="https://ai.facebook.com/">Facebook AI Gradient Checkpointing</a></li>
</ul>
<p><a href="#quick-navigation">Back to Top</a></p>
<p>➡️ <a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="fitting-giants-practical-introduction-to-lora-for-large-models">Fitting Giants: Practical Introduction to LoRA for Large Models</h2>
<h2 id="quick-navigation_4">📌 Quick Navigation</h2>
<ul>
<li><a href="#micro-batching--gradient-accumulation">Micro-Batching &amp; Gradient Accumulation</a></li>
<li><a href="#gradient-checkpointing">Gradient Checkpointing</a></li>
<li><a href="#lora-low-rank-adaptation">LoRA (Low-Rank Adaptation)</a></li>
<li><a href="#batch-size-trade-offs">Batch Size Trade-offs</a></li>
<li><a href="#mixed-precision-training">Mixed Precision Training</a></li>
<li><a href="#technique-comparison-summary">Technique Comparison Summary</a></li>
<li><a href="#references--further-reading">References &amp; Further Reading</a></li>
</ul>
<hr />
<h2 id="micro-batching-gradient-accumulation">Micro-Batching &amp; Gradient Accumulation</h2>
<p>Micro-batching enables large batch benefits on memory-constrained hardware. By splitting a large batch into smaller "micro-batches", the system accumulates gradients across them before a single optimizer update.</p>
<h3 id="gradient-accumulation_1">🔁 Gradient Accumulation</h3>
<ul>
<li>Forward and backward passes are done over smaller micro-batches.</li>
<li>Gradients are accumulated in memory across steps.</li>
<li>A single optimizer update is performed after N steps.</li>
</ul>
<h4 id="formula-for-effective-batch-size">Formula for Effective Batch Size</h4>
<p>[
\text{Effective Batch Size} = \text{Micro Batch Size} \times \text{Accumulation Steps} \times \text{# of GPUs}
]</p>
<ul>
<li><strong>Example 1</strong>: Micro batch = 4, steps = 2, 1 GPU → Effective Batch Size = 8</li>
<li><strong>Example 2</strong>: Micro batch = 1, steps = 16, 4 GPUs → Effective Batch Size = 64</li>
</ul>
<p>🟢 Pros:
- Enables large effective batch sizes on small GPUs
- Good generalization performance</p>
<p>🔴 Cons:
- Slower training due to repeated forward/backward passes</p>
<p>👉 <a href="https://colab.research.google.com/">Open in Colab</a><br />
<a href="https://colab.research.google.com/"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="gradient-checkpointing_1">Gradient Checkpointing</h2>
<p>Gradient checkpointing saves memory by selectively storing activations during the forward pass.</p>
<h3 id="concept">⛓️ Concept</h3>
<ul>
<li>Store only key activations ("checkpoints") during the forward pass</li>
<li>During backpropagation, re-compute non-stored activations as needed</li>
</ul>
<p>🟢 Pros:
- Significant memory savings
- Feasible for training large models on limited hardware</p>
<p>🔴 Cons:
- Slower training due to partial recomputation</p>
<p>📘 <a href="https://huggingface.co/docs/transformers/perf_train_gpu_one#gradient-checkpointing">Hugging Face Docs</a><br />
📄 <a href="https://arxiv.org/abs/1604.06174">Gradient Checkpointing ArXiv Paper</a></p>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="lora-low-rank-adaptation">LoRA (Low-Rank Adaptation)</h2>
<p>LoRA reduces memory usage by freezing the base model and training only a small number of injected low-rank matrices.</p>
<h3 id="how-it-works">⚙️ How It Works</h3>
<ul>
<li>Inject trainable low-rank adapters into transformer layers</li>
<li>Only adapters are updated during fine-tuning</li>
</ul>
<p>🟢 Memory:
- Saves memory by reducing optimizer state size</p>
<p>🔵 Speed:
- Comparable to full fine-tuning</p>
<p>🔴 Accuracy:
- Can cap performance for some tasks
- May help in preventing catastrophic forgetting</p>
<p>📘 <a href="https://huggingface.co/docs/peft/conceptual_guides/lora">LoRA on Hugging Face</a><br />
📄 <a href="https://arxiv.org/abs/2106.09685">LoRA Paper on ArXiv</a></p>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="batch-size-trade-offs">Batch Size Trade-offs</h2>
<p>Batch size directly impacts training memory, speed, and generalization.</p>
<h3 id="trade-off-spectrum">🧮 Trade-off Spectrum</h3>
<ul>
<li>Small batch size → Lower memory, slower speed, better generalization</li>
<li>Large batch size → Higher memory, faster training, risk of overfitting</li>
</ul>
<p>🟢 Rule of Thumb:</p>
<blockquote>
<p>Use the largest batch size that fits in GPU memory.</p>
</blockquote>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="mixed-precision-training">Mixed Precision Training</h2>
<p>Training with lower precision (e.g., FP16, INT8, 4-bit) can greatly reduce memory usage and increase speed.</p>
<h3 id="floating-point-formats">🧊 Floating Point Formats</h3>
<table>
<thead>
<tr>
<th>Format</th>
<th>Memory Usage</th>
<th>Speed</th>
<th>Accuracy Impact</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP32</td>
<td>High</td>
<td>Standard</td>
<td>None</td>
</tr>
<tr>
<td>FP16</td>
<td>Medium</td>
<td>Fast</td>
<td>Minor</td>
</tr>
<tr>
<td>INT8</td>
<td>Low</td>
<td>Slower</td>
<td>Moderate</td>
</tr>
<tr>
<td>4-bit</td>
<td>Very Low</td>
<td>Slower</td>
<td>Noticeable</td>
</tr>
</tbody>
</table>
<p>🟢 Pros:
- Huge memory savings
- Speed boost on supporting hardware (e.g., A100, H100)</p>
<p>🔴 Cons:
- Minor accuracy loss at extreme bit reduction</p>
<p>📘 <a href="https://developer.nvidia.com/automatic-mixed-precision">Mixed Precision on NVIDIA</a><br />
📘 <a href="https://huggingface.co/docs/transformers/perf_train_gpu_one#mixed-precision">Hugging Face Docs</a></p>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="technique-comparison-summary">Technique Comparison Summary</h2>
<table>
<thead>
<tr>
<th>Technique</th>
<th>Memory Usage</th>
<th>Speed Impact</th>
<th>Accuracy Impact</th>
</tr>
</thead>
<tbody>
<tr>
<td>🟢 LoRA</td>
<td>Excellent</td>
<td>Neutral</td>
<td>Neutral/Task Dependent</td>
</tr>
<tr>
<td>🔵 Batch Size Tuning</td>
<td>Moderate</td>
<td>High</td>
<td>Task Dependent</td>
</tr>
<tr>
<td>🟡 Gradient Accumulation</td>
<td>High</td>
<td>Slightly Slower</td>
<td>Positive</td>
</tr>
<tr>
<td>🔴 Gradient Checkpointing</td>
<td>High</td>
<td>Slower</td>
<td>Neutral</td>
</tr>
<tr>
<td>🟢 Mixed Precision</td>
<td>Excellent</td>
<td>Faster (if supported)</td>
<td>Slightly Negative</td>
</tr>
</tbody>
</table>
<h2 id="advanced-configuration-example_1">Advanced Configuration Example</h2>
<pre><code>#Source: advanced_train2.yml
# model params
base_model: unsloth/Meta-Llama-3.1-8B-Instruct

# dataset params
datasets:
  - path: jaydenccc/AI_Storyteller_Dataset
    type: 
      system_prompt: &quot;You are an amazing storyteller. From the following synopsis, create an engaging story.&quot;
      field_system: system
      field_instruction: synopsis
      field_output: short_story
      format: &quot;&lt;|user|&gt;\n {instruction} &lt;/s&gt;\n&lt;|assistant|&gt;&quot;
      no_input_format: &quot;&lt;|user|&gt; {instruction} &lt;/s&gt;\n&lt;|assistant|&gt;&quot;

output_dir: ./models/Llama3_Storyteller2


# model params
sequence_length: 1024
bf16: auto
tf32: false

# training params
micro_batch_size: 4
num_epochs: 4
optimizer: adamw_bnb_8bit
learning_rate: 0.0002

logging_steps: 1


# LoRA
adapter: lora

lora_r: 32
lora_alpha: 16
lora_dropout: 0.05

lora_target_linear: true

# Gradient Accumulation
gradient_accumulation_steps: 1

# Gradient Checkpointing
gradient_checkpointing: true
</code></pre>
<h2 id="try-it-yourself">Try It Yourself</h2>
<p>Explore and run the notebook interactively using Google Colab:</p>
<p><a href="https://colab.research.google.com/drive/1NOO7cyRIN23JMTiWBGc3RkcLMmvqaDrp?usp=sharing">Open the Notebook in Colab</a></p>
<p><a href="https://colab.research.google.com/drive/1NOO7cyRIN23JMTiWBGc3RkcLMmvqaDrp?usp=sharing"><img alt="Open in Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="references-further-reading_4">References &amp; Further Reading</h2>
<ul>
<li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need (Vaswani et al.)</a></li>
<li><a href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models</a></li>
<li><a href="https://arxiv.org/abs/1604.06174">Gradient Checkpointing for Memory Optimization</a></li>
<li><a href="http://jalammar.github.io/illustrated-transformer/">Jay Alammar’s Illustrated Transformer</a></li>
<li><a href="https://developer.nvidia.com/automatic-mixed-precision">NVIDIA Mixed Precision Guide</a></li>
<li><a href="https://huggingface.co/docs/transformers/index">Hugging Face Transformers Documentation</a></li>
<li><a href="https://ai.googleblog.com/">Google Research: Efficient Training Techniques</a></li>
<li><a href="https://openai.com/research">OpenAI Research Blog</a></li>
</ul>
<hr />
<p>➡️ <a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="expanding-lora-adapter-merging-and-effective-evaluations">Expanding LoRA: Adapter Merging and Effective Evaluations</h2>
<h2 id="quick-navigation_5">📌 Quick Navigation</h2>
<ul>
<li><a href="#overview-adapter-merging-in-lora">Overview: Adapter Merging in LoRA</a></li>
<li><a href="#what-is-an-adapter-file">What Is an Adapter File?</a></li>
<li><a href="#using-xolotlmergelora">Using <code>xolotl.merge_lora</code></a></li>
<li><a href="#best-practices-in-lora-fine-tuning">Best Practices in LoRA Fine-Tuning</a></li>
<li><a href="#instructional-prompting">Instructional Prompting</a></li>
<li><a href="#effective-batch-size">Effective Batch Size</a></li>
<li><a href="#model-evaluation-and-loss-comparison">Model Evaluation and Loss Comparison</a></li>
<li><a href="#final-merge-and-output">Final Merge and Output</a></li>
<li><a href="#colab-notebook">Colab Notebook</a></li>
<li><a href="#references--further-reading">References &amp; Further Reading</a></li>
</ul>
<hr />
<h2 id="overview-adapter-merging-in-lora">Overview: Adapter Merging in LoRA</h2>
<p>In this session, we explore how to merge the lightweight adapter produced by LoRA-based training with the base model. We review best practices to optimize accuracy and stability, and correct common mistakes in the fine-tuning process.</p>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="what-is-an-adapter-file">What Is an Adapter File?</h2>
<ul>
<li>After LoRA training, only a <strong>small diff file</strong> is produced—this is the <em>adapter</em>.</li>
<li>It contains the modified 1% of weights from the original model.</li>
<li>When <strong>merged with the base model</strong>, it reconstructs the fully fine-tuned model.</li>
<li>This saves disk space and makes training more efficient.</li>
</ul>
<p>🧠 <strong>Key Concept</strong>: Adapter = Delta weights (not the full model)</p>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="using-xolotlmerge_lora">Using <code>xolotl.merge_lora</code></h2>
<p>To merge the adapter with the base model:</p>
<pre><code class="language-python">from xolotl import merge_lora

merge_lora(
  config_path=&quot;path/to/config.yaml&quot;,
  adapter_path=&quot;path/to/adapter&quot;
)
</code></pre>
<ul>
<li>The process creates a new model directory with the full merged weights.</li>
<li>You can then prompt the model as usual for inference.</li>
</ul>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="best-practices-in-lora-fine-tuning">Best Practices in LoRA Fine-Tuning</h2>
<p>Despite a successful training, multiple common issues were present.</p>
<h3 id="instructional-prompting">Instructional Prompting</h3>
<p>❌ Mistake:
- No explicit task prompt was given to the instruction-following model.</p>
<p>✅ Fix:
- Add a system prompt like:</p>
<pre><code class="language-plaintext">You are an amazing storyteller. From the following synopsis, write an engaging story.
</code></pre>
<ul>
<li>Helps align model behavior with instruction-tuned expectations.</li>
</ul>
<h3 id="effective-batch-size">Effective Batch Size</h3>
<p>❌ Mistake:
- Micro batch size = 4<br />
- Gradient accumulation steps = 4<br />
→ Effective batch size = 16</p>
<ul>
<li>Too small for a meaningful update over a small dataset.</li>
</ul>
<p>✅ Fix:
- <strong>Increase micro batch size</strong> to maximum that fits in memory
- <strong>Reduce accumulation steps</strong> to speed up training</p>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="model-evaluation-and-loss-comparison">Model Evaluation and Loss Comparison</h2>
<ul>
<li>After training with improved prompts and optimized batch size:</li>
<li>Training ran for more steps</li>
<li>Loss values were significantly lower</li>
<li>Indicates improved convergence and generalization</li>
</ul>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="final-merge-and-output">Final Merge and Output</h2>
<ul>
<li>The merged model was tested on a story generation prompt.</li>
<li>Result: A more coherent and structured output, with better alignment to storytelling instructions.</li>
<li>This validates the importance of correct prompts and batch size tuning.</li>
</ul>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="colab-notebook">Colab Notebook</h2>
<p>👉 <a href="https://colab.research.google.com/drive/1xTl3INe-0jRAHLJFQQIyZapZFVvc9_YB?usp=sharing">Open in Colab</a><br />
<a href="https://colab.research.google.com/drive/1xTl3INe-0jRAHLJFQQIyZapZFVvc9_YB?usp=sharing"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<p><a href="#quick-navigation">Back to Top</a></p>
<hr />
<h2 id="references-further-reading_5">References &amp; Further Reading</h2>
<ul>
<li><a href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of LLMs (arXiv)</a></li>
<li><a href="https://huggingface.co/docs/peft/index">Hugging Face – Parameter Efficient Fine-Tuning Guide</a></li>
<li><a href="https://github.com/openai/openai-cookbook">OpenAI Cookbook</a></li>
<li><a href="https://jalammar.github.io/illustrated-transformer/">Jay Alammar – Visualizing Transformers</a></li>
<li><a href="https://ai.googleblog.com">Google AI Blog</a></li>
<li><a href="https://xai-tools.com">XAI – Explainable AI Projects</a></li>
<li><a href="https://www.microsoft.com/en-us/research/project/deepspeed/">Microsoft Research – DeepSpeed</a></li>
</ul>
<hr />
<p>🗓 Generated on: July 27, 2025</p>
<p>➡️ <a href="#quick-navigation">Back to Top</a></p>







  
    
  
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="July 26, 2025 05:23:04 UTC">July 26, 2025</span>
  </span>

    
    
    
    
  </aside>





                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../09-llm-prep/" class="md-footer__link md-footer__link--prev" aria-label="Previous: 9. Preparing LLMs">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                9. Preparing LLMs
              </div>
            </div>
          </a>
        
        
          
          <a href="../11-llm-specialized/" class="md-footer__link md-footer__link--next" aria-label="Next: 11. Specialized LLM Training">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                11. Specialized LLM Training
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/..." target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "content.footer", "navigation.expand", "navigation.sections", "navigation.top", "toc.integrate", "navigation.indexes", "search.suggest", "search.highlight", "navigation.instant", "navigation.footer"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.56ea9cef.min.js"></script>
      
    
  </body>
</html>